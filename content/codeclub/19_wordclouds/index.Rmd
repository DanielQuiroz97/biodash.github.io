---
title: "Session 19: Word Clouds via Tidytext"
summary: "Visualizing text frequency distributions"
authors: [michael-broe]
date: "2021-04-21"
output: hugodown::md_document
toc: true
image:
  caption: ""
  focal_point: ""
  preview_only: false
editor_options: 
  markdown: 
    wrap: 72
---

<br> <br> <br>

------------------------------------------------------------------------

## New To Code Club?

-   First, check out the [Code Club Computer Setup](/codeclub-setup/)
    instructions, which also has some pointers that might be helpful if
    you're new to R or RStudio.

-   Please open RStudio before Code Club to test things out -- if you
    run into issues, join the Zoom call early and we'll troubleshoot.

------------------------------------------------------------------------

## Session Goals

-   Learn the fundamentals of **text mining**.
-   Learn how to do text mining in a tidyverse setting.
-   Reuse some of our dplyr and ggplot skills on text.
-   Learn how to very simply create word cloud visualizations.

------------------------------------------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

This is another in our current series on text processing. We'll be using the following previously used packages which you should load first (install them if you haven't already):

```{r, message=FALSE}
library(tidyverse)
library(bakeoff)
```

We'll also be using the following packages, which you should install and load:

```{r, message=FALSE}
# Uncomment the following line to install:
# install.packages(c("tidytext", "gutenbergr", "wordcloud"))
                 
library(tidytext)
library(gutenbergr)
library(wordcloud)
```

## Introduction

In this CodeClub session we'll see how to create word clouds (also known as tag clouds) from text, using the **tidytext** and **wordcloud** packages. A word cloud is a visualization of word frequencies, graphically highlighting the most common words.

We need to get some text from somewhere, so first let's do it in the simplest possible way. 
Here we manually enter a quote, line by line, as a vector of five character strings. This is the first stanza from Robert Lowell's [Skunk Hour](https://www.poetryfoundation.org/poems/47694/skunk-hour):

```{r}
lowell <- c("Nautilus Island's hermit",
          "heiress still lives through winter in her Spartan cottage;",
          "her sheep still graze above the sea.",
          "Her son's a bishop. Her farmer is first selectman in our village;",
          "she's in her dotage.")
```

In textual analysis we distinguish between word **types**, and word **tokens** (multiple instances of those words in text). For example there are two tokens of the word-type "still" in this stanza:

> heiress `still` lives through winter  
her sheep `still` graze above the sea

And slightly more abstractly there are four tokens of "her", modulo capitalization:

> `her` Spartan cottage   
`her` sheep still graze   
`Her` son's a bishop.  
`Her` farmer

Formally, it's the *token frequency* of the *word types* we are ultimately interested in capturing. So: two tasks, extract the word tokens, and count them! Done!

The reason this is tricky is that natural language text is messy: the task of extracting a clean set of tokens to count is termed **text mining** or **tokenization**. We would also like to get the output into a tidyverse compliant data frame, so we can use familiar **dplyr** and **ggplot** functions to analyze it.

We could imagine attacking this using **stringr** functions:

```{r}
lowell_tokens <- lowell %>% 
  # convert upper to lower case; returns a character vector.
  str_to_lower() %>%
  # remove punctuation with a character class; returns a list.
  str_extract_all("[a-z]+") %>% 
  # flatten that list
  unlist() %>%
  # stick it in a data frame
  as_tibble()                      

print(lowell_tokens, n = 38)
```

This is a good start: it gets rid of the capitalization issue, and also gets rid of the punctuation. But there's a problem. The regular expression pattern `[a-z]+` doesn't recognize *possessives* or *contractions*: it just strips anything that's not a letter, so it messes up with `Island's`, `son's`, and `she's`: welcome to the subtleties of processing natural language text algorithmically! Exceptions, exceptions!! 

We could fiddle about with our regex, but... *there's a package for that!* This kind of text mining is exactly what the [tidytext](https://www.tidytextmining.com) package was built for. It will simultaneously strip punctuation intelligently and 'unnest' lines into word tokens.

Tidytext functions need a dataframe to operate on. So first we need to get the poem into a data frame; here we'll use the column name `text`.

```{r}
lowell_df <- tibble(text = lowell)

lowell_df
```

Each string in the character vector becomes a single row in the data frame.

Again we want one word-token per row, to 'tidy' our data. This is what `tidytext::unnest_tokens()` does.  We're going to unnest words in this case (we can unnest other things, like characters, sentences, regexes, even tweets) and we need to specify the variable in the dataframe we are unnesting (in this case just `text`). This will create a new word-token data frame, and we'll name the variable in the data frame `word`. This is important (see later on stop words). 

```{r}
lowell_tidy <- lowell_df %>%
    unnest_tokens(word, text)

print(lowell_tidy, n = 35)
```

Punctuation has been stripped and all words are lower case, but possessives and contractions are preserved (fancy usage of `str_` regular expression functions under the hood...).

## Bakeoff!

Now that we have the basic idea, let's look at a more interesting data set, from the `bakeoff` package. 

First we'll create a data frame with just the `signature` column from the `bakes` data set:

```{r}
signature_df <- select(bakes, signature)

signature_df
```

Next we tokenize by word on the signature column:

```{r}
signature_tidy <- signature_df %>%
    unnest_tokens(word, signature)

signature_tidy
```

Now we want to count those tokens: i.e. we want to collapse all duplicate word tokens into a single word type, with the corresponding frequency. Since we now have tidy data, dplyr to the rescue!

> dplyr `count()` lets you quickly count the unique values of one or more variables. 
The option `sort`, if TRUE, will show the largest groups at the top.

```{r}
signature_count <- signature_tidy %>% 
    count(word, sort = TRUE)

signature_count
```

We're way more interested in `cake` than `and`: this is an example of a **stop word**:

> In computing, stop words are words which are filtered out before or after processing of natural language data (text). "stop words" usually refers to the most common words in a language.

> One of our major performance (search) optimizations... is removing the top 10,000 most common English dictionary words (as determined by Google search). Itâ€™s shocking how little is left of most posts once you remove the top 10k English dictionary words...

The tidytext package has a database of just over a thousand of these words, including 'and':

```{r}
print(stop_words, n = 30)
```

Note that the name of the stop word column is `word`, and the name we used in our tokenized column is `word` (now you will see why we used that name) so we can use dplyr's `anti_join()` to filter the word tokens!

> `anti_join()` returns all rows from x without a match in y (where x are the word tokens, and y are the stop words)

```{r}
signature_count <- signature_tidy %>% 
    count(word, sort = TRUE) %>% 
    anti_join(stop_words)

signature_count
```

Since we are in the tidyverse, we can pipe our results into ggplot. First we filter on counts above a certain threshold (here 12, just for visualization purposes):

```{r}
signature_count %>%
    filter(n > 12) %>%
    ggplot(aes(n, word)) +
    geom_col() +
    theme_minimal() +
    labs(y = NULL)
```

This is ordered alphabetically by default, bottom to top; but we can reorder by count (n) using dplyr `mutate()`:

```{r}
signature_count %>%
    filter(n > 12) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) +
    geom_col() +
    theme_minimal() +
    labs(y = NULL)
```

We now have everything we need for a word cloud: word types and their token frequencies:

The only **obligatory** arguments to `wordcloud()` are the first two: the rest just let you tweak the graphic:

```{r}
wordcloud(words = signature_count$word, 
          freq = signature_count$n, 
          min.freq = 12, 
          random.order=FALSE, 
          rot.per=0.3, 
          colors=brewer.pal(8, "Dark2"))
```

`min.freq` lets you filter on a frequency threshold. `random.order=FALSE` plots words in decreasing frequency (highest most central); `rot.per` is the proportion of words with 90 degree rotation; `colors=brewer.pal(8, "Dark2")` lets you choose an [RColorBrewer](https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html) color palette of your choice.

{{% callout note %}}

Lemmatization

If you create a count data frame of `signature_tidy` without the `sort = TRUE` option, the words are sorted alphabetically. And if you look through that table you will see many instances such as `apple, apples;  apricot, apricots; cake, cakes` etc. Arguably, these are the same word type (think "dictionary word") just grammatical variations.  Properly collapsing these into a single type is called **lemmatization**: a very difficult problem which would take us far afield into the **morphology of words**. Again in general there are many exceptions, only partly due to English borrowing so many words from other languages: besides `apple, apples` there is `mouse, mice; self, selves; bacillus, bacilli; basis, bases`. etc. These are known as [irregular plurals](https://www.thoughtco.com/irregular-plural-nouns-in-english-1692634). 

Verbs are worse! Perhaps you would also consider the inflectional forms `run, runs, ran, running` as the same type, just as a dictionary does. How do you reduce those algorithmically? And if you consider inflectional forms as the same dictionary word, how would you tackle Ancient Greek, which has **hundreds** of inflected forms for the same verb? Here are just a few, there are pages and pages of them... 

![](greek.jpg)

Currently machine learning has been unleashed on this problem, with limited success. The traditional computational linguists' algorithms are still winning...

{{% /callout %}}

## The `gutenbergr` package

Say we wanted to do a word cloud for a more substantive text like Darwin's *Origin of Species*.

[Project Gutenberg](https://www.gutenberg.org) is a volunteer effort to digitize and archive cultural works and is the oldest digital library. It has over 60,000 books in the public domain (including Darwin's works).

The [gutenbergr](https://cran.r-project.org/web/packages/gutenbergr/vignettes/intro.html) package allows you to download any of these works **directly into a data frame** using just the Project Gutenberg ID. This is then perfect input for tidytext. The package provides all the metadata to search for author and work IDs inside R (you can also just find the ID by searching on the Project Gutenberg website):

```{r}
darwins_works <- gutenberg_metadata %>%
    filter(author == "Darwin, Charles")

darwins_works
```

An inspection of the results of *Origin of Species* on the website reveals that the latest edition is ID 2009. Let's grab it:

```{r}
OoS <- gutenberg_download(2009)
```

In the breakout rooms, we'll work through inspecting the frequencies and creating a word cloud for this text.

The `gutenbergr` package is extremely useful, but as long as you can read a document into R, you can then convert it to a data frame as we did in the very first example above, and then the tidytext pipeline will work. The [readtext](https://cran.r-project.org/web/packages/readtext/vignettes/readtext_vignette.html) package can import a variety of formats, including PDFs and Microsoft Word documents.

## Breakout rooms

### Exercise 1

:::puzzle

Run the command:

```r
OoS <- gutenberg_download(2009)
```
and inspect the data frame. Identify the name of the column you want to tokenize.

Then use the `unnest_tokens()` command to create a data frame of word tokens.

<details> <summary> Hints (click here) </summary>

<br>It's the <code>text</code> column you want. <code>gutenbergr</code> includes the <code>gutenberg_ID</code> in case you download multiple texts into the same data frame. Remember to name the column in the new data frame <code>word</code> so we can filter any stop words later on.
<br>
</details>

<br>

<details> <summary> Solution (click here) </summary>
<br>

```{r}
OoS <- gutenberg_download(2009)

OoS
```
```{r}
OoS_tidy <- OoS %>%
    unnest_tokens(word, text)
    
OoS_tidy
```


<br>


</details>
:::

------------------------------------------------------------------------

### Exercise 2

:::puzzle

Count and sort the tokens into a new data frame. Inspect the output. Are there any stop words?

<details> <summary> Hints (click here) </summary>

<br> Pipe the <code>word</code> column of the data frame into the dplyr <code>count()</code> function with the <code>sort = TRUE</code> option.
<br>
</details>

<br>

<details> <summary> Solution (click here) </summary>
<br>

```{r}
OoS_count <- OoS_tidy %>% 
    count(word, sort = TRUE)

OoS_count
```

<br>


</details>
:::

------------------------------------------------------------------------

### Exercise 3

:::puzzle

Remove the stop words from the output and inspect the results.

<details> <summary> Hints (click here) </summary>

<br> Use <code>antijoin()</code> with the tidytext <code>stop_words</code> data frame:
<br>
</details>

<br>

<details> <summary> Solution (click here) </summary>
<br>

```{r}
OoS_count <- OoS_tidy %>%
    count(word, sort = TRUE) %>% 
    anti_join(stop_words)

OoS_count
```
<br>

</details>
:::

------------------------------------------------------------------------

### Exercise 4

:::puzzle

Visualize the counts using `ggplot()`, from highest frequency to lowest, using a frequency cutoff of 200. Does any one word stand out in any way?

Does the tidytext package perform lemmatization? Are there any irregular plurals in this result?

<details> <summary> Hints (click here) </summary>

<br> Use a dplyr <code>filter()</code> command on the <code>n</code> column, and, well just look at the examples in the presentation for the details of piping it into <code>ggplot()</code>!
<br>
</details>

<br>

<details> <summary> Solution (click here) </summary>
<br>

```{r}
OoS_count %>%
    filter(n > 200) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) +
    geom_col() +
    theme_minimal() +
    labs(y = NULL)
```
<br>

tidytext does not lemmatize. There are many plurals in this list, so undoubtedly there are corresponding singulars of lower frequency. Indeed we see both `forms` and `form`. And of course the irregular `genera` is the plural of `genus`.

</details>
:::

------------------------------------------------------------------------

### Exercise 5

:::puzzle

Create a word cloud of this data frame, with the same frequency cut off as the `ggplot()` (200). Otherwise use the same settings as in the presentation. Tweak those settings, especially the frequency threshold and rotation proportion. See what happens when you set `random.order=TRUE`.

<details> <summary> Hints (click here) </summary>

<br>The option for the the frequency threshold is <code>min.freq = 200</code>.
<br>
</details>

<br>

<details> <summary> Solution (click here) </summary>
<br>

```{r}
wordcloud(words = OoS_count$word, 
          freq = OoS_count$n, 
          min.freq = 200, 
          random.order=FALSE, 
          rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```


<br>


</details>
:::

------------------------------------------------------------------------


