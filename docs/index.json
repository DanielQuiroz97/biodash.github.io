[{"authors":["admin"],"categories":null,"content":"Jelmer Poelstra started at the MCIC (Molecular and Cellular Imaging Center) in June 2020, where he provides bioinformatics support and education. His background is in evolutionary and population genetics, and most of his research has focused on understanding speciation using genomic approaches.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603299924,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://biodash.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Jelmer Poelstra started at the MCIC (Molecular and Cellular Imaging Center) in June 2020, where he provides bioinformatics support and education. His background is in evolutionary and population genetics, and most of his research has focused on understanding speciation using genomic approaches.","tags":null,"title":"Jelmer Poelstra","type":"authors"},{"authors":["jessica-cooperstone"],"categories":null,"content":"Jessica Cooperstone is an Assistant Professor in Horticulture and Crop Science, and Food Science and Technology at The Ohio State University working at the intersection of plant science and human nutrition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1608644921,"objectID":"da60494872ddaf739115b5da033f1fed","permalink":"https://biodash.github.io/authors/jessica-cooperstone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jessica-cooperstone/","section":"authors","summary":"Jessica Cooperstone is an Assistant Professor in Horticulture and Crop Science, and Food Science and Technology at The Ohio State University working at the intersection of plant science and human nutrition.","tags":null,"title":"Jessica Cooperstone","type":"authors"},{"authors":["michael-broe"],"categories":null,"content":"Michael Broe is a Bioinformatics Research Scientist at the Department of Evolution, Ecology, and Organismal Biology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603299924,"objectID":"f515857d0961e2ded569db22cc57c70c","permalink":"https://biodash.github.io/authors/michael-broe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/michael-broe/","section":"authors","summary":"Michael Broe is a Bioinformatics Research Scientist at the Department of Evolution, Ecology, and Organismal Biology.","tags":null,"title":"Michael Broe","type":"authors"},{"authors":["mike-sovic"],"categories":null,"content":"Mike Sovic is a Bioinformatics Research Scientist at CAPS, the Center for Applied Plant Sciences.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603299924,"objectID":"e25a3ca217243530f65efb3cd930207b","permalink":"https://biodash.github.io/authors/mike-sovic/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mike-sovic/","section":"authors","summary":"Mike Sovic is a Bioinformatics Research Scientist at CAPS, the Center for Applied Plant Sciences.","tags":null,"title":"Mike Sovic","type":"authors"},{"authors":["stephen-opiyo"],"categories":null,"content":"Stephen Opiyo is a bioinformatics and biostatistics research scientist at MCIC Columbus.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1603299924,"objectID":"9c82e356b8ac88d63b60055c202796b4","permalink":"https://biodash.github.io/authors/stephen-opiyo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stephen-opiyo/","section":"authors","summary":"Stephen Opiyo is a bioinformatics and biostatistics research scientist at MCIC Columbus.","tags":null,"title":"Stephen Opiyo","type":"authors"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Setup New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also have pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  Session goals Today, you will learn:\n That you should avoid copying your code. What different strategies for iteration exist in R. What vectorization is and how to make use of it. How to write a for loop. Best practices when using for loops. When you should (not) use for loops. Bonus: if statements.   Introduction Don\u0026rsquo;t Repeat Yourself Sometimes, you have a block of code and you need to repeat the operations in that code almost exactly. For instance, you may want to rerun a statistical model with different parameter values, rerun an analysis for a different batch of samples, or extract the same information for many different genes.\nYour first instinct may be to copy-and-paste the block of code, and make the necessary slight adjustments in the pasted block. However, iterating and writing your own functions are strategies that are clearer, less error-prone, and more flexible (and these two can also be combined). When the number of repeats are high, iteration is needed. When the code that needs to be repeated is more than a line or two, writing your own functions becomes useful.\nIteration Loops are the most universal iteration tool and the one we will focus on today. However, R has \u0026ldquo;functional programming\u0026rdquo; iteration methods that are less verbose and that can also be quicker to execute. These are the apply family of functions, and a more recent tidyverse approach implemented in the purrr package: we will learn more about those in the two upcoming Code Club sessions.\nLoops are still a very good place to start using iteration because they make the iteration explicit and are therefore more intuitive than functional alternatives. In addition, they can easily accommodate longer blocks of code without the need to also write your own function.\nToday, we will talk about the most common type of loop: the for loops. (Other types of loops in R are while loops and repeat loops. Related to loops are if statements, see the bonus exercise for some basics.)\nBut first\u0026hellip; Before we tackle loops we should take a step back and explore vectorization a bit more, which was briefly introduced by Michael in Code Club session 9. Besides functional programming methods, vectorization is the other reason that loops are not as widely used in R as in other programming languages.\n I: Vectorization Let\u0026rsquo;s say we have a vector (i.e., a collection of values) that consists of distances in miles:\ndists_miles \u0026lt;- c(24, 81, 48, 29, 177, 175, 20, 11, 62, 156)   Of course, we can\u0026rsquo;t science with miles, so we\u0026rsquo;ll have to convert these distances to kilometers by multiplying each value in the vector by 1.61. You may or may not know that this can be done really easily in R:\ndists_km \u0026lt;- dists_miles * 1.61 dists_km #\u0026gt; [1] 38.64 130.41 77.28 46.69 284.97 281.75 32.20 17.71 99.82 251.16   What is happening here is called a vectorized operation: 1.61 is automatically recycled as many times as needed to be multiplied with each individual value in the dist_miles vector. This is a pretty unique and very useful feature of R!\nIn many other languages, we would need a loop or a similar construct to iterate over each value in the vector and multiply by 1.61. In fact, under the hood, R also uses a loop to do this! So does it even make a difference? Yes \u0026ndash; the advantages of using vectorization in R are:\n  You don\u0026rsquo;t have to write the loop, saving you a fair bit of typing and making the code clearer.\n  The under-the-hood-loop is being executed much faster than a loop that you would write. This is because it is written in C/C++ code which only has to be called once (instead of at least as many times as there are iterations in our loop).\n  Other vectorization patterns Above, we saw an example of multiplying a vector by a single number. We can also use vectorized operations when both objects contain multiple items. For instance, say we have a vector with corresponding values for two dates:\ndists_Mar4 \u0026lt;- c(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) dists_Mar5 \u0026lt;- c(87, 143, 103, 223, 106, 18, 87, 72, 59, 5)   To get the sum of these values at each position (index) of the two vectors (17 + 87, 93 + 143, etc.), we can simply do the following:\ndists_Mar4 + dists_Mar5 #\u0026gt; [1] 104 236 159 242 281 58 156 339 63 96    The two vectors don\u0026rsquo;t need to be of equal length, either:\nin the example below, we negate every other value in a vector:\nc(17, 93, 56, 19, 175, 40, 69, 267, 4, 91) * c(1, -1) #\u0026gt; [1] 17 -93 56 -19 175 -40 69 -267 4 -91     This also works for columns of a data frame, which we can extract using the dataframe_name$column_name notation (see Code Club session 9\u0026rsquo;s section on data frames, and the Base R data frame indexing summary below). Let\u0026rsquo;s say we wanted the mean distance this time:\ndist_df \u0026lt;- data.frame(dists_Mar4, dists_Mar5) dist_df$dists_mean = (dist_df$dists_Mar4 + dist_df$dists_Mar5) / 2 head(dist_df) #\u0026gt; dists_Mar4 dists_Mar5 dists_mean #\u0026gt; 1 17 87 52.0 #\u0026gt; 2 93 143 118.0 #\u0026gt; 3 56 103 79.5 #\u0026gt; 4 19 223 121.0 #\u0026gt; 5 175 106 140.5 #\u0026gt; 6 40 18 29.0   Vectorization with matrices Furthermore, we can also perform vectorized operations on entire matrices. With the following matrix:\n## We use the \"sample\" function to get 25 random values between 1 and a 100, ## and put those in a 5*5 matrix: mat \u0026lt;- matrix(sample(1:100, 25), nrow = 5, ncol = 5) mat #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 28 2 34 82 47 #\u0026gt; [2,] 92 96 18 93 73 #\u0026gt; [3,] 65 41 50 4 22 #\u0026gt; [4,] 19 36 86 24 75 #\u0026gt; [5,] 6 5 44 16 39   \u0026hellip;we could multiple all values by 10 or get the square of each value simply as follows:\nmat_more \u0026lt;- mat * 10 mat_more #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 430 880 600 310 10 #\u0026gt; [2,] 250 470 90 890 820 #\u0026gt; [3,] 860 990 780 630 730 #\u0026gt; [4,] 330 720 850 410 490 #\u0026gt; [5,] 440 160 870 640 900 mat_squared \u0026lt;- mat * mat mat_squared #\u0026gt; [,1] [,2] [,3] [,4] [,5] #\u0026gt; [1,] 1849 7744 3600 961 1 #\u0026gt; [2,] 625 2209 81 7921 6724 #\u0026gt; [3,] 7396 9801 6084 3969 5329 #\u0026gt; [4,] 1089 5184 7225 1681 2401 #\u0026gt; [5,] 1936 256 7569 4096 8100   Vectorization with indices We can also use vectorized solutions when we want to operate only on elements that satisfy a certain condition.\nLet\u0026rsquo;s say we consider any distance in one of our vectors that is below 50 to be insufficient, and we want to turn those values into negatives (a little harsh maybe, but we go with it).\nTo do so, we make use of R\u0026rsquo;s ability to index a vector with a logical vector:\n## \"not_far_enough\" will be a vector of logicals: not_far_enough \u0026lt;- dists_Mar4 \u0026lt; 50 not_far_enough #\u0026gt; [1] TRUE FALSE FALSE TRUE FALSE TRUE FALSE FALSE TRUE FALSE ## When we index the original vector with a logical vector, ## we get only those values for which \"not_far_enough\" is TRUE: dists_Mar4[not_far_enough] #\u0026gt; [1] 17 19 40 4   With the following syntax, we can replace just those low distances in our original vector:\ndists_Mar4[not_far_enough] \u0026lt;- dists_Mar4[not_far_enough] * -1 dists_Mar4 #\u0026gt; [1] -17 93 56 -19 175 -40 69 267 -4 91    In a simple case like this, we could also use the vectorized ifelse() function:\nifelse(dists_Mar5 \u0026lt; 50, dists_Mar5 * -1, dists_Mar5) #\u0026gt; [1] 87 143 103 223 106 -18 87 72 59 -5      II: For loops While it is important to use vectorization whenever possible, it can only be applied to a specific set of problems. A more universal solution when you need to repeat operations is the for loop. for loops iterate over a collection of values, allowing you to perform one or more actions for each value in the collection.\nThe basic syntax is as follows:\nfor (variable_name in collection_name) { #...do things for each item (variable_name) in the collection, one at a time... } On the first line, you initialize the for loop, telling it to assign each item in the collection to a variable (here, variable_name) one at a time.\nThe variable name is arbitrary, and the collection is whatever you want to loop over. However, for, the parentheses (), in, and the curly braces {} are all fixed elements of for loops. A simple example will help to understand the synax:\n## A loop to print negated values: for (one_number in c(1, 2, 3, 4)) \u0026#123; # We iterate over 1, 2, 3, 4 print(one_number * -1) # Multiply each number by -1 \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4   Note that we don\u0026rsquo;t have to use the variable that we are looping over: we could also use a for loop as a roundabout way to simply repeat something as many times as there are values in our collection:\nfor (dummy in c(1, 2, 3, 4)) \u0026#123; print(\"Yes!\") # Print \"Yes!\" in each of our four iterations  \u0026#125; #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\" #\u0026gt; [1] \"Yes!\"   As mentioned, the variable name that we assign is arbitrary: we could use anything, as long as we reference it with the same name inside the loop:\n## Example 1 with a different variable name: \"positive_number\" for (positive_number in c(1, 2, 3, 4)) \u0026#123; print(positive_number * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4 ## Example 2 with a different variable name: \"i\" for (i in c(1, 2, 3, 4)) \u0026#123; print(i * -1) \u0026#125; #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4   Note that the variable as it was last assigned in the loop does persist in your environment:\ni #\u0026gt; [1] 4   The curly braces are not strictly necessary for one-liners like this:\nfor (i in 1:4) print(i * -1) #\u0026gt; [1] -1 #\u0026gt; [1] -2 #\u0026gt; [1] -3 #\u0026gt; [1] -4   for loop output Note that we need the print() function to print anything to screen \u0026ndash; nothing will be printed if we omit this:\nfor (i in 1:4) \u0026#123; i * -1 \u0026#125;   Similarly, if we want the output to be saved in an object of some kind, we need to explicitly make an assignment in each iteration of the loop. This is where we need to start paying attention to the design of our loop. Unless computational speed is of no concern at all, you should avoid growing an object in each iteration of the loop.\nFor example, you might be inclined to do the following if you wanted to compute the median of each column in a data frame:\n## We initialize a vector in which we collect the column medians: column_medians \u0026lt;- vector() for (column_number in 1:ncol(dist_df)) \u0026#123; ## We extract one column using \"dataframe_name[[column_number]]\": column_median \u0026lt;- median(dist_df[[column_number]]) ## We add the single-column median to our vector of medians: column_medians \u0026lt;- c(column_medians, column_median) \u0026#125; column_medians #\u0026gt; [1] 62.50 87.00 78.75   Similarly, you may be tempted to add a column (with cbind()) or a row (with rbind()) to a data frame in each iteration of the loop. However, the problem with these approaches is that R has to create an entirely new object in each iteration of the loop, because the object\u0026rsquo;s memory requirements keep increasing.\nInstead, you\u0026rsquo;ll want to give the final vector (here, column_medians) the appropriate size before you start the loop:\ncolumn_medians \u0026lt;- vector(length = ncol(dist_df)) for (column_number in 1:ncol(dist_df)) \u0026#123; column_median \u0026lt;- median(dist_df[[column_number]]) column_medians[column_number] \u0026lt;- column_median \u0026#125;   Note that for very small problems, such as the example above, there will not be a noticeable difference in computation time between pre-assigning a properly sized object versus growing an object inside the loop. However, it is still good to get into the habit of pre-assigning an object of the right size.\nSummary guidelines (when speed is an issue)  Don\u0026rsquo;t use a loop when you can instead use vectorized operations. Don\u0026rsquo;t grow objects inside the loop. Instead, pre-assign an object large enough to contain all output of the loop and fill it in inside the loop. When you write a loop, avoid doing things inside the loop that don\u0026rsquo;t need to be repeated.  Learning about how to create your own functions and/or to use functional programming techniques like purrr and the apply family of functions (upcoming Code Club sessions!) will likely reduce your reliance on loops. For instance, as we\u0026rsquo;ll see next week, computing the median of each column in a data frame can be done much more succinctly with apply().\nEven for more experienced users, loops remain a more viable option when longer blocks of code need to be repeated: we will practice with that in the exercises.\n Breakout rooms! For the exercises, you can download an R Markdown file with some code to get set up (I recommend coding in that document to get a nice overview of the maps that you will make):\ndir.create('S12') todays_rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/12_loops/exercises.Rmd' download.file(url = todays_rmd, destfile = 'S12/exercises.Rmd')   The following code is already in your R Markdown file, which will download and read the bird dataset and the necessary packages:\n## Download the file with bird data: birds_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_sample_error.tsv' birds_file \u0026lt;- 'backyard-birds_sample_error.tsv' download.file(url = birds_url, destfile = birds_file)    ## Load the tidyverse: library(tidyverse) ## Read the file with bird data: birds \u0026lt;- read_tsv(birds_file) ## Load the maps package and get the state map: # install.packages('maps') # first install if necessary library(maps) states \u0026lt;- map_data(\"state\")   Last week, we learned about making maps. If you attended one of the first few Code Club sessions, you\u0026rsquo;ll recall our Great Backyard Birdcount data set. Here, we\u0026rsquo;ll use a country-wide random subset of this data (the full file is over 4 GB) to see where Carolina Chickadees were seen:\n## With this line, we select only the rows where the column \"species_en\" ## (English species name) equals \"Carolina Chickadee\", ## i.e. we are getting just the records for the Carolina Chickadee: caro_chickadee \u0026lt;- birds[birds$species_en == 'Carolina Chickadee', ] ## Or in tidyverse-speak: # caro_chickadee \u0026lt;- birds %\u0026gt;% filter(species_en == 'Carolina Chickadee') # Next, we create a map much like we did last week: ggplot(data = states, # Use \"states\" for underlying map mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + # Black state outlines, white fill geom_point(data = caro_chickadee, # Plot points from bird data set aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + # Green points, somewhat transparent coord_fixed(1.3) + # Fix projection labs(title = 'Carolina Chickadee')   Uh-oh! Something appears to have gone wrong. In the first exercise, you\u0026rsquo;ll use vectorization to fix the coordinates in the bird data set.\nIn the second exercise, you\u0026rsquo;ll use a loop to quickly produce similar plots for several other species.\n Exercise 1: Vectorization Try to fix the coordinates using vectorized operations, and recreate the map to see if it worked.\n Start with the latitude, which is wrong for all points.    Hints (click here)     You\u0026rsquo;ll need to modify the caro_chickadee data frame, while you can keep the plotting code exactly the same.\n  Simply prepending the latitude column with a minus sign ( -) will negate the values.\n  Equivalent base R ways to refer to the column with latitudes are caro_chickadee$lat and caro_chickadee[['lat']].\n     Solution (click here)  First we fix the latitude, which was simply negated:\ncaro_chickadee$lat \u0026lt;- -caro_chickadee$lat ## Or equivalently: # caro_chickadee[['lat']] \u0026lt;- -caro_chickadee[['lat']] ## Or a tidyverse way of doing this: # caro_chickadee \u0026lt;- caro_chickadee %\u0026gt;% mutate(lat = -lat)   Create the first map with the same code as the example:\nggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = caro_chickadee, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = 'Carolina Chickadee')     Once you have fixed the latitude, you should notice that for one state, there is a problem with the longitude (the offset is 10 decimal degrees).    Hints (click here)     The displaced state is North Carolina.\n  The state of each sighting is in the stateProvince column, and North Carolina\u0026rsquo;s name is simply \u0026ldquo;North Carolina\u0026rdquo; in that column.\n  It may help to first create a logical vector indicating whether for each row in the caro_chickadee data frame, stateProvincefor equals \u0026ldquo;North Carolina\u0026rdquo;.\n  Your final map will look nicer if you get rid of the plotting canvas by adding\n+ theme_void() to the code for the plot.\n     Solution (click here)  It turns out that North Carolina\u0026rsquo;s chickadees are above the Atlantic. Let\u0026rsquo;s perform a rescue operation by fixing the longitudes, which are offset by 10 degrees, just for North Carolina:\n## Get a vector of logicals, indicating which rows are from North Carolina: NC_rows \u0026lt;- caro_chickadee$stateProvince == \"North Carolina\" ## Only for North Carolina rows, change the longitude: caro_chickadee$long[NC_rows] \u0026lt;- caro_chickadee$long[NC_rows] - 10 ## Or: #caro_chickadee[NC_rows, 'long'] \u0026lt;- caro_chickadee[NC_rows, 'long'] - 10   And we create the final map:\nggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = caro_chickadee, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = 'Carolina Chickadee') + theme_void()   Nice!\n    Exercise 2: for loops Find the 10 most commonly observed bird species in the data set, and save their English names (found in the species_en column) in a vector.\nFeel free to check out the solution if you\u0026rsquo;re not sure how, because the focus here is on the next step: trying to create a loop.\n  Solution (click here)   top10 \u0026lt;- birds %\u0026gt;% count(species_en, sort = TRUE) %\u0026gt;% # Produces a sorted count table for \"species_en\" pull(species_en) %\u0026gt;% # Extracts the \"species_en\" column head(n = 10) # Take the top 10    Next, loop over the top-10 species to produce a plot for each one of them.\nStart with the code for the Carolina Chickadee, including the subsetting operation, and modify that.\n  Hints (click here)     In the subsetting operation where you select data for the focal species, replace \u0026ldquo;Carolina Chickadee\u0026rdquo; with whatever you name the variable (indicating an individual species) that you loop over.\nBecause this is a variable name, and not a string like \u0026ldquo;Carolina Chickadee\u0026rdquo;, don\u0026rsquo;t forget to omit the quotes.\n  You\u0026rsquo;ll also need to change the title with the looping variable.\n    Solution (click here) for (one_species in top10) \u0026#123; ## Select just the data for one species: one_bird_data \u0026lt;- birds[birds$species_en == one_species, ] ## Or in tidyverse-speak: # one_bird_data \u0026lt;- birds %\u0026gt;% filter(species_en == one_species) p \u0026lt;- ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = one_bird_data, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = one_species) + # Make sure to change this to the looping variable theme_void() print(p) \u0026#125;      Bonus exercise: if statements if statements are similar in syntax to for loops, and are also considered a \u0026ldquo;control flow\u0026rdquo; structure. But their purpose is different from loops: instead of iterating, if statements do something once and they only do it when a condition is fulfilled.\nFor instance, we may want to check in a script whether a certain directory (folder) exists on our computer, and if it doesn\u0026rsquo;t, then we create the directory:\nif (dir.exists('path/to/my/dir')) \u0026#123; warning(\"Oh my, the output directory doesn't exist yet!\") dir.create('path/to/my/dir') \u0026#125;   Inside the parentheses () after if should be a statement that evaluates to either TRUE or FALSE ( dir.exists() will be TRUE if the directory exists, and FALSE if it does not). If it is TRUE, whatever is inside the curly braces {} will be executed, and if it is FALSE, what is inside the curly braces will be ignored.\nif statements are commonly combined with for loops: we may want to only execute the functions in our loop for items in our collection that fulfill a certain condition:\nfor (one_number in 1:10) \u0026#123; if(one_number \u0026gt; 7) \u0026#123; print(one_number) \u0026#125; \u0026#125; #\u0026gt; [1] 8 #\u0026gt; [1] 9 #\u0026gt; [1] 10   In the example above, one number \u0026gt; 7 will only be TRUE for numbers larger than 7. This example is quite contrived, as it would have been easier (and faster!) to remove these items from the vector before the loop, but it hopefully gets the point across of how an if statement works.\n Many of the maps we produced in the previous exercise looked quite similar, with most species very widespread and a few restricted to the east of the US. Maybe if we select species that haven\u0026rsquo;t been seen in Ohio, we can find some other distributional patterns.\nFirst, select the the top 50 most observed bird species, just like you did in exercise 2.\nThen, use an if statement to create plots only for those top-50 birds that have not been seen in Ohio.\n  Solution (click here)   Select the top-50 birds:  all_species \u0026lt;- birds %\u0026gt;% count(species_en, sort = TRUE) %\u0026gt;% pull(species_en) %\u0026gt;% head(n = 50)    Loop over the species:  for (one_species in all_species) \u0026#123; ## Select the focal species: one_bird \u0026lt;- birds[birds$species_en == one_species, ] ## Create a data frame with only records from Ohio: one_bird_ohio \u0026lt;- one_bird[one_bird$stateProvince == 'Ohio', ] ## Test whether the data frame with only records from Ohio has any rows. ## If it does not, we create the map for the species in question:  if(nrow(one_bird_ohio) == 0) \u0026#123; p \u0026lt;- ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = one_bird, aes(x = long, y = lat, group = NULL), color = \"green4\", alpha = 0.5) + coord_fixed(1.3) + labs(title = one_species) + theme_void() print(p) \u0026#125; \u0026#125;       Going further  Base R data frame indexing Extract a column as a vector:\n## By name: birds$lat birds[['lat']] # Equivalent, $ notation is shorthand ## By index (column number): birds[[8]]   Extract one or more columns as a data frame using [row, column] notation,\nwith a leading comma ([, column]) meaning all rows:\n## By name: birds[, 'lat'] # dataframe['row_name', 'column_name'] birds[, c('lat', 'long')] ## By index (column numbers): birds[, 8] # dataframe[row_number, column_number] birds[, c(8, 9)]   Subset rows by a condition, with a trailing comma ([row, ]) meaning all columns:\nbirds[birds$lat \u0026gt; 25, ] birds[birds$species_en == 'Carolina Chickadee', ]      seq_along() To loop over column indices, we have used 1:ncol() above, and to loop over vector indices, you could similarly use 1:length().\nHowever, an alternative is seq_along(), which will create an index for you.\nbirds \u0026lt;- c('titmouse', 'chickadee', 'cardinal') seq_along(birds) #\u0026gt; [1] 1 2 3   The advantage of seq_along() is thtat it will behave better when your vector accidentally has length 0 (because 1:length() will have 1 and 0 when the length is 0, and you\u0026rsquo;ll get odd-seeming errors).\n  Further reading   Vectorization in R: Why? (Noam Ross, 2014) The Iteration chapter in Hadley Wickham\u0026rsquo;s R for Data Science (2017)  ","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614884947,"objectID":"0d1188d36cd786cdd66d047b20214d54","permalink":"https://biodash.github.io/codeclub/12_loops/","publishdate":"2021-03-01T00:00:00Z","relpermalink":"/codeclub/12_loops/","section":"codeclub","summary":"This is be the first of several sessions broadly about iterating in order to avoid copying-and-pasting of code, and today we will talk about vectorization and for loops.","tags":["codeclub","markdown","rmarkdown"],"title":"Session 12: Vectorization and loops in R","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"\n Prep and setup New to Code Club?   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot2? Check out the two Code Club pages for Session 4 and Session 5.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\nDownload the CSV file on Ohio ohio_csv \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/11_ggplot-maps/Ohio.csv' download.file(url = ohio_csv, destfile = 'Ohio.csv')    Creating maps in R Today, we will cover the visualization of spatial data in R using the layered grammar of graphics implementation of ggplot2 in conjunction with the contextual information of static maps from world maps in the maps package.\nBefore we look at mapping using ggplot2, let us define some terms.\nAreal data  Areal data is data which corresponds to geographical extents with polygonal boundaries.\nThe layered grammar of graphics  By definition, the layered grammar demands that every plot consist of five components: \n  a default dataset with aesthetic mappings,\n  one or more layers, each with either a geometric object (\u0026ldquo;geom\u0026rdquo;), a statistical transformation (\u0026ldquo;stat\u0026rdquo;), etc.\n  a scale for each aesthetic mapping (which can be automatically generated),\n  a coordinate system, and \n  a facet specification. \n  Since ggplot2 is an implementation of the layered grammar of graphics, every plot made with ggplot2 has each of the above elements. Consequently, map plots also have these elements, but certain elements are ﬁxed to map components: the x aesthetic is ﬁxed to longitude, the y aesthetic is ﬁxed to latitude.\nDrawing a map  Drawing a map in R requires two things. First, we have to draw the map using data that directs R to draw the polygon shapes that constitute the map. Then we add information to our map to plot color and marks. It\u0026rsquo;s the same basic logic that we have used in ggplot figures. The key thing is to have datasets that link that geographic data with the information that we want to put on the plot.\nThe maps package in R  The \u0026ldquo;maps\u0026rdquo; package in R contains a set of maps of the United States and the world drawn using longitude and latitude data. With world map, the USA map with the individual states you can accomplish a lot of the mapping tasks using the maps package. The maps package contains a lot of outlines of continents, countries, states, and counties\nMaking dataframes from map outlines by ggplot2  Recall that ggplot2 operates on dataframes. Therefore, we need some way to translate the maps data into a data frame format the ggplot can use. The package ggplot2 provides the map_data() function. The function turns a series of points along an outline into a data frame of those points. The package ggplot2 uses the following syntax: map_data(\u0026quot;name\u0026quot;) where \u0026ldquo;name\u0026rdquo; is a quoted string of the name of a map in the maps package.\nOur first maps Let us start by drawing maps of the World, USA, states, Ohio, Ohio and Indiana, and part of Europe using the maps package.\nlibrary(tidyverse) library(maps) library(scales) library(stringr) # Let us get a world map using the \"map_data\" function  world \u0026lt;- map_data(\"world\") ## Let us get a US map: usa \u0026lt;- map_data(\"usa\") # Let us get the states: states \u0026lt;- map_data(\"state\") # Select Ohio using the filter function: ohio \u0026lt;- states %\u0026gt;% filter(region == \"ohio\")    Let us plot a world map:  ggplot(data = world, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\")    Let us plot a map of the US:  ggplot(data = usa, mapping = aes(x = long, y= lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\")    Let us plot a map of the US with states:  ggplot(data = states, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"blue\", color = \"black\")    Let us plot a map of Ohio:  ggplot(data = ohio, mapping=aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"green\")    We can also plot a map for an arbitrary selection of states:  # We can select data for two states, for example Ohio and Indiana: ohio_indiana \u0026lt;- states %\u0026gt;% filter(region == \"ohio\" | region == \"indiana\") # Plot the map of Ohio and Indiana: ggplot(data = ohio_indiana, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"green\" , color = \"red\")    We can also plot only a specific region by filtering by latitude and longitude:  world \u0026lt;- map_data(\"world\") a_region \u0026lt;- filter(world, long \u0026gt;- 10 \u0026amp; long \u0026lt; 15.1 \u0026amp; lat \u0026gt; 32 \u0026amp; lat \u0026lt; 55) ggplot(data = a_region, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(fill = \"white\", color = \"black\") + coord_fixed(1.3)   The structure of the dataframe ohio. head(ohio) #\u0026gt; long lat group order region subregion #\u0026gt; 1 -80.51776 40.64563 42 10440 ohio \u0026lt;NA\u0026gt; #\u0026gt; 2 -80.55787 40.63990 42 10441 ohio \u0026lt;NA\u0026gt; #\u0026gt; 3 -80.62089 40.63417 42 10442 ohio \u0026lt;NA\u0026gt; #\u0026gt; 4 -80.66100 40.61698 42 10443 ohio \u0026lt;NA\u0026gt; #\u0026gt; 5 -80.66673 40.60552 42 10444 ohio \u0026lt;NA\u0026gt; #\u0026gt; 6 -80.67245 40.58833 42 10445 ohio \u0026lt;NA\u0026gt;   Look at the variables in ohio, note what they refer to: \n  long = longitude. Lines of longitude, or meridians, run between the North and South Poles and measure east-west positions. While prime meridian is assigned the value of 0 degrees, and runs through Greenwich (England), meridians to the west of the prime meridian are measured in degrees west (up to 180 degrees) and those to the east of the prime meridian are measured to in degrees east (up to 180 degrees).\n  lat = latitude. Lines of latitude measure north-south position between the poles with the equator defined as 0 degrees, the North Pole defined as 90 degrees north, and the South Pole defined as 90 degrees south. \n  group = an identifier that is unique for each subregion (here the counties). Group is very important! ggplot2\u0026rsquo;s functions can take a group argument which controls (amongst other things) whether adjacent points should be connected by lines. If they are in the same group, then they get connected, but if they are in different groups then they don\u0026rsquo;t. \n  order = an identifier that indicates the order in which the boundary lines should be drawn \n  region = string indicator for regions (here the states) \n  subregion = string indicator for sub-regions (here the county names) \n  Add information to the maps The second part of mapping in R, is to add information on the map created in the first part.  In drawing the map, the \u0026ldquo;maps\u0026rdquo; package creates the backbone for visualizations. Then we add additional information to show colors and shapes. \nWe will:  - fill a map by region,  - draw a Bubble map using city population,  - make a point for every city,  - vary size of point by city size and vary the color of the dots, and  - add external data to the map. \n Let us fill by region and make sure the the lines of state borders are white:  ggplot(data = states) + geom_polygon(aes(x = long, y = lat, fill = region, group = group), color = \"white\") + coord_fixed(1.3) + guides(fill = FALSE) # Do this to omit the legend    Let us draw a \u0026ldquo;Bubble map\u0026rdquo;:  # The maps package has city data head(maps::world.cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 'Abasan al-Jadidah Palestine 5629 31.31 34.34 0 #\u0026gt; 2 'Abasan al-Kabirah Palestine 18999 31.32 34.35 0 #\u0026gt; 3 'Abdul Hakim Pakistan 47788 30.55 72.11 0 #\u0026gt; 4 'Abdullah-as-Salam Kuwait 21817 29.36 47.98 0 #\u0026gt; 5 'Abud Palestine 2456 32.03 35.07 0 #\u0026gt; 6 'Abwein Palestine 3434 32.03 35.20 0 my_cities \u0026lt;- maps::world.cities usa_cities \u0026lt;- filter(my_cities,country.etc == \"USA\") head(usa_cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 Abilene USA 113888 32.45 -99.74 0 #\u0026gt; 2 Akron USA 206634 41.08 -81.52 0 #\u0026gt; 3 Alameda USA 70069 37.77 -122.26 0 #\u0026gt; 4 Albany USA 45535 44.62 -123.09 0 #\u0026gt; 5 Albany USA 75510 31.58 -84.18 0 #\u0026gt; 6 Albany USA 93576 42.67 -73.80 0    Make a point for every city:  ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_cities, color = \"red\", aes(x = long, y = lat, group = NULL))    Let\u0026rsquo;s pick just the big cities:  usa_big_cities \u0026lt;- filter(my_cities, country.etc == \"USA\" \u0026amp; pop \u0026gt; 500000) ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL))    Vary size of point by city size:  ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL, size = pop))    Now vary the color of the dots:  usa_big_cities$qual \u0026lt;- sample(LETTERS[1:5], nrow(usa_big_cities), replace = TRUE) ggplot(data = usa, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, aes(x = long, y = lat, group = NULL, color = qual, size = pop))    Tweak the map:  # No scientific notation in legend r ggplot: # scales package adds the \"scale_size_continuous\" function, and we can set label=comma library(scales) # Change the column name to make the legend nicer\" usa_big_cities$Population \u0026lt;- usa_big_cities$pop usa_big_cities$Qualitative \u0026lt;- usa_big_cities$qual # Do some additional refining: ggplot(data = usa, mapping = aes(x = long, y= lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = usa_big_cities, aes(x = long, y = lat, group = NULL, color = Qualitative, size = Population)) + scale_size_continuous(label = comma)    Work with Ohio counties with external data:  # Get basic map data for all USA counties: usa_counties = map_data(\"county\") # Subset to counties in Ohio: oh = subset(usa_counties, region == \"ohio\") head(oh) #\u0026gt; long lat group order region subregion #\u0026gt; 59960 -83.66902 39.02989 2012 59960 ohio adams #\u0026gt; 59961 -83.56590 39.02989 2012 59961 ohio adams #\u0026gt; 59962 -83.37109 39.06426 2012 59962 ohio adams #\u0026gt; 59963 -83.30806 39.06426 2012 59963 ohio adams #\u0026gt; 59964 -83.30233 39.05280 2012 59964 ohio adams #\u0026gt; 59965 -83.25649 39.01842 2012 59965 ohio adams   # Plot ohio counties ggplot() + geom_polygon(data = oh, aes(x = long, y = lat, group = group, fill = subregion), color = \"black\", alpha = 0.3) + coord_fixed(1.3) + guides(fill = FALSE) + ggtitle(\"Ohio counties\")    Read population data for Ohio counties:  # The data of the estimated population of each county in 2021 and percent change from 2010 Ohio \u0026lt;- read_csv(\"Ohio.csv\") head(Ohio) #\u0026gt; # A tibble: 6 x 6 #\u0026gt; county Pop Perc Town x_1 y_1 #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adams 27706 -2.91 West Union -83.5 38.8 #\u0026gt; 2 Allen 101603 -4.47 Lima -84.1 40.7 #\u0026gt; 3 Ashland 53040 -0.53 Ashland -82.3 40.9 #\u0026gt; 4 Ashtabula 96549 -4.79 Jefferson -80.8 41.7 #\u0026gt; 5 Athens 64943 -0.35 Athens -82.1 39.3 #\u0026gt; 6 Auglaize 45496 -0.88 Wapakoneta -84.2 40.6    Prepare the data for plotting:  # Create a new column called \"county\" so that counties start with capital letters # using str_to_title function  oh$county \u0026lt;- str_to_title(oh$subregion) # Merge population data with counties data by county variable using inner_join # function, and named the new object \"ohio_pop\" ohio_pop \u0026lt;- inner_join(oh, Ohio, by = \"county\") # Select counties with population greater than 100000 ohio_big_pop \u0026lt;- filter(ohio_pop, Pop \u0026gt; 100000)    Create the plot where we vary point size by population size:  ### vary size of point by population size ggplot(data = ohio_pop, mapping = aes(x = long, y = lat, group = group))+ geom_polygon(color = \"black\", fill = \"white\")+ geom_point(data = ohio_big_pop, color = \"red\", aes(x = x_1, y = y_1, group = NULL, size = Pop)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend    Improve the graph by creating groups of population using quantile.  ApplyQuintiles \u0026lt;- function(x) \u0026#123; cut(x, breaks = c(quantile(ohio_pop$Pop, probs = seq(0, 1, by = 0.2))), labels = c(\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"), include.lowest = TRUE) \u0026#125; ohio_pop$grouped_pop \u0026lt;- sapply(ohio_pop$Pop, ApplyQuintiles) head(ohio_pop) #\u0026gt; long lat group order region subregion county Pop Perc Town #\u0026gt; 1 -83.66902 39.02989 2012 59960 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 2 -83.56590 39.02989 2012 59961 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 3 -83.37109 39.06426 2012 59962 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 4 -83.30806 39.06426 2012 59963 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 5 -83.30233 39.05280 2012 59964 ohio adams Adams 27706 -2.91 West Union #\u0026gt; 6 -83.25649 39.01842 2012 59965 ohio adams Adams 27706 -2.91 West Union #\u0026gt; x_1 y_1 grouped_pop #\u0026gt; 1 -83.54616 38.79483 0-20 #\u0026gt; 2 -83.54616 38.79483 0-20 #\u0026gt; 3 -83.54616 38.79483 0-20 #\u0026gt; 4 -83.54616 38.79483 0-20 #\u0026gt; 5 -83.54616 38.79483 0-20 #\u0026gt; 6 -83.54616 38.79483 0-20    Plot the map:  ggplot() + geom_polygon(data = ohio_pop, aes(x = long, y = lat, group = group, fill = grouped_pop), color = \"black\") + coord_fixed(1.3) + scale_fill_brewer(palette = \"Reds\", direction = 1) + labs(fill = \"Population Quantiles\")   Breakout rooms Exercise 1 Use the dataset of 2021 Ohio county\u0026rsquo;s population to plot counties with % positive population growth.\n  Solution (click here)  # Get basic map data for all USA counties: usa_counties = map_data(\"county\") # Subset to counties in Ohio: oh = subset(usa_counties, region == \"ohio\") # Read population data: Ohio \u0026lt;- read_csv(\"Ohio.csv\") # Create a new column called \"county\" so that counties start with capital # letters using str_to_title function  oh$county = str_to_title(oh$subregion) # Merge counties with population: ohio_pop\u0026lt;-inner_join(oh, Ohio, by = \"county\") # Select counties with % positive population growth: ohio_pos_pop \u0026lt;- filter(ohio_pop, Perc \u0026gt; 0) ggplot(data = ohio_pop, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = ohio_pos_pop, color = \"red\", aes(x = x_1, y = y_1, group = NULL, size = Perc)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend     Exercise 2 Use the same data to plot counties with % negative population growth with quantile of 0-20, 20-40, 40-60, 60-80, and 80-100.\n  Solution (click here)  ohio_neg_pop \u0026lt;- filter(ohio_pop, Perc\u0026lt;0) ggplot(data = ohio_pop, mapping = aes(x= long, y= lat, group = group))+ geom_polygon(color=\"black\",fill=\"white\")+ geom_point(data = ohio_neg_pop, color = \"red\", aes(x = x_1,y = y_1, group = NULL, size = Perc)) + coord_fixed(1.3) + guides(size = FALSE) # Omit the legend   ApplyQuintiles_n \u0026lt;- function(x) \u0026#123; cut(x, breaks = c(quantile(ohio_neg_pop$Perc, probs = seq(0, 1, by = 0.2))), labels = c(\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"), include.lowest = TRUE) \u0026#125; ohio_neg_pop$grouped_pop \u0026lt;- sapply(ohio_neg_pop$Perc, ApplyQuintiles_n) # Plot the map ggplot() + geom_polygon(data = ohio_neg_pop, aes(x = long, y = lat, group = group, fill = grouped_pop), color = \"black\") + coord_fixed(1.3) + scale_fill_brewer(palette = \"Set1\", direction = -1) + labs(fill = \"Negative population growth counties\")     Bonus exercise Plot the cities of France with population greater than 100,000. Vary size of point by city size, and vary the color of the dots.\n  Solution (click here)  world \u0026lt;- map_data(\"world\") france \u0026lt;- filter(world,region == \"France\") ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + labs(fill = \"France\")  # The \"maps\" package has city data head(maps::world.cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 'Abasan al-Jadidah Palestine 5629 31.31 34.34 0 #\u0026gt; 2 'Abasan al-Kabirah Palestine 18999 31.32 34.35 0 #\u0026gt; 3 'Abdul Hakim Pakistan 47788 30.55 72.11 0 #\u0026gt; 4 'Abdullah-as-Salam Kuwait 21817 29.36 47.98 0 #\u0026gt; 5 'Abud Palestine 2456 32.03 35.07 0 #\u0026gt; 6 'Abwein Palestine 3434 32.03 35.20 0 my_cities \u0026lt;-maps::world.cities france_cities \u0026lt;- filter(my_cities, country.etc == \"France\") head(france_cities) #\u0026gt; name country.etc pop lat long capital #\u0026gt; 1 Abbeville France 26656 50.12 1.83 0 #\u0026gt; 2 Acheres France 23219 48.97 2.06 0 #\u0026gt; 3 Agde France 23477 43.33 3.46 0 #\u0026gt; 4 Agen France 34742 44.20 0.62 0 #\u0026gt; 5 Aire-sur-la-Lys France 10470 50.64 2.39 0 #\u0026gt; 6 Aix-en-Provence France 148622 43.53 5.44 0 # Make a point for every city: ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_cities, color = \"red\", aes(x = long, y = lat, group = NULL))   # Let's pick just the big cities: france_big_cities \u0026lt;- filter(my_cities,country.etc == \"France\" \u0026amp; pop \u0026gt; 100000) ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL))   # vary size of point by city size ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, color = \"red\", aes(x = long, y = lat, group = NULL, size = pop))   # Now vary the color of the dots france_big_cities$qual \u0026lt;- sample(LETTERS[1:5], nrow(france_big_cities), replace = TRUE) ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\",fill = \"white\") + geom_point(data = france_big_cities, aes(x = long, y = lat, group = NULL, color = qual, size = pop))   # Do some tweaking: # no scientific notation in legend r ggplot # scales package adds the \"scale_size_continuous\" function to our arsenal, and we can set label=comma library(scales) # Change the column name to make the legend nicer: france_big_cities$Population \u0026lt;- france_big_cities$pop france_big_cities$Qualitative \u0026lt;- france_big_cities$qual # Do some additional refining: ggplot(data = france, mapping = aes(x = long, y = lat, group = group)) + geom_polygon(color = \"black\", fill = \"white\") + geom_point(data = france_big_cities, aes(x = long, y = lat, group = NULL, color = Qualitative, size = Population)) + scale_size_continuous(label = comma)     ","date":1614124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614293945,"objectID":"29ed38d8f1680daed1e667b71019651a","permalink":"https://biodash.github.io/codeclub/11_ggplot-maps/","publishdate":"2021-02-24T00:00:00Z","relpermalink":"/codeclub/11_ggplot-maps/","section":"codeclub","summary":"Today, we will be making cool maps in R!","tags":null,"title":"Session 11: Spatial data visualization with ggplot2","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll help you troubleshoot.\n  New to ggplot? Check out the two Code Club pages for Session 4 and Session 5.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\n Getting Started RMarkdown for today\u0026rsquo;s session # directory for Code Club Session 2: dir.create(\"S10\") # directory for our RMarkdown # (\"recursive\" to create two levels at once.) dir.create(\"S10/Rmd/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/10_faceting-animating/Session10_faceting_animating_multifigs.Rmd' # indicate the name of the new script file Session10_Rmd \u0026lt;- \"S10/Rmd/Session10_faceting_animating_multifigs.Rmd\" # go get that file!  download.file(url = todays_R_script, destfile = Session10_Rmd)   1 - How can I do more with my figures? Artwork by Allison Horst\nSometimes we have so much data that it is difficult to make sense of it if we look at it all at once. One way to get around this is to create facets in your data \u0026ndash; small subplots that help you to see different relationships among different variables in your dataset.\nToday we will be using ggplot2 to make a series of plots that help us better understand the underlying structure in our dataset.\n What will we go over today\nThese functions or packages will help you to get better visualize with your data.\n  facet_wrap() and facet_grid()- makes small multiple plots based on some variable. set scales to indicate the linked or not-linked nature of your axes in a faceted plot  gghighlight() - allows you to direct focus on a particular portion of your data.  patchwork - to compose super easy multi-plot figures  gganimate - to make your plots gif!  I will also go over a few tricks along the way.\n   2 - Accessing our data Let\u0026rsquo;s get set up and grab some data so that we can learn more about this world (and ggplot2)\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries. We are using a lot of new packages today.\nIf you\u0026rsquo;ve never downloaded these packages before, use the chunk below.\ninstall.packages(c(\"gghighlight\", \"gganimate\", \"magick\", \"patchwork\", \"ggrepel\", \"gapminder\"))  Once you have the packages above downloaded, load your libraries.\nlibrary(tidyverse) library(gghighlight) # for bringing attention to certain parts of your plot library(gganimate) # for animating library(magick) # for rendering gifs and saving them library(patchwork) # for making multi-panel plots library(ggrepel) # for getting labels to not be on top of your points # data for today library(gapminder)  Then let\u0026rsquo;s access the dataset gapminder, which is both the name of the package, and the name of the dataset. It contains a subset of data from Gapminder.org, an educational non-profit aimed to fight global misconceptions about statistics of our world.\nFrom Gapminder.org\nLet\u0026rsquo;s look at the data in gapminder.\n# look at the first 6 rows, all columns head(gapminder) #\u0026gt; # A tibble: 6 x 6 #\u0026gt; country continent year lifeExp pop gdpPercap #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Afghanistan Asia 1952 28.8 8425333 779. #\u0026gt; 2 Afghanistan Asia 1957 30.3 9240934 821. #\u0026gt; 3 Afghanistan Asia 1962 32.0 10267083 853. #\u0026gt; 4 Afghanistan Asia 1967 34.0 11537966 836. #\u0026gt; 5 Afghanistan Asia 1972 36.1 13079460 740. #\u0026gt; 6 Afghanistan Asia 1977 38.4 14880372 786. # check the structure # this tell us what is contained within our df glimpse(gapminder) #\u0026gt; Rows: 1,704 #\u0026gt; Columns: 6 #\u0026gt; $ country \u0026lt;fct\u0026gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… #\u0026gt; $ continent \u0026lt;fct\u0026gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… #\u0026gt; $ year \u0026lt;int\u0026gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… #\u0026gt; $ lifeExp \u0026lt;dbl\u0026gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… #\u0026gt; $ pop \u0026lt;int\u0026gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… #\u0026gt; $ gdpPercap \u0026lt;dbl\u0026gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,…  This dataset contains the following measurements about the life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. It includes the following variables:\n country continent year lifeExp pop gdpPercap  Note, this data is already in tidy-style format meaning:\n Each variable must have its own column. Each observation must have its own row. Each value must have its own cell.  Learn more about tidy data here.\nTo make things a bit less complex, let\u0026rsquo;s look at data just from the Americas (i.e., North and South America). To do that, we can use filter() like we learned using dplyr back in Code Club Session 2\n# make a df with data only from the Americas gapminder_americas \u0026lt;- gapminder %\u0026gt;% filter(continent == \"Americas\") # what countries do we have? unique(gapminder_americas$country) #\u0026gt; [1] Argentina Bolivia Brazil  #\u0026gt; [4] Canada Chile Colombia  #\u0026gt; [7] Costa Rica Cuba Dominican Republic  #\u0026gt; [10] Ecuador El Salvador Guatemala  #\u0026gt; [13] Haiti Honduras Jamaica  #\u0026gt; [16] Mexico Nicaragua Panama  #\u0026gt; [19] Paraguay Peru Puerto Rico  #\u0026gt; [22] Trinidad and Tobago United States Uruguay  #\u0026gt; [25] Venezuela  #\u0026gt; 142 Levels: Afghanistan Albania Algeria Angola Argentina Australia ... Zimbabwe   3 - Life expectancy vs. time We will plot the relationship between lifeExp and year with the goal of understanding how life expectancy has changed in the second half of the 20th century. We will use geom_line() to make a line plot.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, group = country, color = country)) + geom_line()   This plot has so many countries, and we can only visually distinguish colors so well, that it makes this a bit of a mess. We can do better!\n 4 - Highlighting What if we want to highlight one country of interest, with the backdrop of all the data in the Americas? We can do this using gghighlight(), which will distinguish our country of interest, from the rest of the countries which will be indicated in gray.\nLet\u0026rsquo;s highlight the United States, and since we are at it, let\u0026rsquo;s also add x and y axis labels, a title, subtitle, and caption with labs().\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, group = country, color = country)) + geom_line() + gghighlight(country == \"United States\") + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\") #\u0026gt; Warning: Tried to calculate with group_by(), but the calculation failed. #\u0026gt; Falling back to ungrouped filter operation... #\u0026gt; label_key: country    5 - Faceting What if we want to see all the data at once, but just be able to better attribute each line to the correct country? We can use the principle of small multiples, popularized by Edward Tufte, to make a series of charts all on the same scale to allow comparison between them easily.\nWe can facet using facet_wrap to create small plots for each country. If you want a certain number of rows or columns you can indicate them by including ncol and nrow in the facet_wrap() statement.\nI have also made the strip text, or the label on top of each of the facets bigger using theme.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp, color = country)) + geom_line() + theme(strip.text.x = element_text(size = 14)) + facet_wrap(vars(country)) + # facet_wrap(~country) also works labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   Now our legend is not necessary, so let\u0026rsquo;s remove it. Let\u0026rsquo;s also remove the gray background since its not really doing much for us. We will also change to theme_minimal() to get rid of the grey background which I don\u0026rsquo;t think we need.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   Wow better! But now its a bit hard to contextualize the line for each country to the whole dataset. We can fix this too.\n 6 - Highlighting plus faceting Let\u0026rsquo;s bring the rest of data back in, and highlight in each facet the country of interest. We can do this by just adding gghighlight() to our ggplot call.\nNote: if you want to assign something in R to an object, and then view it, you can put the whole thing in parentheses, without having to call that object back at the end.\n(americas_lifeexp \u0026lt;- gapminder_americas %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_line(aes(color = country)) + gghighlight() + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Life Expectancy (years)\", title = \"Life Expectancy in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")) #\u0026gt; label_key: country #\u0026gt; Too many data series, skip labeling   Wow, we now have so much more information about our data!\n 7 - Adjusting scales while faceting The default in faceting is that the x and y-axes for each plot are all the same. This aids in the interpretation of each small plot in relation to the others, but sometimes you may want freedom to adjust your axes.\nFor example, if we wanted to plot population over time, if we used the same scale, it would be really hard to see trends within a country.\n(americas_pop \u0026lt;- gapminder_americas %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country) + labs(x = \"Year\", y = \"Population\", title = \"Population in Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\"))   Let\u0026rsquo;s change the scales so that the y-axis is \u0026ldquo;free\u0026rdquo; - i.e., each plot will have an independent y-axis. Note, when you do this, you aren\u0026rsquo;t really using the principle of small multiples anymore, since the data isn\u0026rsquo;t all on comparable scales.\ngapminder_americas %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_line(aes(color = country)) + theme_minimal() + theme(legend.position = \"none\", strip.text.x = element_text(size = 14)) + facet_wrap(~country, scales = \"free_y\") + labs(x = \"Year\", y = \"Population\", title = \"Population of Countries in the Americas\", subtitle = \"From 1952 to 2007\", caption = \"Data from gapminder.org\")   The default for scales is \u0026quot;fixed\u0026quot;, but you can also set to be \u0026quot;free_x\u0026quot;, \u0026quot;free_y\u0026quot;, or \u0026quot;free\u0026quot;, which means both x and y are free.\n 8 - Multi-panel plots What if I take plots I\u0026rsquo;ve already made and assemble them together? You can do that simply with the package patchwork().\nArtwork by Allison Horst\nYou can use the syntax:\n plot1 + plot2 to get two plots next to each other plot1 / plot2 to get two plots stacked vertically plot1 | (plot2 + plot3) to get plot1 in the first row, and plots 2 and 3 in a second row  You can use plot_annotation() to indicate your plots with letters or numbers.\nI am going to make some quick plots so we can see how it works. Let\u0026rsquo;s look at some plots of the United States.\n# make df with just United States data gapminder_usa \u0026lt;- gapminder %\u0026gt;% filter(country == \"United States\") # make some plots (usa_lifeexp \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = lifeExp)) + geom_point())   (usa_gdppercap \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = gdpPercap)) + geom_line())   (usa_pop \u0026lt;- gapminder_usa %\u0026gt;% ggplot(aes(x = year, y = pop)) + geom_col())   Let\u0026rsquo;s make multi-panel plots. If you need to wrap around a line, make sure you don\u0026rsquo;t start your line with the +, it won\u0026rsquo;t work (this is true for all ggplot2 syntax).\n(usa_lifeexp + usa_gdppercap) / usa_pop + plot_annotation(title = \"Some plots about the United States\", tag_levels = \"A\")   You can see how this would be really useful for publications!\n 9 - Animating Artwork by Allison Horst\nSince we have time-scale data here, we could also build an animation that would help us look at our data. What if we wanted to look at how life expectancy (lifeExp) and population (pop) change over time? We could animate over the variable year, and do this by using the function animate(), and set transition_states() to the variable we are giffing over.\nNote, I have included closest_state in the subtitle so the viewer can see what is the year at any stage of the animation.\nTo be able to tell which dot belongs to which country, I added a geom_text_repel() statement, which labels each point but is smart enough to not let the labels overlap.\nI have also set pop to be on a log10 scale.\nIf you want to increase the resolution of your gif, and set the code chunk to cache = TRUE if the chunk runs slowly, so that it doesn\u0026rsquo;t re-run when knitting if nothing has been edited, you can do this in the curly brackets at the top of your chunk, like this:\n{r, cache = TRUE, dpi = 600}\n# install.packages(\"transformr\")  # if you are having problems with gganimate you may need to install transformr (p \u0026lt;- ggplot(gapminder_americas, aes(x = lifeExp, y = pop, fill = country, label = country)) + geom_point(shape = 21, color = \"black\") + geom_text_repel() + scale_y_log10() + theme_classic() + theme(legend.position = 'none') + labs(title = \"Population and Life Expectancy in the Americas\", subtitle = 'Year: \u0026#123;closest_state\u0026#125;', x = \"Life Expectancy\", y = \"Log10 Population\") + transition_states(year))  There are many different ways to transition your data in gganimate - and you can learn more about them here.\nSaving my gif\nNow I want to save my gif. We can do that simply with the function anim_save() which works a lot like ggsave().\n# set parameters for your animation animate(p, duration = 10, fps = 10, width = 700, height = 700, renderer = magick_renderer()) # save it anim_save(filename = \"gapminder_gif.gif\", animation = last_animation(), path = \"/Users/jessicacooperstoneimac\")   10 - Breakout rooms Loading data and get set up Load the palmerpenguins dataset, look at its structure, and view the beginning of the df.\nlibrary(palmerpenguins) str(penguins) #\u0026gt; tibble [344 × 8] (S3: tbl_df/tbl/data.frame) #\u0026gt; $ species : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #\u0026gt; $ island : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ... #\u0026gt; $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #\u0026gt; $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #\u0026gt; $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #\u0026gt; $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... #\u0026gt; $ sex : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ... #\u0026gt; $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ... head(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt;   Main exercises Exercise 1  Like we did in Code Club 7, convert the two columns about penguin bill dimensions bill_length_mm and bill_depth_mm to two columns called bill_dimension and value. Drop your NAs also. Save this as a new df called penguins_long.\n  Hints (click here)  Use a combination of drop_na() and pivot_longer(), and it\u0026rsquo;s helpful if you also set names_prefix in your pivot_longer() statement but not totally necessary.    Solutions (click here)  penguins_long \u0026lt;- penguins %\u0026gt;% drop_na() %\u0026gt;% pivot_longer(cols = bill_length_mm:bill_depth_mm, names_to = \"bill_dimension\", values_to = \"value_mm\", names_prefix = \"bill_\") head(penguins_long) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island flipper_length_… body_mass_g sex year bill_dimension #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Adelie Torge… 181 3750 male 2007 length_mm  #\u0026gt; 2 Adelie Torge… 181 3750 male 2007 depth_mm  #\u0026gt; 3 Adelie Torge… 186 3800 fema… 2007 length_mm  #\u0026gt; 4 Adelie Torge… 186 3800 fema… 2007 depth_mm  #\u0026gt; 5 Adelie Torge… 195 3250 fema… 2007 length_mm  #\u0026gt; 6 Adelie Torge… 195 3250 fema… 2007 depth_mm  #\u0026gt; # … with 1 more variable: value_mm \u0026lt;dbl\u0026gt;      Exercise 2  Plot body mass (body_mass_g) as related to bill length and depth.\n  Hints (click here)  Faceting will be useful here.    Solutions (click here)  penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point() + facet_wrap(vars(bill_dimension))       Exercise 3  Take your plot from Exercise 2 and make it prettier. You can do things like change your axis labels, add title, change themes as you see fit. Color your points by sex.\n  Hints (click here)  Pick a theme you like. theme_classic() is a good place to start, and if you want to download the package hrbrthemes, I really like the theme_ipsum_rc().\n   Solutions (click here)  I\u0026rsquo;ve included some code that let\u0026rsquo;s you re-name the strip text, or the text that is above each of your facets. You do this with the labeller() function.\nlibrary(hrbrthemes) # for pretty \u0026amp; easy themes # formatting facet strip text labels dim_mm \u0026lt;- c(\"Culman Bill Depth\", \"Culman Bill Length\") names(dim_mm) \u0026lt;- c(\"depth_mm\", \"length_mm\") # this is just one example penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm, color = sex)) + geom_point() + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5), strip.text = element_text(hjust = 0.5)) + labs(x = \"Body Mass (g)\", y = \"mm\", title = \"Bill length and depth vs. body mass in penguins\", color = \"Sex\", caption = \"Data from https://allisonhorst.github.io/palmerpenguins/\") + facet_wrap(vars(bill_dimension), labeller = labeller(bill_dimension = dim_mm))       Exercise 4  Add a second dimension of faceting by species.\n  Hints (click here)  You do this within your facet_wrap() call. You might want to try the formula syntax, which works like this: var1~var2.    Solutions (click here)  penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm, color = sex)) + geom_point() + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5), strip.text = element_text(hjust = 0.5)) + labs(x = \"Body Mass (g)\", y = \"mm\", title = \"Bill length and depth vs. body mass in penguins\", color = \"Sex\", caption = \"Data from https://allisonhorst.github.io/palmerpenguins/\") + facet_wrap(bill_dimension~species, labeller = labeller(bill_dimension = dim_mm))       Exercise 5  Using your plot from Exercise 3, highlight the datapoints coming from Dream Island in purple.\n  Hints (click here)  You can use syntax inside gghighlight() just like you do in filter().    Solutions (click here)  # what are our islands? unique(penguins_long$island) #\u0026gt; [1] Torgersen Biscoe Dream  #\u0026gt; Levels: Biscoe Dream Torgersen penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point(color = \"purple\") + gghighlight(island == \"Dream\") + facet_wrap(vars(bill_dimension))       Exercise 6  Using your sample plot for Exercise 3, highlight penguins that have a body_mass_g less than 3500 g, in blue.\n  Hints (click here)  You can use syntax inside gghighlight() just like you do in filter(), and you can also use these filter functions like \u0026lt;, \u0026gt;, \u0026lt;=, ! and AND inside your call.    Solutions (click here)  # what are our islands? unique(penguins_long$island) #\u0026gt; [1] Torgersen Biscoe Dream  #\u0026gt; Levels: Biscoe Dream Torgersen penguins_long %\u0026gt;% ggplot(aes(x = body_mass_g, y = value_mm)) + geom_point(color = \"blue\") + gghighlight(island == \"Dream\") + facet_wrap(vars(bill_dimension))       Bonus exercises Bonus 1  Plot flipper_length_mm vs. body_mass_g and animate the plot to show only one species at a time.\n  Hints (click here)  Try animating over species, using transition_states() and set {closest_state} in your title or subtitle so you can tell what you\u0026rsquo;re looking at.    Solutions (click here)  flipper_by_BW \u0026lt;- penguins %\u0026gt;% ggplot(aes(x = body_mass_g, y = flipper_length_mm, fill = species)) + geom_point(shape = 21, color = \"black\") + theme_classic() + theme(legend.position = 'none') + labs(title = \"Population and Life Expectancy in the Americas\", subtitle = 'Penguin Species: \u0026#123;closest_state\u0026#125;', x = \"Body Mass (g)\", y = \"Flipper Length (mm)\") + transition_states(species) animate(flipper_by_BW)      Bonus 2  You have now made an excellent gif, so save it!\n  Hints (click here)  Use anim_save() to save your animation, which works in a similar way to ggsave(), which is another very useful function.    Solutions (click here)  # set parameters for your animation animate(flipper_by_BW, duration = 10, fps = 10, width = 700, height = 700, renderer = magick_renderer()) # save it anim_save(filename = \"flippers_by_mass.gif\", animation = last_animation(), path = \"YOUR_PATH_HERE\")      Bonus 3  Let\u0026rsquo;s practice making multi-panel plots. Plot:\nBoxplot of body_mass_g by sex\nHistogram of number of observations per island\nDistribution of flipper_length_mm by species.\nTag your plots so each has a lowercase letter associated with it.\n  Hints (click here)  Use the syntax from the package patchwork. You can learn more here. Also use plot_annotation().    Solutions (click here)   title allows you to set a title tag_levels allows you to determine how you want your panels to be tagged.  Boxplot of body_mass_g by sex.\npenguins_mass_by_sex \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(x = sex, y = body_mass_g)) + geom_boxplot() penguins_mass_by_sex   Histogram of number of observations per island.\npenguins_by_island \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(y = island, fill = island)) + geom_histogram(stat = \"count\") #\u0026gt; Warning: Ignoring unknown parameters: binwidth, bins, pad penguins_by_island   Distribution of flipper_length_mm by species.\npenguins_flipper_species \u0026lt;- penguins_long %\u0026gt;% ggplot(aes(x = flipper_length_mm, group = species, fill = species)) + geom_density(alpha = 0.5) + scale_fill_viridis_d() penguins_flipper_species   penguins_flipper_species / (penguins_mass_by_sex + penguins_by_island) + plot_annotation(title = \"Looking at penguins...\", tag_levels = \"a\")       ","date":1613520000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613604588,"objectID":"3abf1bebd3e7269759e016e707cae871","permalink":"https://biodash.github.io/codeclub/10_faceting-animating/","publishdate":"2021-02-17T00:00:00Z","relpermalink":"/codeclub/10_faceting-animating/","section":"codeclub","summary":"During this tenth session of Code Club, we will be continuing to work on making visualizations using ggplot2, including faceting plots, making multi-panel figures, and animating charts.","tags":null,"title":"Session 10: Faceting, multi-plots, and animating","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"  Image from https://r4ds.had.co.nz    New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the uses of R\u0026rsquo;s three basic subsetting operators: [ ], [[ ]], and $. Learn how the behavior of these operators varies depending on the data structure you are subsetting (vector, matrix, list, or data frame). Prepare to learn how these resemble, and differ from, subsetting operators in Python.   Intro: What is \u0026lsquo;subsetting\u0026rsquo; anyway? Subsetting (also known as indexing) is simply a formal way of pulling out specific pieces of a data structure. We\u0026rsquo;ve already seen two dplyr verbs that perform this kind of operation for tibbles: filter (to pull out specific rows) and select (to pull out specific columns).\nBut these are tidyverse commands, and only work with tibbles. R has two more-basic data structures, vectors and lists, and for these we need different subsetting operators. We\u0026rsquo;ll also see that matrices are simply a special kind of vector, that data frames are a special kind of list, and basic subsetting operators also work for these.\nSince the behavior of these operators depends on the actual data structure you are working with, it\u0026rsquo;s useful when experimenting to use them in conjunction with the str() function, which compactly displays the internal structure of an arbitrary R object. A knowledge of the make-up of these data structures is also important when you come to write your own loops, iterations, and functions.\nThe most important distinction between vectors and lists is that vectors are homogeneous, while lists can be heterogeneous.\n Terminological note: \u0026lsquo;under-the-hood\u0026rsquo; both of these are vectors in the technical sense, and sometimes the distinction is referred to as atomic vectors versus recursive vectors. I\u0026rsquo;ll continue to use just \u0026lsquo;vector\u0026rsquo; and \u0026lsquo;list\u0026rsquo; here. This usage also lines-up with Python.\n   Vectors A vector is absolutely the most basic data structure in R. Every value in a vector must be of the same type. Strikingly, this sets R apart from Python. Using this kind of vector in Python requires loading a whole separate package: numpy. The most basic data structure in Python is the list.\nThere are four basic types of vector: integer, double, character, and logical. Vectors are created by hand with the c() (combine, concatenate) function. We can check the type with the typeof() operator. This is totally redundant if you just created the vector yourself, but when you are debugging code or creating a vector using an expression you might want to check exactly what type of vector is being used:\nvec_dbl \u0026lt;- c(1, 2, 3, 4, 5) typeof(vec_dbl) #\u0026gt; [1] \"double\"   vec_int \u0026lt;- c(1L, 2L, 3L, 4L, 5L) typeof(vec_int) #\u0026gt; [1] \"integer\"   vec_which \u0026lt;- seq(1, 10) typeof(vec_which) #\u0026gt; [1] \"integer\"   vec_which2 \u0026lt;- 1:10 typeof(vec_which2) #\u0026gt; [1] \"integer\"   vec_log \u0026lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE) typeof(vec_log) #\u0026gt; [1] \"logical\"   vec_chr \u0026lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\") typeof(vec_chr) #\u0026gt; [1] \"character\"   What happens when we perform a basic mathematical operation on a vector?\n2 * vec_int #\u0026gt; [1] 2 4 6 8 10   (This is completely different than what you obtain when multiplying a Python list).\nL = [1, 2, 3, 4, 5] 2 * L # [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] So it\u0026rsquo;s not just that vectors are a basic R data structure, but R is a vectorised language. In many cases applying an operation to a vector automagically applies the operation to every element in the vector. This means that for many basic operations for loops and mapping functions, necessary in Python, are not needed in R (although if you write your own functions you will need these iteration tools). In Python we could use a \u0026lsquo;list comprehension\u0026rsquo; (a compact and fast version of a for loop):\n[2 * i for i in L] # [2, 4, 6, 8, 10] Or install the numpy package that makes vectors and vectorized functions available.\nVectors have an insanely simple structure:\nstr(vec_dbl) #\u0026gt; num [1:5] 1 2 3 4 5    str() also displays the type, and RStudio displays the result of str() in the Values pane.\nFor such a simple structure, there are a surprisingly large number of ways to subset a vector. We\u0026rsquo;ll use the following example:\nx \u0026lt;- c(2.1, 4.2, 3.3, 5.4)   (Notice that the number after the decimal point indicates the position (index) of the element of the vector.)\nPositive integers return elements at the specified positions. Any expression that evaluates to a vector of positive integers can be used as the index. The index operator is []:\nx[3] #\u0026gt; [1] 3.3 x[c(3, 1)] #\u0026gt; [1] 3.3 2.1 x[2:4] #\u0026gt; [1] 4.2 3.3 5.4 x[seq(1, 4, 2)] #\u0026gt; [1] 2.1 3.3   (In R the indices run from 1 to the length of the object: in Python indices run from 0 to length-1).\nNegative integers exclude elements at the specified positions:\nx[-3] #\u0026gt; [1] 2.1 4.2 5.4 x[-c(3, 1)] #\u0026gt; [1] 4.2 5.4 #\u0026gt; [1] 4.2 5.4   Logical vectors select elements where the corresponding logical value is TRUE. This is most useful if you can write a comparison expression 2 \u0026gt; 3, 4 == 4, that returns TRUE (or FALSE) for each element of the vector:\nx[c(TRUE, TRUE, FALSE, FALSE)] #\u0026gt; [1] 2.1 4.2 x[x \u0026gt; 3] #\u0026gt; [1] 4.2 3.3 5.4   Attributes. One of the unusual features of R as opposed to Python is that you can assign metadata of various kinds to the elements of vectors (and lists). For example, we can assign a name to each element, and then use a character vector as the index expression:\ny \u0026lt;- c(a = 2.1, b = 4.2, c = 3.3, d = 5.4) str(y) #\u0026gt; Named num [1:4] 2.1 4.2 3.3 5.4 #\u0026gt; - attr(*, \"names\")= chr [1:4] \"a\" \"b\" \"c\" \"d\" y[c(\"d\", \"c\", \"a\")] #\u0026gt; d c a  #\u0026gt; 5.4 3.3 2.1   (Again, Python has no direct equivalent of this, but we can get a similar effect using a dictionary data structure, which explicitly assigns a name to each value).\nNothing ([]) returns the entire vector:\nx[] #\u0026gt; [1] 2.1 4.2 3.3 5.4   This is not useful for (one dimensional) vectors, but is behind the notation for extracting rows and columns from matrices. Keep in mind a \u0026ldquo;nothing\u0026rdquo; index returns \u0026ldquo;everything\u0026rdquo;.\nMatrices A matrix is simply a vector with a dimensions attribute. Here we convert a vector to a two-dimensional matrix, with two rows and three columns, with dim(rows, cols).\nz \u0026lt;- c(2.1, 4.2, 3.3, 5.4, 8.5, 1.6) dim(z) \u0026lt;- c(2, 3) z #\u0026gt; [,1] [,2] [,3] #\u0026gt; [1,] 2.1 3.3 8.5 #\u0026gt; [2,] 4.2 5.4 1.6   Now we can index a specific value using comma notation, where the first index specifies the row, and the second index the column (in Python this is reversed):\nz[2,3] #\u0026gt; [1] 1.6   And in two dimensions the nothing after the , returns every column for that row:\nz[2,] #\u0026gt; [1] 4.2 5.4 1.6   And here is a way of selecting a submatrix (every row for all columns except the first):\nz[,-1] #\u0026gt; [,1] [,2] #\u0026gt; [1,] 3.3 8.5 #\u0026gt; [2,] 5.4 1.6   Lists There are two main differences between vectors and lists: (i) lists can contain elements of different types; and (ii) lists can contain other lists. This is why lists are sometimes referred to as recursive vectors. We will see examples of these below, but first let\u0026rsquo;s directly compare a list of numbers to a vector of numbers, and examine the structure. Lists are constructed with the list() function.\nl \u0026lt;- list(2.1, 4.2, 3.3, 5.4) l #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4.2 #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.3 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 5.4 str(l) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ : num 4.2 #\u0026gt; $ : num 3.3 #\u0026gt; $ : num 5.4   Here we see the appearance of a new subsetting operator [[ ]]. What does it yield?\nll_2 \u0026lt;- l[[2]] ll_2 #\u0026gt; [1] 4.2 typeof(ll_2) #\u0026gt; [1] \"double\"   Now compare this to the result of using the [ ] operator:\nl_2 \u0026lt;- l[2] l_2 #\u0026gt; [[1]] #\u0026gt; [1] 4.2 typeof(l_2) #\u0026gt; [1] \"list\" str(l_2) #\u0026gt; List of 1 #\u0026gt; $ : num 4.2   The behavior of the [ ] operator is very different for lists: it selects the element(s) you request, but it always returns a subsetted version of the original list. It \u0026lsquo;shrinks\u0026rsquo; the original list. There is nothing like this in Python; it\u0026rsquo;s quite unique to R. (The reason this is the case will become clear when we examine data frames.) The [[ ]] operator on the other hand just returns the object you select.\nAs mentioned above, it\u0026rsquo;s quite possible that an element of a list might itself be a list:\nm \u0026lt;- list(2.1, list(4.21, 4.22), 3.3, 5.4) m #\u0026gt; [[1]] #\u0026gt; [1] 2.1 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [[2]][[1]] #\u0026gt; [1] 4.21 #\u0026gt;  #\u0026gt; [[2]][[2]] #\u0026gt; [1] 4.22 #\u0026gt;  #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.3 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [1] 5.4   This (print) output focuses on content, whereas the str() function focuses on structure, and is very useful for nested lists:\nstr(m) #\u0026gt; List of 4 #\u0026gt; $ : num 2.1 #\u0026gt; $ :List of 2 #\u0026gt; ..$ : num 4.21 #\u0026gt; ..$ : num 4.22 #\u0026gt; $ : num 3.3 #\u0026gt; $ : num 5.4   Once we combine nested lists and multiple types, things can get pretty hairy. There are various ways to visualize what\u0026rsquo;s going on. Here is one example:\nx1 \u0026lt;- list(c(1, 2), c(3, 4)) x2 \u0026lt;- list(list(1, 2), list(3, 4)) x3 \u0026lt;- list(1, list(2, list(3)))   However the printed form provides us a clue on how to extract an individual element from inside a nested list:\n# m \u0026lt;- list(2.1, list(4.21, 4.22), 3.3, 5.4) m[[2]] #\u0026gt; [[1]] #\u0026gt; [1] 4.21 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4.22   m[[2]][[1]] #\u0026gt; [1] 4.21   In short, [[ ]] drills down into a list, while [ ] returns a diminished version of the original list.\nHere\u0026rsquo;s a visualization of various list subsetting operations:\na \u0026lt;- list(1:3, \"a string\", pi, list(-1, -5)) a #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a string\" #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 3.141593 #\u0026gt;  #\u0026gt; [[4]] #\u0026gt; [[4]][[1]] #\u0026gt; [1] -1 #\u0026gt;  #\u0026gt; [[4]][[2]] #\u0026gt; [1] -5 str(a) #\u0026gt; List of 4 #\u0026gt; $ : int [1:3] 1 2 3 #\u0026gt; $ : chr \"a string\" #\u0026gt; $ : num 3.14 #\u0026gt; $ :List of 2 #\u0026gt; ..$ : num -1 #\u0026gt; ..$ : num -5   Subsetting a list, visually. Here is a recursive pepper shaker, p\nHere is the first packet, but still inside the shaker, p[1]\nHere is the first packet by itself, p[[1]]\nAnd here is the contents of that packet, p[[1]][[1]]\nWe\u0026rsquo;ll play with yet another type of visualization in the exercises.\nData frames Let\u0026rsquo;s look at a simple data frame:\ndf \u0026lt;- data.frame(x = 1:3, y = c(\"a\", \"b\", \"c\")) typeof(df) #\u0026gt; [1] \"list\" str(df) #\u0026gt; 'data.frame': 3 obs. of 2 variables: #\u0026gt; $ x: int 1 2 3 #\u0026gt; $ y: chr \"a\" \"b\" \"c\"   So a data frame is basically a list (of columns), with a names attribute for the column names; and with the extra condition that all the columns are of the same length. So we should be able to use our standard list subsetting operators on it:\ndf_col_1 \u0026lt;- df[1] str(df_col_1) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ x: int 1 2 3   Since a data frame is a list, subsetting using [ ] returns the specified column still inside a data frame. What about [[ ]]?\ncol_1 \u0026lt;- df[[1]] str(col_1) #\u0026gt; int [1:3] 1 2 3   Using [[ ]] returns the individual column.\nWe can also subset a data frame by name:\ndf_name \u0026lt;- df[\"x\"] str(df_name) #\u0026gt; 'data.frame': 3 obs. of 1 variable: #\u0026gt; $ x: int 1 2 3   df_nname \u0026lt;- df[[\"x\"]] str(df_nname) #\u0026gt; int [1:3] 1 2 3   Finally df$x is simply a shorthand for df[[\u0026quot;x\u0026quot;]] without the [[ ]] and the \u0026quot; \u0026quot;:\ndf_dollar_name \u0026lt;- df$x str(df_dollar_name) #\u0026gt; int [1:3] 1 2 3   Just as a matter of interest, in the grand scheme of things lists are just a special kind of vector (a \u0026lsquo;heterogeneous recursive vector\u0026rsquo;), so it should be possible to stick a list column into a data frame. We can, but we have to use the I \u0026lsquo;identity\u0026rsquo; operator around the list:\ndf_mixed \u0026lt;- data.frame( x = 1:3, y = I(list(4L, 7.2, \"string\"))) str(df_mixed) #\u0026gt; 'data.frame': 3 obs. of 2 variables: #\u0026gt; $ x: int 1 2 3 #\u0026gt; $ y:List of 3 #\u0026gt; ..$ : int 4 #\u0026gt; ..$ : num 7.2 #\u0026gt; ..$ : chr \"string\" #\u0026gt; ..- attr(*, \"class\")= chr \"AsIs\"   Further reading and acknowledgement For more details on subsetting see R for Data Science and Advanced R both by Hadley Wickham, from which much of the material in this module was borrowed.\n Exercise 1 A surprisingly useful operator for extracting elements of a numerical vector is the modulo operator x %% y. This returns the remainder when x is divided by y. It is a vectorized operation, so we can apply it to a list.\nx \u0026lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x %% 3 #\u0026gt; [1] 1 2 0 1 2 0 1 2 0 1   Use this operator to extract every third element of the above vector x.\n  Hints (click here)  \nCheck the example in the presentation about selecting elements when the logical comparison is TRUE. What is the logical test we need to identify every third element?    Solution (click here)  x \u0026lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) x[x %% 3 == 0] #\u0026gt; [1] 3 6 9      Exercise 2 Here is a visualization of a list:\nCreate a list in R called train that captures this structure. Print the list, and also display its structure.\n  Hints (click here)  \nThis list has no nested lists, it\u0026rsquo;s just a list of vectors and individual values.    Solution (click here)  train \u0026lt;- list(1:3, \"a\", 4:6) train #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a\" #\u0026gt;  #\u0026gt; [[3]] #\u0026gt; [1] 4 5 6 str(train) #\u0026gt; List of 3 #\u0026gt; $ : int [1:3] 1 2 3 #\u0026gt; $ : chr \"a\" #\u0026gt; $ : int [1:3] 4 5 6      Exercise 3 For each of the following sub-trains/carriages, determine the subsetting expression by eye, and then check that it works by subsetting your train list from exercise 1.\n  Hints (click here)  There's more than one way to do these; you will have to use both `[ ]` and `[[ ]]` operators. The last two are tricky, experiment with them...    Solution (click here)  train[1] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3   train[[1]] #\u0026gt; [1] 1 2 3   train[1:2] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] \"a\"   train[-2] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 4 5 6   train[c(1, 1)] #\u0026gt; [[1]] #\u0026gt; [1] 1 2 3 #\u0026gt;  #\u0026gt; [[2]] #\u0026gt; [1] 1 2 3   train[0] #\u0026gt; list()      Exercise 4 A common use of recursive structures in biology is to represent phylogenetic trees. Create a recursive list in R called tree which captures the following visual representation\n  Hints (click here)  Start at the top and work down. Start with a simpler subtree, then expand terminals. Alternatively, start at the bottom with the smallest subtree, then work up, adding sisters into parent nodes.\nIn either case, check your working with str() as you incrementally add structure.\nNotice this is a binary branching tree, so the root node of every subtree should contain two elements.\nOne of the tricks with these nested lists is to keep track of paired parentheses\u0026hellip;\nStay calm and recurse.    Solution (click here)  tree \u0026lt;- list(\"a\", list(list(\"b\", \"c\"), \"d\")) str(tree) #\u0026gt; List of 2 #\u0026gt; $ : chr \"a\" #\u0026gt; $ :List of 2 #\u0026gt; ..$ :List of 2 #\u0026gt; .. ..$ : chr \"b\" #\u0026gt; .. ..$ : chr \"c\" #\u0026gt; ..$ : chr \"d\"      \n","date":1612915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613152528,"objectID":"1f0acc316d61e12ddf0b12b66dfb0a7c","permalink":"https://biodash.github.io/codeclub/09_subsetting/","publishdate":"2021-02-10T00:00:00Z","relpermalink":"/codeclub/09_subsetting/","section":"codeclub","summary":"In this session of Code Club, we'll move below and beyond the tidyverse to get an overview of subsetting various kinds of data structure in R.","tags":null,"title":"Session 9: Subsetting","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":" Session Goals  Describe differences in long data vs wide data. Identify scenarios where it might be helpful to have data in one format over another (longer vs. wider). Use the functions pivot_longer() and pivot_wider() to reshape data. Use NHANES data to address whether blood pressure values vary in a predictable way with successive measurements.   Intro: The Shape Of A Dataset A single set of data can sometimes be stored in different ways, or in other words, it can have different shapes. Below is a small example. It\u0026rsquo;s a hypothetical dataset that stores the number of visitors at each of two parks over a long weekend, and we\u0026rsquo;ll look at two different versions of it\u0026hellip;\nWide Format #create the dataset visitors_wide \u0026lt;- data.frame(\"park\" = c(\"north_park\", \"south_park\"), \"Fri\" = c(65, 80), \"Sat\" = c(184, 160), \"Sun\" = c(135, 140), \"Mon\" = c(87, 71)) #view the dataset visitors_wide #\u0026gt; park Fri Sat Sun Mon #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   Long Format #create the dataset visitors_long \u0026lt;- data.frame(\"park\" = rep(c(\"north_park\", \"south_park\"), 4), \"day\" = c(\"Fri\",\"Fri\",\"Sat\",\"Sat\",\"Sun\",\"Sun\",\"Mon\",\"Mon\"), \"visitors\" = c(65,80,184,160,135,140,87,71)) #view the dataset visitors_long #\u0026gt; park day visitors #\u0026gt; 1 north_park Fri 65 #\u0026gt; 2 south_park Fri 80 #\u0026gt; 3 north_park Sat 184 #\u0026gt; 4 south_park Sat 160 #\u0026gt; 5 north_park Sun 135 #\u0026gt; 6 south_park Sun 140 #\u0026gt; 7 north_park Mon 87 #\u0026gt; 8 south_park Mon 71   Notice that both datasets store the same information - it\u0026rsquo;s just formatted differently. These two datasets can be said to have different shapes. The first has a wider shape - it has more columns, stretching it out from left to right. The second has a longer shape, as it has fewer columns and more rows. Again, importantly, both datasets store the same information.\nWhat Shape Should Your Data Be In? The best answer to the question of what shape your data should be in is probably something like \u0026lsquo;Whatever shape makes it easiest to accomplish your goals with the data at any given time\u0026rsquo;. For example, sometimes when you\u0026rsquo;re entering data - say in to a spreadsheet in Excel or a similar program, you might find the data entry process easier if the dataset is in a wider format. In contrast, longer formats will generally be better when analyzing your data. This is consistent with the idea of tidy data we talked about in Session 2. For example, tidy data will be long because a characteristic of tidy data is that each variable has its own column. For these reasons, you might find it helpful or even necessary to reshape the data - possibly multiple times as you continue to work with the same dataset.\nHow To Reshape Data R offers several approaches for reshaping data. Functions for doing so often come in pairs that transform from wider to longer, and longer to wider, respectively. Pairs of functions include cast() and melt(), spread() and gather(), and pivot_longer() and pivot_wider(). While any of these can be used, we\u0026rsquo;ll focus on the \u0026lsquo;pivot\u0026rsquo; pair that come from the package tidyr, as they were written most recently with a goal of being the most user-friendly of the available functions so far.\nPivoting Resources If you want to dig in to pivoting a bit more, R offers a very useful vignette on pivoting, which is worth a look - portions of today\u0026rsquo;s breakout sessions will come from there. Chapter 12 of \u0026ldquo;R For Data Science\u0026rdquo; by Wickham and Grolemund, which covers tidy data, also includes a nice section on pivoting.\n Examples Let\u0026rsquo;s revisit the park visitors dataset for an example of how pivot_longer() and pivot_wider() work in their most basic form. Previously, I created each of the wide and long forms of this dataset by hand. It was manageable to do that, since it\u0026rsquo;s a very small dataset, but for most datasets, you\u0026rsquo;re not going to want to just recreate a data frame from scratch each time you need to reshape the data. Let\u0026rsquo;s start with the data in wide format\u0026hellip;\n#view the data frame visitors_wide #\u0026gt; park Fri Sat Sun Mon #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   What if we wanted to plot the total mean number of visitors per day across both parks? To get the mean values, we might think about applying some of the functions we\u0026rsquo;ve been working with in previous sessions like group_by() and summarize(). For example, we might want to try grouping by day and then calculating the means from a column that stores the number of visitors. However, in it\u0026rsquo;s current wide form, this dataset doesn\u0026rsquo;t have the day and visitors columns we need. pivot_longer() can help us here. The command might look like this\u0026hellip;\nlibrary(tidyverse) visitors_longer \u0026lt;- visitors_wide %\u0026gt;% pivot_longer(-park, names_to = \"day\", values_to = \"visitors\")   First, we need to point it to the dataset we\u0026rsquo;re interested in reshaping - I\u0026rsquo;m doing that by piping the visitors_wide data frame to pivot_longer(). Next, we need to specify what columns to use to lengthen the dataset. This argument recognizes tidy-select notation, which can really simplify things. Here, I\u0026rsquo;m using -park, which tells it to use all the column names except park. Those column names will be transformed to values in a single new column, which needs a name. We\u0026rsquo;ll call it day, so names_to = \u0026quot;day\u0026quot;. Finally, the values in the current columns will be stacked in to a single column, and it too needs a name, so values_to = \u0026quot;visitors\u0026quot;. This lengthens the dataset, taking it from 5 columns down to 3.\n#view the data visitors_longer #\u0026gt; # A tibble: 8 x 3 #\u0026gt; park day visitors #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 north_park Fri 65 #\u0026gt; 2 north_park Sat 184 #\u0026gt; 3 north_park Sun 135 #\u0026gt; 4 north_park Mon 87 #\u0026gt; 5 south_park Fri 80 #\u0026gt; 6 south_park Sat 160 #\u0026gt; 7 south_park Sun 140 #\u0026gt; 8 south_park Mon 71   In this longer format, we\u0026rsquo;re able to apply the group_by() and summarize() functions\u0026hellip;\nvisitors_longer %\u0026gt;% group_by(day) %\u0026gt;% summarise(\"mean\" = mean(visitors)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 4 x 2 #\u0026gt; day mean #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Fri 72.5 #\u0026gt; 2 Mon 79  #\u0026gt; 3 Sat 172  #\u0026gt; 4 Sun 138.   And we can go in the opposite direction with pivot_wider()\u0026hellip;\nvisitors_longer %\u0026gt;% pivot_wider(names_from = day, values_from = visitors) #\u0026gt; # A tibble: 2 x 5 #\u0026gt; park Fri Sat Sun Mon #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 north_park 65 184 135 87 #\u0026gt; 2 south_park 80 160 140 71   The examples above represent the most basic uses of pivot_longer() and pivot_wider(). But each of these functions offer additional arguments that can help deal with more complicated situations. The next example is from the pivoting vignette I referenced above. It uses the billboard dataset that should already be available in your R session, and that stores weekly rankings of Billboard top 100 songs from the year 2000.\n#preview billboard data head(billboard) #\u0026gt; # A tibble: 6 x 79 #\u0026gt; artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA #\u0026gt; 2 2Ge+h… The … 2000-09-02 91 87 92 NA NA NA NA NA #\u0026gt; 3 3 Doo… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 #\u0026gt; 4 3 Doo… Loser 2000-10-21 76 76 72 69 67 65 55 59 #\u0026gt; 5 504 B… Wobb… 2000-04-15 57 34 25 17 17 31 36 49 #\u0026gt; 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 #\u0026gt; # … with 68 more variables: wk9 \u0026lt;dbl\u0026gt;, wk10 \u0026lt;dbl\u0026gt;, wk11 \u0026lt;dbl\u0026gt;, wk12 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk13 \u0026lt;dbl\u0026gt;, wk14 \u0026lt;dbl\u0026gt;, wk15 \u0026lt;dbl\u0026gt;, wk16 \u0026lt;dbl\u0026gt;, wk17 \u0026lt;dbl\u0026gt;, wk18 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk19 \u0026lt;dbl\u0026gt;, wk20 \u0026lt;dbl\u0026gt;, wk21 \u0026lt;dbl\u0026gt;, wk22 \u0026lt;dbl\u0026gt;, wk23 \u0026lt;dbl\u0026gt;, wk24 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk25 \u0026lt;dbl\u0026gt;, wk26 \u0026lt;dbl\u0026gt;, wk27 \u0026lt;dbl\u0026gt;, wk28 \u0026lt;dbl\u0026gt;, wk29 \u0026lt;dbl\u0026gt;, wk30 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk31 \u0026lt;dbl\u0026gt;, wk32 \u0026lt;dbl\u0026gt;, wk33 \u0026lt;dbl\u0026gt;, wk34 \u0026lt;dbl\u0026gt;, wk35 \u0026lt;dbl\u0026gt;, wk36 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk37 \u0026lt;dbl\u0026gt;, wk38 \u0026lt;dbl\u0026gt;, wk39 \u0026lt;dbl\u0026gt;, wk40 \u0026lt;dbl\u0026gt;, wk41 \u0026lt;dbl\u0026gt;, wk42 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk43 \u0026lt;dbl\u0026gt;, wk44 \u0026lt;dbl\u0026gt;, wk45 \u0026lt;dbl\u0026gt;, wk46 \u0026lt;dbl\u0026gt;, wk47 \u0026lt;dbl\u0026gt;, wk48 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk49 \u0026lt;dbl\u0026gt;, wk50 \u0026lt;dbl\u0026gt;, wk51 \u0026lt;dbl\u0026gt;, wk52 \u0026lt;dbl\u0026gt;, wk53 \u0026lt;dbl\u0026gt;, wk54 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk55 \u0026lt;dbl\u0026gt;, wk56 \u0026lt;dbl\u0026gt;, wk57 \u0026lt;dbl\u0026gt;, wk58 \u0026lt;dbl\u0026gt;, wk59 \u0026lt;dbl\u0026gt;, wk60 \u0026lt;dbl\u0026gt;, #\u0026gt; # wk61 \u0026lt;dbl\u0026gt;, wk62 \u0026lt;dbl\u0026gt;, wk63 \u0026lt;dbl\u0026gt;, wk64 \u0026lt;dbl\u0026gt;, wk65 \u0026lt;dbl\u0026gt;, wk66 \u0026lt;lgl\u0026gt;, #\u0026gt; # wk67 \u0026lt;lgl\u0026gt;, wk68 \u0026lt;lgl\u0026gt;, wk69 \u0026lt;lgl\u0026gt;, wk70 \u0026lt;lgl\u0026gt;, wk71 \u0026lt;lgl\u0026gt;, wk72 \u0026lt;lgl\u0026gt;, #\u0026gt; # wk73 \u0026lt;lgl\u0026gt;, wk74 \u0026lt;lgl\u0026gt;, wk75 \u0026lt;lgl\u0026gt;, wk76 \u0026lt;lgl\u0026gt;   Notice there are columns named \u0026lsquo;wk1\u0026rsquo; through \u0026lsquo;wk73\u0026rsquo; that store the weekly ranking for each song. Week itself is a variable with values that could be represented in a single column. We could do something similar to our above use of pivot_longer()\u0026hellip;\nbillboard %\u0026gt;% pivot_longer(cols = starts_with(\"wk\"), names_to = \"week\", values_to = \"rank\") %\u0026gt;% head() #\u0026gt; # A tibble: 6 x 5 #\u0026gt; artist track date.entered week rank #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk1 87 #\u0026gt; 2 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk2 82 #\u0026gt; 3 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk3 72 #\u0026gt; 4 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk4 77 #\u0026gt; 5 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk5 87 #\u0026gt; 6 2 Pac Baby Don't Cry (Keep... 2000-02-26 wk6 94   This is a start - we\u0026rsquo;ve gone from 79 columns to just 6. But we can clean this up a bit more. Notice the values in the new week column all include the \u0026lsquo;wk\u0026rsquo; prefix. Since we\u0026rsquo;ve labeled the column \u0026lsquo;week\u0026rsquo;, it\u0026rsquo;s kind of redundant and unnecessary to have \u0026lsquo;wk\u0026rsquo; at the beginning of each value. We can add the \u0026lsquo;names_prefix\u0026rsquo; argument, which accepts a regular expression (regex). Characters at the beginning of column names that match the regex get removed.\nbillboard %\u0026gt;% pivot_longer(cols = starts_with(\"wk\"), names_to = \"week\", values_to = \"rank\", names_prefix = \"wk\") %\u0026gt;% head() #\u0026gt; # A tibble: 6 x 5 #\u0026gt; artist track date.entered week rank #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 2 Pac Baby Don't Cry (Keep... 2000-02-26 1 87 #\u0026gt; 2 2 Pac Baby Don't Cry (Keep... 2000-02-26 2 82 #\u0026gt; 3 2 Pac Baby Don't Cry (Keep... 2000-02-26 3 72 #\u0026gt; 4 2 Pac Baby Don't Cry (Keep... 2000-02-26 4 77 #\u0026gt; 5 2 Pac Baby Don't Cry (Keep... 2000-02-26 5 87 #\u0026gt; 6 2 Pac Baby Don't Cry (Keep... 2000-02-26 6 94   We haven\u0026rsquo;t dealt with regular expressions in Code Club yet - they\u0026rsquo;ll make a good topic for a future session, but if you\u0026rsquo;re interested in the meantime, I did a couple short videos introducing them as part of this set of videos on command line computing.\nBreakout Rooms In the breakout rooms, we\u0026rsquo;ll use a pivot function to analyze a portion of the NHANES dataset. We\u0026rsquo;ll use the data to try to address whether successive blood pressure measurements from the same individual differ in a predictable way.\nIf you haven\u0026rsquo;t already done it, you can install the NHANES dataset with\u0026hellip;\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/s7/y_mgh3c54h9fjcyw9wqdkb8x4zs_jy/T//RtmpWxvWIv/downloaded_packages   Exercise 1  First let\u0026rsquo;s load and preview the NHANES dataset.\n  Hints (click here)  \nUse library() to load the dataset. The functions head() are glimpse() are a couple good options for previewing the data.    Solution (click here)  library(NHANES) glimpse(NHANES) #\u0026gt; Rows: 10,000 #\u0026gt; Columns: 76 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 516… #\u0026gt; $ SurveyYr \u0026lt;fct\u0026gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, male, male, male, female, male, male, female, … #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10,… #\u0026gt; $ AgeDecade \u0026lt;fct\u0026gt; 30-39, 30-39, 30-39, 0-9, 40-49, 0-9, 0-9, 4… #\u0026gt; $ AgeMonths \u0026lt;int\u0026gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795… #\u0026gt; $ Race1 \u0026lt;fct\u0026gt; White, White, White, Other, White, White, White, Whi… #\u0026gt; $ Race3 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, High School, High School, NA, Some Coll… #\u0026gt; $ MaritalStatus \u0026lt;fct\u0026gt; Married, Married, Married, NA, LivePartner, NA, NA, … #\u0026gt; $ HHIncome \u0026lt;fct\u0026gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, … #\u0026gt; $ HHIncomeMid \u0026lt;int\u0026gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 875… #\u0026gt; $ Poverty \u0026lt;dbl\u0026gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00… #\u0026gt; $ HomeRooms \u0026lt;int\u0026gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3… #\u0026gt; $ HomeOwn \u0026lt;fct\u0026gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, … #\u0026gt; $ Work \u0026lt;fct\u0026gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, … #\u0026gt; $ Weight \u0026lt;dbl\u0026gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7… #\u0026gt; $ Length \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HeadCirc \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.… #\u0026gt; $ BMICatUnder20yrs \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ BMI_WHO \u0026lt;fct\u0026gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plu… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, … #\u0026gt; $ BPSysAve \u0026lt;int\u0026gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111,… #\u0026gt; $ BPDiaAve \u0026lt;int\u0026gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, … #\u0026gt; $ BPSys1 \u0026lt;int\u0026gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124,… #\u0026gt; $ BPDia1 \u0026lt;int\u0026gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, … #\u0026gt; $ BPSys2 \u0026lt;int\u0026gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108,… #\u0026gt; $ BPDia2 \u0026lt;int\u0026gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, … #\u0026gt; $ BPSys3 \u0026lt;int\u0026gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114,… #\u0026gt; $ BPDia3 \u0026lt;int\u0026gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, … #\u0026gt; $ Testosterone \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ DirectChol \u0026lt;dbl\u0026gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, … #\u0026gt; $ TotChol \u0026lt;dbl\u0026gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, … #\u0026gt; $ UrineVol1 \u0026lt;int\u0026gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113,… #\u0026gt; $ UrineFlow1 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1… #\u0026gt; $ UrineVol2 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ UrineFlow2 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Diabetes \u0026lt;fct\u0026gt; No, No, No, No, No, No, No, No, No, No, No, No, No, … #\u0026gt; $ DiabetesAge \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HealthGen \u0026lt;fct\u0026gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vg… #\u0026gt; $ DaysPhysHlthBad \u0026lt;int\u0026gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0… #\u0026gt; $ DaysMentHlthBad \u0026lt;int\u0026gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA… #\u0026gt; $ LittleInterest \u0026lt;fct\u0026gt; Most, Most, Most, NA, Several, NA, NA, None, None, N… #\u0026gt; $ Depressed \u0026lt;fct\u0026gt; Several, Several, Several, NA, Several, NA, NA, None… #\u0026gt; $ nPregnancies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, … #\u0026gt; $ nBabies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, N… #\u0026gt; $ Age1stBaby \u0026lt;int\u0026gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ SleepHrsNight \u0026lt;int\u0026gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, … #\u0026gt; $ SleepTrouble \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, … #\u0026gt; $ PhysActive \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes,… #\u0026gt; $ PhysActiveDays \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2,… #\u0026gt; $ TVHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ CompHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ TVHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, … #\u0026gt; $ CompHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, … #\u0026gt; $ Alcohol12PlusYr \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ AlcoholDay \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA,… #\u0026gt; $ AlcoholYear \u0026lt;int\u0026gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, … #\u0026gt; $ SmokeNow \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA,… #\u0026gt; $ Smoke100 \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No,… #\u0026gt; $ Smoke100n \u0026lt;fct\u0026gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smok… #\u0026gt; $ SmokeAge \u0026lt;int\u0026gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, … #\u0026gt; $ Marijuana \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Y… #\u0026gt; $ AgeFirstMarij \u0026lt;int\u0026gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, … #\u0026gt; $ RegularMarij \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes… #\u0026gt; $ AgeRegMarij \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, … #\u0026gt; $ HardDrugs \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes,… #\u0026gt; $ SexEver \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ SexAge \u0026lt;int\u0026gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, … #\u0026gt; $ SexNumPartnLife \u0026lt;int\u0026gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA,… #\u0026gt; $ SexNumPartYear \u0026lt;int\u0026gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1… #\u0026gt; $ SameSex \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, … #\u0026gt; $ SexOrientation \u0026lt;fct\u0026gt; Heterosexual, Heterosexual, Heterosexual, NA, Hetero… #\u0026gt; $ PregnantNow \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …       Exercise 2  As you might know, blood pressure consists of two values - systolic and diastolic. Each participant in the NHANES survey had their blood pressure measured three times in succession, giving us the columns: BPSys1, BPDia1, BPSys2, BPDia2, BPSys3, BPDia3. Let\u0026rsquo;s work first with just the three systolic values.\nSubset the dataset to get just the columns BPSys1, BPSys2, and BPSys3. Name the new object \u0026lsquo;sys_values\u0026rsquo;, then get the dimensions of sys_values and preview it.\n  Hints (click here)  \nUse select() from dplyr to get the three columns we want. dim() and glimpse() can be used to get the dimensions and preview the data, respectively.    Solution (click here)  sys_values \u0026lt;- NHANES %\u0026gt;% select(matches(\"BPSys[123]$\")) #I used the 'matches' helper along with a regular expression  #above, but there are a number of ways you could do this.  #One equivalent would be... # sys_values \u0026lt;- NHANES %\u0026gt;% select(BPSys1, BPSys2, BPSys3) dim(sys_values) #\u0026gt; [1] 10000 3 head(sys_values) #\u0026gt; # A tibble: 6 x 3 #\u0026gt; BPSys1 BPSys2 BPSys3 #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 114 114 112 #\u0026gt; 2 114 114 112 #\u0026gt; 3 114 114 112 #\u0026gt; 4 NA NA NA #\u0026gt; 5 118 108 116 #\u0026gt; 6 84 84 88       Exercise 3  We can see just from the preview in Exercise 2 that the dataset has some missing data - let\u0026rsquo;s remove rows that have NA\u0026rsquo;s. Call the new dataset \u0026lsquo;sys_noNA\u0026rsquo;. Then check the dimensions and preview again.\n  Hints (click here)  \nTry the drop_na function from tidyr to eliminate rows containing missing data.    Solution (click here)  sys_noNA \u0026lt;- sys_values %\u0026gt;% drop_na() dim(sys_noNA) #\u0026gt; [1] 7971 3 head(sys_noNA) #\u0026gt; # A tibble: 6 x 3 #\u0026gt; BPSys1 BPSys2 BPSys3 #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 114 114 112 #\u0026gt; 2 114 114 112 #\u0026gt; 3 114 114 112 #\u0026gt; 4 118 108 116 #\u0026gt; 5 84 84 88 #\u0026gt; 6 114 108 106       Exercise 4  We\u0026rsquo;ll explore these data a bit to see if there\u0026rsquo;s any evidence of a trend in systolic blood pressure with respect to the sequence of measurements (differences among measurements 1, 2, and 3). First, lets reshape the data so we end up with just two columns named \u0026lsquo;measurement\u0026rsquo; and \u0026lsquo;sys_bp\u0026rsquo;. Save the new objects as \u0026lsquo;sys_long\u0026rsquo;. Then check the dimensions and preview again.\n  Hints (click here)  \nUse pivot_longer() to lengthen the dataset. You\u0026rsquo;ll need to include the arguments \u0026ldquo;cols\u0026rdquo;, \u0026ldquo;names_to\u0026rdquo;, and \u0026ldquo;values_to\u0026rdquo;.    Solution (click here)  sys_long \u0026lt;- sys_noNA %\u0026gt;% pivot_longer(cols = starts_with(\"BP\"), names_to = \"measurement\", values_to = \"sys_bp\") dim(sys_long) #\u0026gt; [1] 23913 2 head(sys_long) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; measurement sys_bp #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 BPSys1 114 #\u0026gt; 2 BPSys2 114 #\u0026gt; 3 BPSys3 112 #\u0026gt; 4 BPSys1 114 #\u0026gt; 5 BPSys2 114 #\u0026gt; 6 BPSys3 112       Exercise 5  Now let\u0026rsquo;s calculate and compare the mean values for each measurement.\n  Hints (click here)  \nUse group_by() and summarize() to get a mean for each of the three measurements.    Solution (click here)  sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; measurement mean_sys #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 BPSys1 119. #\u0026gt; 2 BPSys2 118. #\u0026gt; 3 BPSys3 118.       Exercise 6  The summarise() functions outputs a tibble. Tibbles are intended to be tidy, and as part of that, by default the values they display tend to be truncated/rounded to a greater degree than they would be otherwise in R. In this case, we might want to see a bit more precision in the values. Try adjusting (increasing) the number of significant figures that are displayed in the tibble that was output in Exercise 5.\n  Hints (click here)  \nThis can be done in a couple different ways. One is to convert the tibble to a data frame with as.data.frame(), since data frames, by default, will likely show more significant digits. Alternatively, try setting options(pillar.sigfig) to a new value.    Solution (click here)  sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) %\u0026gt;% as.data.frame() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; measurement mean_sys #\u0026gt; 1 BPSys1 119.1682 #\u0026gt; 2 BPSys2 118.4333 #\u0026gt; 3 BPSys3 117.8479 #OR options(pillar.sigfig = 6) sys_long %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_sys\" = mean(sys_bp)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; measurement mean_sys #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 BPSys1 119.168 #\u0026gt; 2 BPSys2 118.433 #\u0026gt; 3 BPSys3 117.848       Bonus 1  Are those differences statistically significant? A one-way anova might be a good option to test that. Check out the help page for the function aov() and try running an ANOVA.\n  Hint 1 (click here)  \nR often uses the tilde (~) to indicate formula notation. So, for example, you can generate a scatterplot in base R by plotting y~x, assuming y and x are numeric vectors of equal lengths. The aov() function requires a formula with the pattern values~group. You can use the column names in the data frame to define these, but then you need to use the \u0026lsquo;data\u0026rsquo; argument to tell the function the name of the data frame where those columns exist.    Hint 2 (click here)  \nOnce you get the aov() function to work, you can get a p-value with the summary function. See info under the \u0026ldquo;Value\u0026rdquo; heading on the help page for aov().    Solution (click here)  aov(sys_bp~measurement, data = sys_long) %\u0026gt;% summary() #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; measurement 2 6977 3489 11.87 7.05e-06 *** #\u0026gt; Residuals 23910 7028228 294  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1       Bonus 2  Repeat all of the above for diastolic blood pressure with a couple of modifications along the way. First, when you reshape/lengthen the data, make the values in the \u0026lsquo;measurement\u0026rsquo; column numeric. For example, in the sys_long data frame we created above, the values in the measurement column were characters, and looked like \u0026ldquo;BPsys1\u0026rdquo;. This time, make them a factor with the levels \u0026ldquo;1\u0026rdquo;, \u0026ldquo;2\u0026rdquo;, and \u0026ldquo;3\u0026rdquo;.\n  Hint (click here)  \nUse the pivot_longer() arguments \u0026ldquo;names_prefix\u0026rdquo; and \u0026ldquo;names_transform\u0026rdquo;.    Solution (click here)  dia_data \u0026lt;- NHANES %\u0026gt;% select(matches(\"BPDia[123]$\")) %\u0026gt;% drop_na() %\u0026gt;% pivot_longer(cols = starts_with(\"BP\"), names_to = \"measurement\", values_to = \"dia_bp\", names_prefix = \"BPDia\", names_transform = list(measurement = \"as.factor\")) head(dia_data) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; measurement dia_bp #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 1 88 #\u0026gt; 2 2 88 #\u0026gt; 3 3 82 #\u0026gt; 4 1 88 #\u0026gt; 5 2 88 #\u0026gt; 6 3 82 dia_data %\u0026gt;% group_by(measurement) %\u0026gt;% summarize(\"mean_dia\" = mean(dia_bp)) %\u0026gt;% as.data.frame() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; measurement mean_dia #\u0026gt; 1 1 68.28830 #\u0026gt; 2 2 67.46280 #\u0026gt; 3 3 67.06762 aov(dia_bp~measurement, data = dia_data) %\u0026gt;% summary() #\u0026gt; Df Sum Sq Mean Sq F value Pr(\u0026gt;F)  #\u0026gt; measurement 2 6185 3092.3 14.91 3.38e-07 *** #\u0026gt; Residuals 23910 4958916 207.4  #\u0026gt; --- #\u0026gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1       ","date":1612224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612457344,"objectID":"0ce28958d05d4030b2130203b1616eb7","permalink":"https://biodash.github.io/codeclub/08_pivoting/","publishdate":"2021-02-02T00:00:00Z","relpermalink":"/codeclub/08_pivoting/","section":"codeclub","summary":"In this session of Code Club, we'll consider the shape of our datasets and practice with the *tidyr* functions `pivot_longer()` and `pivot_wider()`, which allow us to reformat, or reshape our data - going from a longer form to a wider form, or vice versa.","tags":null,"title":"Session 8: Reshaping Your Data","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":null,"content":"\n Introduction R Markdown consists of an amazing ecosystem of R packages to produce many types of technical content. Its signature capability is that is can run R code and print the code along with its results and nicely formatted prose.\nTo understand R Markdown, we need to learn about three new things:\n  Markdown, a very lightweight text formatting language.\n  Code chunks, which allow us to incorporate R code that can be executed and whose results we can display in text, figures, and tables.\n  The YAML header, which encodes metadata about the output, such as the desired output format and specific formatting features.\n  We\u0026rsquo;ll focus on HTML page output, but will glimpse at the many possibilities for the output format: with R Markdown, it is possible to create not just technical reports, but also slide decks, websites, books, scientific articles, and so on.\nSetup At the core of the R Markdown ecosystem is the rmarkdown package. We need to install this but don\u0026rsquo;t need to load it:\ninstall.packages(\"rmarkdown\")   Inside your directory for Code Club, create a directory for this week:\ndir.create('S07')   First, an example Before we go into details, let\u0026rsquo;s first see a quick demonstration of what we are talking about:\n  Go to File =\u0026gt; New File =\u0026gt; R Markdown, change the Title to \u0026ldquo;Markdown demo\u0026rdquo;, and click OK.\n  Take a look at the R Markdown document, and notice that there seems to be some sort of header (=\u0026gt; YAML), followed by R code wrapped in strange constructs with backticks (=\u0026gt; Code chunks), and plain written text (=\u0026gt; Markdown).\n  Before we can create output, we need to save the document. Click the Save button and save the files as demo.Rmd inside your newly created directory.\n  Now click the Knit button in one of the top bars, and a document should show up in a pop-up or the Viewer pane. This is the rendered output from the R Markdown document.\n  Notice that the YAML header is not printed, at least not verbatim, while some of the code is printed, and we also see code output including a plot!\nThis is what the raw and rendered output look side-by-side:\nWe\u0026rsquo;ll now talk about Markdown, code chunks, and the YAML header in turn.\n I: Markdown Markdown is a very lightweight language to format plain text files, which evolved from simple in-line formatting applied in emails before those started using HTML.\nNeed to emphasize a word without being able to make it italic or bold? How about adding emphasis with asterisks *like so*?\nAn overview of commonly used Markdown syntax    Syntax Result     # My Title Header level 1 (largest)   ## My Section Header level 2   ### My Subsection Header level 3 \u0026ndash; and so forth   *italic* or _italic_ italic   **bold** or __bold__ bold   [Markdown Guide](markdownguide.org)  Markdown Guide (Link with custom text)   ![](path/to/figure.png) Figure   - List item Unordered (bulleted) list   1. List item Ordered (numbered) list   `inline code` inline code   ``` \u0026hellip;code\u0026hellip; ``` Generic code block (for formatting only) (Alternative syntax: 4 leading spaces.)   ```r \u0026hellip;code\u0026hellip; ``` r code block (for formatting only)   --- Horizontal rule (line)    To see this formatting in action, see below an example of a raw Markdown file on the left, and its rendered (formatted) output on the right:\n \u0026ldquo;Plain\u0026rdquo; Markdown files have the extension .md, whereas R Markdown files have the extension .Rmd.\n   II: Integrating R code As we saw above, plain Markdown has syntax for code formatting, but the code is not actually being executed. In R Markdown, however, we are able run code! The syntax to do so is only slightly modified from what we saw above:\n  For inline code, we add r and a space before the R code that is to be executed, for example:\n   Raw Rendered     There are `r 365*24` hours in a year There are 8760 hours in a year      To generate code blocks, which we call code chunks in an R Markdown context,\nwe add r inside curly braces: ```{r}\nWe can optionally add settings that we want to apply to that chunk and/or chunk labels:\n```{r, option1=value, ...} or ```{r, unique-chunk-label, option1=value, ...}\n RStudio keyboard shortcut to insert a code chunk: Cmd/Ctrl+Alt+I.\n    Code chunk examples   A code chunk with default options\u0026hellip;\n\u0026hellip;will be executed and shown followed by the output of the code:\nmean(penguins$bill_depth_mm, na.rm = TRUE) #\u0026gt; [1] 17.15117     As an example of using a code chunk option, we will disable printing the code using echo=FALSE (the code will still run and the output will still be shown):\n#\u0026gt; [1] 17.15117     Figures can, of course, also be printed:\nggplot(penguins) + geom_point(aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + theme_bw() #\u0026gt; Warning: Removed 2 rows containing missing values (geom_point).  Fig. 1: Bill length and depth are correlated within species, and differ subtly between species.     Above, we added a caption for the figure using the fig.cap argument (with a little trick to force a line break, using the \u0026lt;br\u0026gt; HTML syntax).\n Code chunk options Here is an overview of some the most commonly made changes to defaults for code chunk options. This quickly gets confusing, but you\u0026rsquo;ll get the hang of it after experimenting a bit.\n echo=FALSE \u0026ndash; Don\u0026rsquo;t print the code in the output file. eval=FALSE \u0026ndash; Don\u0026rsquo;t run (evaluate) the code. include=FALSE \u0026ndash; Run but don\u0026rsquo;t print the code, nor any of its results. results=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print the text output of the code. fig.show=\u0026quot;hide\u0026quot; \u0026ndash; Don\u0026rsquo;t print figures produced by the code.  Furthermore, you can use message=FALSE and warning=FALSE to suppress any messages (like the output when loading packages) and warnings (like the warning for the penguin figure above), respectively, that R might produce.\nFor figures, the following options are especially useful:\n fig.cap=\u0026quot;My caption\u0026quot; \u0026ndash; Include a caption. fig.asp=0.6 \u0026ndash; Aspect ratio. fig.width=6 \u0026ndash; Width of in inches: same as sizing in regular R code. fig.height=9.6 \u0026ndash; Height in inches: same as sizing in regular R code. out.width=\u0026quot;70%\u0026quot; \u0026ndash; Figure width as printed in the document (in % or pixels, px). out.height=\u0026quot;500px\u0026quot; \u0026ndash; Figure height as printed in the document.  Finally, if your document takes a long time to knit, use cache=TRUE to enable caching of results.\n   Default chunk options It is often practical to set default chunk options for the entire document, and you can do so with the opts_chunk$set() function as shown below:\nThis is usually done in separate \u0026ldquo;global setup chunk\u0026rdquo; at the start of the document.\nWhenever necessary, you can then override these defaults for specific chunks.\n   III: The YAML header YAML (\u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a simple format commonly used for configuration files, which allows you to provide key-value pairs such as author: John Doe.\nIn R Markdown files, it is used as a header which configures certain aspects of the output, especially the formatting. Put another way, the YAML header contains the metadata for the output.\nA basic YAML header Here is an example of a very basic YAML header:\n--- author: My name title: The document's title output: html_document ---  Note the lines which just contain three dashes, which mark the beginning and the end of the YAML header.\nAdding options Often, a value (like html_document) can itself be given key-value pairs to specify additional options \u0026ndash; see the example below where we include a Table of Contents (toc) and also set it to \u0026ldquo;float\u0026rdquo;:\n--- output: html_document: toc: true toc_float: true ---    Note the syntax changes (newlines and added indentation) between the above two examples, this is perhaps a little awkward and often leads to mistakes.\n  Indentation in YAML is using two or four spaces (no tabs!) per indentation level, and it is sensitive to indentation errors. (Fortunately, RStudio inserts spaces for tabs by default \u0026ndash; check/set in Tools =\u0026gt; Global Options =\u0026gt; Code =\u0026gt; Editing.)\n   Some options for html_document output html_document is the most commonly used output format for R Markdown documents, and here are few particularly useful options to customize the output:\n code_download: true \u0026ndash; Include a button to download the code. code_folding: hide \u0026ndash; Using hide or show will enable the folding of code chunks, with hide hiding them by default. toc: true \u0026ndash; Include a table of contents (Also: toc_depth: 3 sets depth to 3, toc_float: true lets the TOC \u0026ldquo;float\u0026rdquo; as you scroll down the document). number_sections: true \u0026ndash; Number the section headings. df_print: paged \u0026ndash; Get nicely formatted and paged data frame printing (also try: df_print: kable). theme: cerulean \u0026ndash; Use a pre-built theme, controlling the overall look and feel of the document. See here for a visual overview.    Three HTML document theme options: darkly, flatly, and cerulean.      IV: R Markdown and RStudio The RMarkdown ecosystem of packages is being developed by RStudio, so it should come as no surprise that the RStudio IDE has some nice RMarkdown functionality.\nKnitting and previewing your document The process of rendering an R Markdown file into another format, as specified by the YAML header, is called knitting. We already saw the button to knit the current document (keyboard shortcut: Cmd/Ctrl+Shift+K).\n If you get preview pop-up windows in RStudio, click the cog wheel icon next to the Knit button, and then select \u0026ldquo;Preview in Viewer Pane\u0026rdquo;.\n  Instead of knitting the entire document, you can also run individual code chunks using the green \u0026ldquo;play button\u0026rdquo; (or Cmd/Ctrl+Shift+Enter), or all code chunks up until the current one (button to the left of the play button).\nFor a live preview (!) of R Markdown output for your active document,\nuse the infinite moon reader from the xaringan package:\ninstall.packages(\"xaringan\") # Simply running the function without arguments will start the preview: xaringan::inf_mr() # To shut down the preview server, if needed, run `servr::daemon_stop()`   Visual Markdown Editor If your RStudio version is at least 1.4 (Click Help =\u0026gt; About RStudio), which was released last fall, you can also use the Visual Markdown Editor.\nThis makes writing in R Markdown almost like using a word processor, and also includes many other features such as better citation support with Zotero integration. Read more about the visual editor here.\nTo switch between the visual editor and regular (\u0026ldquo;source\u0026rdquo;) editing mode, click the A-shaped ruler button in the top-right corner or press Cmd/Ctrl+Shift+F4.\nThis is what our document looks like in the visual editor \u0026ndash; kind of intermediate between the raw R Markdown and the rendered output:\n V: A single source doc, many output formats! One of the greatest features of R Markdown is that you can output to many formats. So from one source document, or very similar variants, you can create completely different output depending on what you need.\nBuilt-in output formats The built-in output formats, which can be used with just the rmarkdown package, are listed below. These include HTML, PDF, Word, PowerPoint, and different HTML slide show formats!\nExtension output formats It\u0026rsquo;s worth highlighting a few of the output formats that can be used with the following packages in the R Markdown ecosystem:\n   distill \u0026ndash; An output format geared towards technical content, e.g. with extended support for equations, citations, and footnotes. Can also create websites.\n   rticles \u0026ndash; R Markdown templates to format output for specific scientific journals.\n   flexdashboard \u0026ndash; Create interactive \u0026ldquo;dashboards\u0026rdquo; to present data.\n   bookdown \u0026ndash; A book format, the R Markdown book is an example.\n   xaringan \u0026ndash; Create fancier presentation slides thanks to a JavaScript library.\n  Starting to use these and other output formats is often as simple as changing the YAML header:\n---output:distill::distill_article---\n Breakout rooms! In the exercises, we will work with an .Rmd file that you can download as follows:\n# dir.create(\"S07\") # You should have already done this # Save the URL for the Rmd file: todays_rmd \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/07_rmarkdown/penguins.Rmd' # Download the Rmd file: download.file(url = todays_rmd, destfile = 'S07/penguins.Rmd')   Next, open the document in RStudio, and fire up the infinite moon reader:\n# install.packages(\"xaringan\") xaringan::inf_mr()   This way, you will be able to nearly instantaneously see the effect of your changes: save the document whenever you want the server to update.\nYou can use either the \u0026ldquo;visual editor\u0026rdquo; or the regular (\u0026ldquo;source\u0026rdquo;) editor \u0026ndash; and you could also start by compating the two.\n Exercise 1: Output formatting with YAML In this exercise, you will fiddle with the YAML header to modify aspects of the html_document output format:\n  Add a theme key to html_output, and try a few of the available value options (\u0026quot;default\u0026quot;, \u0026ldquo;cerulean\u0026rdquo;, \u0026ldquo;journal\u0026rdquo;, \u0026ldquo;flatly\u0026rdquo;, \u0026ldquo;darkly\u0026rdquo;, \u0026ldquo;readable\u0026rdquo;, \u0026ldquo;spacelab\u0026rdquo;, \u0026ldquo;united\u0026rdquo;, \u0026ldquo;cosmo\u0026rdquo;, \u0026ldquo;lumen\u0026rdquo;, \u0026ldquo;paper\u0026rdquo;, \u0026ldquo;sandstone\u0026rdquo;, \u0026ldquo;simplex\u0026rdquo;, \u0026ldquo;yeti\u0026quot;).\nDetermine, once and for all, what the best theme is.\n  Try some of the other options mentioned above (code_download, code_folding, toc, toc_float, toc_depth, df_print), and look at the effects on the rendered output.\n    Hints (click here)     To add options to html_document in the YAML header, you\u0026rsquo;ll need to go from output: html_document on a single line, to a multi-line format with indentation, and with a colon added after html_document:\noutput:html_document:\u0026lt;option\u0026gt;     Solutions (click here)   An example YAML header with several options added:  ---title:\u0026#34;Penguins, demystified.\u0026#34;author:\u0026#34;Jelmer Poelstra\u0026#34;date:\u0026#34;1/29/2021\u0026#34;output:html_document:theme:flatlytoc:truetoc_float:truetoc_depth:5number_sections:truecode_download:truecode_folding:hidedf_print:kable---\n    Exercise 2: Code chunks Our output document looks nice, but there is plenty of room for improvement. In this exercise, we\u0026rsquo;ll refine the output using code chunk options.\nBefore you start, take another look at the box Code chunk options above.\n  Did you notice those messages (when the tidyverse is loaded) and warnings (for the two plots) in the output? Let\u0026rsquo;s get rid of those all at once: suppress R messages and warnings for all chunks by adding arguments to the knitr::opts_chunk$set() function in the first code chunk.\n  Currently, the code line in the install-package code chunk is commented out to avoid the code from running, while still printing it. Try to accomplish this using a code chunk option instead, so you can uncomment the line.\n  We do want to print the code in some cases, but not in others. For the chunk labeled print-tibble, which prints penguins, alter the settings such that the code is no longer printed.\n  Our first figure is kind of squished, and the point and font sizes are perhaps too large. Compare this with the second figure, which has a different setting only for out.width.\nPlay around with the values for the three options that are already in the code chunks (fig.width, out.width, and fig.asp), for one or both figures, see what the effects are, and try to make some improvements.\nDo you understand the difference between the two methods to indicate the figure size (fig.width and out.width)?\n  Insert a new code chunk that prints the penguins_raw tibble in some way (this is available in your environment).\n    Hints (click here)     To suppress messages and warnings throughout:\nAdd message=FALSE and warnings=FALSE inside knitr::opts_chunk$set() in the setup chunk.\n  To avoid running the code:\nUse eval=FALSE in the header of the install-package code chunk.\n  To avoid printing the code:\nUse the echo option in the header of the print-tibble code chunk.\n  Figure sizing:\nThere are two types of sizes that you can set: the size at which R creates figures (fig.width and fig.height), and the size at which the figures are inserted in the document (out.width and out.height). The former will effectively only control relative font and point sizes, whereas the latter controls the \u0026ldquo;actual\u0026rdquo; / final size. For more details and advice, see this section in R for Data Science.\nThe aspect ratio (fig.asp) is height/width, so a value smaller than one creates a wide figure and a value larger than one creates a narrow figure.\nHere, we\u0026rsquo;ve been setting width only \u0026ndash; you can also set fig.height and out.height, but these options become redundant when you set the width and the aspect ratio.\n     Solutions (click here)    To suppress messages and warnings throughout:\nknitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)\n  To avoid running the code: {r install-package, eval=FALSE}\n  To avoid printing the code:\n{r print-tibble, echo=FALSE}\n  Figure sizing:\nExample settings for better-sized figures \u0026ndash;\n{r plot-bills, out.width=\u0026quot;80%\u0026quot;, fig.width=6, fig.asp=0.7}\n  A code chunk to print the penguins_raw tibble (replace single quotes by backticks):\n  '''{r} penguins_raw '''      Bonus 1: Markdown and inline code The formatting for the prose in our document could also be improved. For instance:\n  Use inline code formatting in a couple of cases where this is appropriate.\n  Instead of simply saying \u0026ldquo;8 variables (n = 344 penguins)\u0026rdquo; (under the Summary of the dataset\u0026quot; heading), use inline R code that makes these calculations and print the results.\n  Try a couple of other things: heading levels (one of them is currently not right!), italic text, bold text, and/or ordered (numbered) and unordered (bulleted) lists.\n    Hints (click here)     Simply put backticks around the inline text you want have formatted as code. You can do this, for instance, for mentions of palmerpenguins::penguins.\n  For inline code that runs, use `r my-code`.\nThe number of variables and penguins in the penguin dataset are the number of columns and rows, respectively, in the penguin tibble.\n     Solutions (click here)  Inline calculation of the number of variables and penguins:\n[...] that contains `r ncol(penguins)` variables (n = `r nrow(penguins)` penguins).      Bonus 2: Other output formats Try one or more output formats other than html_document, see this website for the list of available options. If you want to try presentations, note that three dashes --- are used to separate slides.\n It might be confusing that on the website linked to above (see also the screenshot in section V), the output formats are listed functions (html_document() rather than html_document) \u0026ndash; but this is simply because under the hood, these functions are called via the YAML header.\n    \n Go further Pitfalls / Tips   The working directory\nBy default, the working directory for an R Markdown document is the directory in which the file resides.\nThis can be a bit annoying if you\u0026rsquo;re used to using your project\u0026rsquo;s root directory as your working directory (which you should be) and the R Markdown file is not in the project\u0026rsquo;s root directory (which it probably shouldn\u0026rsquo;t be). Nevertheless, simply using ../ notation to move one or two directories up should generally work.\nIf you really need to set a different working directory, you should be aware that surprisingly, setting the working directory with setwd() in a code chunk is not persistent across code chunks. To set a different working directory for the entire document, use knitr::opts_knit$set(root.dir = '/my/working/dir') in a setup chunk.\n  Chunk labels\nChunk labels are optional but if you do give them, note that they have to be unique: the document will fail to render if have two chunks with the same label. Also, avoid using spaces and underscores in the labels (good-chunk-label, bad chunk label, bad_chunk_label).\n  Tables   Tables produced by Markdown text\nThe syntax for basic Markdown tables is as follows:\n| Time | Session | Topic | |:--------------|:-------:|---------:| | _left_ | _center_| _right_ | | Wed 5 pm | 1 | Getting started | | Fri 3 pm | | | | Wed 5 pm | 2 | *dplyr* | | Fri 3 pm | | *Break* |     Time Session Topic     left center right   Wed 5 pm 1 Getting started   Fri 3 pm     Wed 5 pm 2 dplyr   Fri 3 pm  Break    In the Visual Markdown editor in RStudio, you can simply insert a table with a little dialogue box after clicking Table =\u0026gt; Insert Table.\n  Tables (dataframes) produced by R code\nUsing kable(my_df) in a code chunk will create nicer output for individual dataframes (recall the df_print: kable YAML option for document-wide \u0026ldquo;kable\u0026rdquo; printing).\nThere are many packages available for more advanced options, such as GT, DT, and reactable.\n  Websites Note that rmarkdown::render_site() can create simple websites that connects multiple pages with a navigation bar. All you need is a simple YAML file called _site.yml with some settings, and a file for the front page which needs to be called index.Rmd. See here in the R Markdown book for more details.\nOptions with more features, like a blog, are distill websites, and the blogdown package for Hugo sites.\nFurther resources  Free online books by the primary creator of R Markdown and other authors:   R Markdown \u0026ndash; The Definitive Guide  R Markdown Cookbook    RStudio\u0026rsquo;s 5-page R Markdown Reference PDF  RStudio\u0026rsquo;s R Markdown Cheatsheet  RStudio R Markdown lessons  Markdown tutorial  ","date":1611446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611941749,"objectID":"c2268ec4049a3a40fece1f72853db533","permalink":"https://biodash.github.io/codeclub/07_rmarkdown/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/codeclub/07_rmarkdown/","section":"codeclub","summary":"In this 7th session of Code Club, we will learn about Markdown syntax and some of the great functionality of R Markdown.","tags":["codeclub","markdown","rmarkdown"],"title":"Session 7: R Markdown","type":"codeclub"},{"authors":["Stephen Opiyo"],"categories":null,"content":"\n Factors form the basis for many powerful operations in R, including many performed on tabular data. The motivation for factors comes from the notion of categorical variables. These variables are non-numeric in nature corresponding to categories such as male and female, or Democrat, Republican and Independent.\nA factor might be viewed simply as a vector with a bit of more information added. The extra information consists of a record of distinct values in that vector, which are called: levels.\nLet us look at some examples of factors. We will make use of the package forcats, which is one of the 8 core tidyverse packages. Therefore, we start by loading the tidyverse:\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.3 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.4.0 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag() ## Check whether \"forcats\" is listed among the loaded packages. ## Alternatively, you could load \"forcats\" (and \"ggplot2\") separately: # install.packages(\"forcats\") # library(forcats) # library(ggplot2)    Example 1: From a numeric vector to a factor Let us create a factor xf from a vector x with the numbers 5, 12, 13, and 12:\nx \u0026lt;- c(5,12, 13,12) x #\u0026gt; [1] 5 12 13 12 # Convert the vector to a factor: xf \u0026lt;- factor(x) xf #\u0026gt; [1] 5 12 13 12 #\u0026gt; Levels: 5 12 13   The distinct values in xf are 5, 12 and 13, and are listed as levels.\nLet us look in a bit more details at our factor using the R functions str and unclass:\nstr(xf) #\u0026gt; Factor w/ 3 levels \"5\",\"12\",\"13\": 1 2 3 2 unclass(xf) #\u0026gt; [1] 1 2 3 2 #\u0026gt; attr(,\"levels\") #\u0026gt; [1] \"5\" \"12\" \"13\"   Notice that the values in the factor are not stored as (5, 12, 13, 12), but rather as (1, 2, 3, 2)!\nThis means that our data consists first of a level-1 value, then level-2 and level 3 values, and finally another level-2 value. So, the data has been recorded by level.\nThe values attached to each level are recorded too, but as characters such as \u0026quot;5\u0026quot; rather than as numbers such as 5.\n Example 2: From a character vector to a factor We will use the levels Democrat (D), Republican (R), and Independent (I). First, we save a vector:\ny \u0026lt;- c(\"D\", \"R\", \"R\", \"I\", \"R\", \"I\", \"D\", \"I\") y #\u0026gt; [1] \"D\" \"R\" \"R\" \"I\" \"R\" \"I\" \"D\" \"I\" str(y) #\u0026gt; chr [1:8] \"D\" \"R\" \"R\" \"I\" \"R\" \"I\" \"D\" \"I\"   Then, we again convert the vector to a factor, and look at the levels:\nfy \u0026lt;- factor(y) fy #\u0026gt; [1] D R R I R I D I #\u0026gt; Levels: D I R unclass(fy) #\u0026gt; [1] 1 3 3 2 3 2 1 2 #\u0026gt; attr(,\"levels\") #\u0026gt; [1] \"D\" \"I\" \"R\"    Example 3: Ordering factor levels Some variables can be challenging to sort automatically, because the desired sorting order is not alphabetical or numeric.\nFor instance, months that are listed using characters:\nmonths_vector \u0026lt;- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\") # Try to sort using the `sort` function sort(months_vector) #\u0026gt; [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"   That didn\u0026rsquo;t sort in a useful way. But, the problem can be fixed by using a factor.\nFirst, we create a list of the valid levels, which are all 12 months in a year:\nmonth_levels \u0026lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")   Then we convert the vector into a factor, like before, but now we additionally specify the desired levels of the factor, in order, using the levels argument:\nmonths_factor \u0026lt;- factor(months_vector, levels = month_levels)   Now it sorts the way we want to!\nsort(months_factor) #\u0026gt; [1] Jan Mar Apr Dec #\u0026gt; Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec    Example 4: Use of factors in plots with forcats 4A: Plot after reordering manually with fct_relevel() We will use the mtcars data, which was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973\u0026ndash;74 models) \u0026ndash; a data frame with 32 observations for 11 (numeric) variables,\ndata(mtcars) names(mtcars) #\u0026gt; [1] \"mpg\" \"cyl\" \"disp\" \"hp\" \"drat\" \"wt\" \"qsec\" \"vs\" \"am\" \"gear\" #\u0026gt; [11] \"carb\" dim(mtcars) #\u0026gt; [1] 32 11 str(mtcars) #\u0026gt; 'data.frame': 32 obs. of 11 variables: #\u0026gt; $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... #\u0026gt; $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... #\u0026gt; $ disp: num 160 160 108 258 360 ... #\u0026gt; $ hp : num 110 110 93 110 175 105 245 62 95 123 ... #\u0026gt; $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... #\u0026gt; $ wt : num 2.62 2.88 2.32 3.21 3.44 ... #\u0026gt; $ qsec: num 16.5 17 18.6 19.4 17 ... #\u0026gt; $ vs : num 0 0 1 1 0 1 0 1 1 1 ... #\u0026gt; $ am : num 1 1 1 0 0 0 0 0 0 0 ... #\u0026gt; $ gear: num 4 4 4 3 3 3 3 4 4 4 ... #\u0026gt; $ carb: num 4 4 1 1 2 1 4 2 2 4 ...   we will select six variables (mpg, cyl, disp, hp, and wt) to create a dataset Data.\n mpg: Miles per (US) gallon, cyl: Number of cylinders disp: Displacement (cu.in.) hp: Horse power wt: Weight (in 1000 lbs)  Data \u0026lt;- mtcars %\u0026gt;% select(\"mpg\", \"cyl\", \"disp\", \"hp\", \"wt\")   Now, we\u0026rsquo;ll add a new column cyl_chr by converting cyl from numeric to character:\nData \u0026lt;- Data %\u0026gt;% mutate(cyl_chr = recode(cyl,`4` = \"Four\", `6` = \"Six\", `8` = \"Eight\")) head(Data) #\u0026gt; mpg cyl disp hp wt cyl_chr #\u0026gt; 1 21.0 6 160 110 2.620 Six #\u0026gt; 2 21.0 6 160 110 2.875 Six #\u0026gt; 3 22.8 4 108 93 2.320 Four #\u0026gt; 4 21.4 6 258 110 3.215 Six #\u0026gt; 5 18.7 8 360 175 3.440 Eight #\u0026gt; 6 18.1 6 225 105 3.460 Six   We plot a bar chart for cyl_chr:\nData %\u0026gt;% ggplot(aes(x = cyl_chr)) + geom_bar()   In the plot, the levels of the factor were arranged in alphabetical order (Eight, Four, and Six).\nInstead, we want the bar graph arranged in the order Four, Six, and Eight.\nAn alternative to using factor(levels = ...) like we did above, is to use the fct_relevel() function from the forcats package:\nData %\u0026gt;% mutate(cyl_chr = fct_relevel(cyl_chr, \"Four\", \"Six\", \"Eight\")) %\u0026gt;% ggplot(aes(x = cyl_chr)) + geom_bar() + labs(x = \"Cylinder\", y = \"Number of cars\")   4B: Plot after reordering by the value of another column (fct_reorder) Create a dataset called Data_a:\nData_a \u0026lt;- data.frame(name = c(\"North\", \"South\", \"East\", \"West\"), var = sample(seq(1, 10), 4))   Plot a bar chart of Data_a:\nData_a %\u0026gt;% ggplot(aes(x = name, y = var)) + geom_bar(stat = \"identity\", fill = \"#f68034\", alpha = 0.6, width = 0.4)   Reorder following the value of another column using the fct_reorder() function, and flip the plot:\nData_a %\u0026gt;% mutate(name = fct_reorder(name, var)) %\u0026gt;% ggplot(aes(x = name, y = var)) + geom_bar(stat = \"identity\", fill = \"#f68034\", alpha = 0.6, width = 0.4) + coord_flip()   There are several more convenient reordering functions in the forcats package, including:\n  fact_infreq() to reorder by occurrence frequencies of each level (see the picture at the top of the post).\n  fct_inorder() to reorder by order of appearance in the dataframe. This can be useful, for example, if your dataframe has already been sorted properly, and you just need to prevent automatic alphabetic reordering when plotting.\n   Breakout rooms! For the Breakout room exercises, we will use datasets from mtcars and the gss_cat dataset from the forcats package.\nExercise 1  Convert the variable gear from mtcars to a character vector with words for each number (link in example 4A), and plot a bar chart.\nThen, use a factor to reorder the bars to appear in the regular \u0026ldquo;numeric\u0026rdquo; order: \u0026ldquo;Three\u0026rdquo; then \u0026ldquo;Four\u0026rdquo; then \u0026ldquo;Five\u0026rdquo;.\n  Hints (click here)     First, create a dataframe with a column that codes the gears as words, using the mutate() and recode() functions.\n  Then, create a factor from this modified gear column, and order it manually using the fct_relevel() function.\n     Solutions (click here)   Start by loading the dataset:  data(\"mtcars\")    Now, create a new dataset Gear from mtcars, adding a column gear_chr:  gear_df \u0026lt;- mtcars %\u0026gt;% mutate(gear_chr = recode(gear, `3`= \"Three\", `4` =\"Four\", `5`= \"Five\")) head(gear_df) #\u0026gt; mpg cyl disp hp drat wt qsec vs am gear carb gear_chr #\u0026gt; 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Four #\u0026gt; 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Four #\u0026gt; 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Four #\u0026gt; 4 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Three #\u0026gt; 5 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Three #\u0026gt; 6 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Three    Finally, use the forcats function fct_relevel() to rearrange gear_chr in nonalphabetical order, and plot the barchart using geom_bar():  gear_df %\u0026gt;% mutate(gear_fct = fct_relevel(gear_chr, \"Three\", \"Four\", \"Five\")) %\u0026gt;% ggplot(aes(x = gear_fct)) + geom_bar() + labs(x = \"Gear\", y = \"Number of cars\")       Exercise 2  Using the gss_cat dataset from the forcats package (available as gsscat in your environment), create a plot that compares the average number of hours spent watching TV per day across religions, and where religions are ordered by the average number of hours.\n(Despite what we\u0026rsquo;ve learned last week, start by merely plotting the mean, and no distributions, using a barplot or with geom_point().)\nSource: (R for Data Science)\n  Hints (click here)  In order to be able to order the factor by the average number of hours spent watching TV, first compute this average per religion, and save the results in a dataframe (use `mutate()` and `summarize()`). Then, use fct_recorder() to reorder the factor.\n   Solutions (click here)  First, have a look at the dataset:\nforcats::gss_cat #\u0026gt; # A tibble: 21,483 x 9 #\u0026gt; year marital age race rincome partyid relig denom tvhours #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2000 Never ma… 26 White $8000 to … Ind,near r… Protesta… Souther… 12 #\u0026gt; 2 2000 Divorced 48 White $8000 to … Not str re… Protesta… Baptist… NA #\u0026gt; 3 2000 Widowed 67 White Not appli… Independent Protesta… No deno… 2 #\u0026gt; 4 2000 Never ma… 39 White Not appli… Ind,near r… Orthodox… Not app… 4 #\u0026gt; 5 2000 Divorced 25 White Not appli… Not str de… None Not app… 1 #\u0026gt; 6 2000 Married 25 White $20000 - … Strong dem… Protesta… Souther… NA #\u0026gt; 7 2000 Never ma… 36 White $25000 or… Not str re… Christian Not app… 3 #\u0026gt; 8 2000 Divorced 44 White $7000 to … Ind,near d… Protesta… Luthera… NA #\u0026gt; 9 2000 Married 44 White $25000 or… Not str de… Protesta… Other 0 #\u0026gt; 10 2000 Married 47 White $25000 or… Strong rep… Protesta… Souther… 3 #\u0026gt; # … with 21,473 more rows   Then, calculate the mean number of tv-hours and create a plot:\nrelig \u0026lt;- gss_cat %\u0026gt;% group_by(relig) %\u0026gt;% summarize(tvhours = mean(tvhours, na.rm = TRUE)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) ggplot(relig, aes(tvhours, relig)) + geom_point()   It is difficult to interpret this plot because there is no overall pattern.\nWe can improve the plot by reordering the level of religion using fct_reorder():\nrelig \u0026lt;- gss_cat %\u0026gt;% group_by(relig) %\u0026gt;% summarize(tvhours = mean(tvhours, na.rm = TRUE)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) relig %\u0026gt;% mutate(relig = fct_reorder(relig, tvhours)) %\u0026gt;% ggplot(aes(tvhours, relig)) + geom_point()   Reordering religion makes it much easier to see that people in the \u0026ldquo;Don\u0026rsquo;t know\u0026rdquo; category watch much more TV.\n    Bonus: Exercise 3  In exercise 2, we saw large differences in the average time spent watching TV across religions, but we should perhaps have a closer look at the data by plotting distributions.\nGo back to the previous Code Club session and decide which type of plot could be ideal with so many categories.\n  Hints (click here)  [`geom_density_ridges()`](https://wilkelab.org/ggridges/reference/geom_density_ridges.html) from the *ggridges* package is very well suited for a plot with so many categories.    Solutions (click here)  library(ggridges) ggplot(gss_cat, aes(x = tvhours, y = relig, fill = relig)) + geom_density_ridges(alpha = 0.8) + labs(x = 'Number of hours spent watching TV', y = 'Religion') + guides(fill = FALSE) + theme_minimal() #\u0026gt; Picking joint bandwidth of 0.586 #\u0026gt; Warning: Removed 10146 rows containing non-finite values (stat_density_ridges).      ","date":1610928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611345251,"objectID":"a8e91bcb80337e5e587f785ed259f392","permalink":"https://biodash.github.io/codeclub/06_factors/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/codeclub/06_factors/","section":"codeclub","summary":"In this sixth session of Code Club, we'll learn how to use factors to our advantage","tags":["codeclub","factors"],"title":"Session 6: Factors","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n  New to ggplot? Check out the last Code Club Session 4 on Visualizing Data.\nIf you\u0026rsquo;ve never used ggplot2 before (or even if you have), you may find this cheat sheet useful.\n Getting Started Script for today\u0026rsquo;s session # directory for Code Club Session 2: dir.create(\"S05\") # directory for our script # (\"recursive\" to create two levels at once.) dir.create(\"S05/scripts/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/05_ggplot-round-2/Session5_ggplot2.R' # indicate the name of the new script file Session5_script \u0026lt;- \"S05/scripts/Session5_script.R\" # go get that file!  download.file(url = todays_R_script, destfile = Session5_script)    1 - Why visualize our data? Artwork by Allison Horst\nWe make data visualizations for two main reasons:\n To explore our data To share our data with others  Often, we think about figure generation as the last part of the scientic process, something you do as you prepare a manuscript for publication. I hope to convince you that exploring your data, and making exploratory plots is a critical part of the data analysis and interpretation process.\nToday we will be using ggplot2 to make a series of plots that help us better understand the underlying structure in our dataset.\nWhen summary statistics don\u0026rsquo;t cut it\nThis \u0026ldquo;Datasaurus Dozen\u0026rdquo; shows the value of looking at your data beyond means and standard deviations. In the gif above, created by Alberto Cairo, each of these 13 datasets have identical means, standard eviations, and correlations to two decimal places. And one of the datasets is a dinosaur!\n What will we go over today\nThese geoms will help you to get better acquainted with your data.\n  geom_col() - makes bar plots. I will show you how to do this and then recommend that you don\u0026rsquo;t.  geom_boxplot() - makes infinitely useful boxplots.  geom_violin() - makes violin plots, a hybrid between a boxplot and a density plot. Very musical.  geom_density_ridges() - a density plot giving you the impression of a side view of a mountain range. Requires the package ggridges  geom_jitter() - adds all datapoints to your plot, and jitters them to handle overplotting.  I will also go over a few tricks along the way, including coord_flip(), adding labels using labs(), and changing the overall look of the plot with theme(), or pre-set themes like theme_classic() which is my go-to.\n   2 - Accessing our data Let\u0026rsquo;s get set up and grab some data so that we can learn more about penguins (and ggplot2)\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries.\nlibrary(tidyverse)   Then let\u0026rsquo;s access the wintry palmerpenguins dataset. We will then look at penguins, the dataset we will be using for the first part of today\u0026rsquo;s Code Club. This data is collected on penguins from the Palmer Station Antarctica Long-Term Ecological Research study area.\nArtwork by Allison Horst\ninstall.packages(\"palmerpenguins\")   library(palmerpenguins)   Let\u0026rsquo;s look at the data in penguins.\n# look at the first 6 rows, all columns head(penguins) #\u0026gt; # A tibble: 6 x 8 #\u0026gt; species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; #\u0026gt; 1 Adelie Torge… 39.1 18.7 181 3750 male  #\u0026gt; 2 Adelie Torge… 39.5 17.4 186 3800 fema… #\u0026gt; 3 Adelie Torge… 40.3 18 195 3250 fema… #\u0026gt; 4 Adelie Torge… NA NA NA NA NA  #\u0026gt; 5 Adelie Torge… 36.7 19.3 193 3450 fema… #\u0026gt; 6 Adelie Torge… 39.3 20.6 190 3650 male  #\u0026gt; # … with 1 more variable: year \u0026lt;int\u0026gt; # check the structure # this tell us what is contained within our df glimpse(penguins) #\u0026gt; Rows: 344 #\u0026gt; Columns: 8 #\u0026gt; $ species \u0026lt;fct\u0026gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Ade… #\u0026gt; $ island \u0026lt;fct\u0026gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgers… #\u0026gt; $ bill_length_mm \u0026lt;dbl\u0026gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1,… #\u0026gt; $ bill_depth_mm \u0026lt;dbl\u0026gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1,… #\u0026gt; $ flipper_length_mm \u0026lt;int\u0026gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 18… #\u0026gt; $ body_mass_g \u0026lt;int\u0026gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475,… #\u0026gt; $ sex \u0026lt;fct\u0026gt; male, female, female, NA, female, male, female, mal… #\u0026gt; $ year \u0026lt;int\u0026gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 200…   This dataset contains the following measurements about penguins at Palmer Station in Antarctica:\n species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year  We are going to be plotting to get an understanding of bill_length_mm which is the length of the bill from the penguins face, protruding outwards (and more easily understood in the image below).\nArtwork by Allison Horst\n 3 - Removing NAs Sometimes you will have NAs (or missing data). That might be informative to you, but here we are going to remove missing data using drop_na(), and assign it to a new dataframe called penguins_noNA.\n# check dimensions of penguins dim(penguins) #\u0026gt; [1] 344 8 # remove NAs penguins_noNA \u0026lt;- penguins %\u0026gt;% drop_na() dim(penguins_noNA) # we have removed 11 observations #\u0026gt; [1] 333 8   Note - by removing NAs, we have gotten rid of 11 observations\n 4 - Bar charts with geom_col() and stat_summary() Often, people use bar charts, representing the height or the length of the bar as proportional to the average value that it represents. These charts are sometimes called dynamite plots because they resemble (when they have an error bar with whisker) those cartoon style dynamite sticks. Pow!\nHowever, these bar charts, even if you add a standard deviation/error, really can hide the true distribution of your data, and for this reason, I and others hope you don\u0026rsquo;t select to make them.\nI hope after today, you see that there is always a better chart type to make than a bar chart. But I will show you how to make them anyway.\nBefore we plot, let\u0026rsquo;s calculate some summary statistics so we know what we should expect.\n# calculating mean bill_length_mm by species penguins_noNA %\u0026gt;% group_by(species) %\u0026gt;% summarize(mean_bill_length = mean(bill_length_mm)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; species mean_bill_length #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Adelie 38.8 #\u0026gt; 2 Chinstrap 48.8 #\u0026gt; 3 Gentoo 47.6   Just calling geom_col() does not give us what we want. Look at the y-axis scale and how out of line this is with our summary statistics.\n# bar plot with geom_col() # this is wrong! penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_col()   Using geom_col() the right way.\n# bar plot, the right way with geom_col() penguins_noNA %\u0026gt;% group_by(species) %\u0026gt;% summarize(mean_bill_length = mean(bill_length_mm)) %\u0026gt;% ggplot(aes(x = species, y = mean_bill_length)) + geom_col() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument)  # or you could do this in a less bulky way with stat_summary() penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + stat_summary(fun = \"mean\", geom = \"bar\")    5 - Boxplots with geom_boxplot() A boxplot has the benefit of showing you more than the median and the standard deviation, so you can better see the true distribution of your data. In geom_boxplot():\n lower whisker = smallest observation greater than or equal to lower hinge - 1.5 * IQR lower hinge/bottom line of box part of boxplot = 25% quantile middle = median, 50% quantile upper hinge/top line of box part of boxplot = 75% quantile upper whisker = largest observation less than or equal to upper hinge + 1.5 * IQR  # vertical boxplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot()   Adding coord_flip() makes your vertical boxplot horizontal. You could do the same thing by flipping the variables on the x and y mappings.\n# horizontal boxplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm)) + geom_boxplot() + coord_flip()   Look at how much more information we have here than in our bar plots!\n 5 - Violin plots with geom_violin() A violin plot is boxplot-esque, but shows a mirrored density distribution. This type of plot is useful when you are trying to particularly show data distribution.\nNote here I have also mapped species to color, within the aes statement so it will apply globally to this plot.\n# violin plot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = species)) + geom_violin()   Adding geom_point() lets you add another layer of all the actual data points, on top of your violin plot. Remember that this is inherent in the design of ggplot2, that you can layer your plots, of different types, on top of each other.\n# violin plot with data points overlaid penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_violin() + geom_point()   Note, I am now mapping species to fill instead of color. See the difference?\nThis doesn\u0026rsquo;t look too good because of overplotting, i.e., the smear of datapoints that doesn\u0026rsquo;t give you much information about distribution.\nWe can add geom_jitter() to introduce some small amount of randomness to our points to make us able to see them better. Seeing all your data points also lets the reader easily get a sense of your sample size.\n# violin plot with data points jittered penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_violin() + geom_jitter()   geom_jitter() is a specialized version of geom_point(), but you could replace the geom_jitter() call with geom_point(position = \u0026quot;jitter) and get the same result. You can also use geom_point(position = position_jitterdodge()) if you only want jitter in the x, and don\u0026rsquo;t want any jitter in the y.\nWow, we now have so much more information about our data!\n 6 - Dot plots with geom_dotplot() A dot plot plots each individual datapoint, and can stack how you like. These look a lot like the SigmaPlot plots to me.\n binaxis can be set to \u0026ldquo;x\u0026rdquo; or \u0026ldquo;y\u0026rdquo; stackdir indicates how to stack the dots: \u0026ldquo;up\u0026rdquo; (default), \u0026ldquo;down\u0026rdquo;, \u0026ldquo;center\u0026rdquo;, \u0026ldquo;centerwhole\u0026rdquo; (centered, but with dots aligned) dotsize indicates the size of the dots, with 1 as default  # dotplot penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, fill = species)) + geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.5) #\u0026gt; `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`.    7 - Density ridge plots with geom_density_ridges() A density ridge plots with geom_density_ridges() requires the packages ggridges, and make multiple density plots in a staggered orientation.\nYou can adjust scale within geom_density_ridges() to adjust the size of each density plot, though I have left it on the default. Adding alpha sets transparency.\n# install.packages(\"ggridges\") library(ggridges) penguins_noNA %\u0026gt;% ggplot(aes(x = bill_length_mm, y = species, fill = species)) + geom_density_ridges(alpha = 0.8) #\u0026gt; Picking joint bandwidth of 1.08    8 - ggplot is made for layering! I have shown you a bunch of different plot types, and you can combine many of them together. Here is an example of combining geom_violin() and geom_jitter(), while mapping new variables to aesthetics.\npenguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex, shape = island, group = species)) + geom_violin() + geom_jitter(position = position_jitterdodge(jitter.width = 2))    9 - Increase clarity and visual appeal We can quickly make our plot:\n prettier by setting a theme more clear by setting plot labels (eg., axes, titles, legend) with labs  penguins_noNA %\u0026gt;% ggplot(aes(x = species, y = bill_length_mm, color = sex, shape = island, group = species)) + geom_violin() + geom_jitter(position = position_jitterdodge(jitter.width = 2), alpha = 0.7) + theme_classic() + labs(title = \"Penguin Bill Length by Species, Sex and Location\", subtitle = \"Collected at Palmer Station, Antarctica\", x = \"Penguin Species\", # x axis label y = \"Bill length (mm)\", # y axis label color = \"Sex\", # legend title shape = \"Island\") # legend title    10 - Breakout rooms! Main exercises Get data We are going to use the NHANES dataset we used in Session 3 on joining. What was that data about again? Let\u0026rsquo;s refresh our memory.\n library(NHANES) knitr::kable(head(NHANES))     ID SurveyYr Gender Age AgeDecade AgeMonths Race1 Race3 Education MaritalStatus HHIncome HHIncomeMid Poverty HomeRooms HomeOwn Work Weight Length HeadCirc Height BMI BMICatUnder20yrs BMI_WHO Pulse BPSysAve BPDiaAve BPSys1 BPDia1 BPSys2 BPDia2 BPSys3 BPDia3 Testosterone DirectChol TotChol UrineVol1 UrineFlow1 UrineVol2 UrineFlow2 Diabetes DiabetesAge HealthGen DaysPhysHlthBad DaysMentHlthBad LittleInterest Depressed nPregnancies nBabies Age1stBaby SleepHrsNight SleepTrouble PhysActive PhysActiveDays TVHrsDay CompHrsDay TVHrsDayChild CompHrsDayChild Alcohol12PlusYr AlcoholDay AlcoholYear SmokeNow Smoke100 Smoke100n SmokeAge Marijuana AgeFirstMarij RegularMarij AgeRegMarij HardDrugs SexEver SexAge SexNumPartnLife SexNumPartYear SameSex SexOrientation PregnantNow     51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51624 2009_10 male 34 30-39 409 White NA High School Married 25000-34999 30000 1.36 6 Own NotWorking 87.4 NA NA 164.7 32.22 NA 30.0_plus 70 113 85 114 88 114 88 112 82 NA 1.29 3.49 352 NA NA NA No NA Good 0 15 Most Several NA NA NA 4 Yes No NA NA NA NA NA Yes NA 0 No Yes Smoker 18 Yes 17 No NA Yes Yes 16 8 1 No Heterosexual NA   51625 2009_10 male 4 0-9 49 Other NA NA NA 20000-24999 22500 1.07 9 Own NA 17.0 NA NA 105.4 15.30 NA 12.0_18.5 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 4 1 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA   51630 2009_10 female 49 40-49 596 White NA Some College LivePartner 35000-44999 40000 1.91 5 Rent NotWorking 86.7 NA NA 168.4 30.57 NA 30.0_plus 86 112 75 118 82 108 74 116 76 NA 1.16 6.70 77 0.094 NA NA No NA Good 0 10 Several Several 2 2 27 8 Yes No NA NA NA NA NA Yes 2 20 Yes Yes Smoker 38 Yes 18 No NA Yes Yes 12 10 1 Yes Heterosexual NA   51638 2009_10 male 9 0-9 115 White NA NA NA 75000-99999 87500 1.84 6 Rent NA 29.8 NA NA 133.1 16.82 NA 12.0_18.5 82 86 47 84 50 84 50 88 44 NA 1.34 4.86 123 1.538 NA NA No NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 5 0 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA    # kable just formats as a scrollable table for this website # you can just use head(NHANES) or glimpse(NHANES)    Exercise 1  Create a new data frame includes the NHANES data only from individuals that are 20 years of age or older, and removes observations where there are NAs for either age subdivided by decade (AgeDecade) or total cholesterol (TotChol).\n  Hints (click here)  Try using a series of filter() statements. Remember, you can tell filter what you want, or what you don\u0026rsquo;t want. You can filter for if specific variables have NAs by using is.na() on your variable of interest. Also remember that ! means \u0026ldquo;not.\u0026rdquo; You will notice that if you want to use drop_NA() you need to specific which specific variables you want to use, or you will inadvertably drop a lot of observations which have missing data for variables other than those we are plotting..    Solutions (click here)  # here are a few ways to do this NHANES_over20_noNA \u0026lt;- NHANES %\u0026gt;% filter(Age \u0026gt;20) %\u0026gt;% drop_na(AgeDecade, TotChol) dim(NHANES_over20_noNA) #\u0026gt; [1] 6408 76 NHANES_over20_noNA \u0026lt;- NHANES %\u0026gt;% filter(Age \u0026gt;20, !is.na(AgeDecade), !is.na(TotChol)) dim(NHANES_over20_noNA) #\u0026gt; [1] 6408 76       Exercise 2  Create a boxplot to show the relationship between total cholesterol (TotChol) and age (AgeDecade).\n  Hints (click here)  Try geom_boxplot(). Map your variables of interest to the x and y aesthetics. Which you variable you put on x and y will determine if your boxplot is vertical or horizontal.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol)) + geom_boxplot()       Exercise 3  Take your plot from Exercise 2 and make it a violin plot instead of a boxplot. Then color by age.\n  Hints (click here)  The geom for a violin plot is geom_violin(). You can change color by mapping to color or to fill.    Solutions (click here)  Note the difference between mapping to color vs. fill.\nNHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_violin()   NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, fill = AgeDecade)) + geom_violin()       Exercise 4  Make add a boxplot to your violin plot from Exercise 3. Adjust the parameters so you the plot looks good to you.\n  Hints (click here)  In geom_boxplot(), you can adjust the width of the boxplot by setting width = X. A width of 1 is the default.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_violin() + geom_boxplot(width = 0.2)       Exercise 5  Add all of the data points on top of your boxplot from Exercise 2 of total cholesterol by age. Adjust the parameters so you the plot looks good to you. While you are at it, clean up your plot labels and give your plot a title.\n  Hints (click here)  Remember that ggplot layers your plots, so layers that are further down in your code, will be applied on top of those that come earlier.    Solutions (click here)  geom_boxplot(outlier.shape = NA) removes the outliers from geom_boxplot(), since we are plotting all of the points, we do not want the outliers appearing twice.\nNHANES_over20_noNA %\u0026gt;% ggplot(aes(x = AgeDecade, y = TotChol, color = AgeDecade)) + geom_boxplot(outlier.shape = NA) + geom_jitter(width = 0.3, alpha = 0.1) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", x = \"Age, by Decade\", y = \"Total Cholesterol, mmol/L\", color = \"Age (years)\")       Bonus exercises Bonus 1  Make a density ridge plot for age by total cholesterol.\n  Hints (click here)  Try geom_density_ridges(), and remember, this is not a part of ggplot2, so be sure to call library(ggridges).    Solutions (click here)  # install.packages(\"ggridges\") library(ggridges) NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7) #\u0026gt; Picking joint bandwidth of 0.224       Bonus 2  Take your density ridge plot from Bonus 1, and try applying a theme from hrbrthemes to it.\n  Hints (click here)  hrbrthemes is not part of ggplot2 so remember to install the package, and then call library(hrbrthemes). You can google the package to see what all your theme options are. I like theme_ipsum_rc(), try that one if you like!    Solutions (click here)  # install.packages(\"hrbrthemes\") library(hrbrthemes) #\u0026gt; NOTE: Either Arial Narrow or Roboto Condensed fonts are required to use these themes. #\u0026gt; Please use hrbrthemes::import_roboto_condensed() to install Roboto Condensed and #\u0026gt; if Arial Narrow is not on your system, please see https://bit.ly/arialnarrow NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + theme_ipsum_rc() #\u0026gt; Picking joint bandwidth of 0.224       Bonus 3  Tidy up your plot from Bonus 2 by giving it a title, axis labels, and try adding the median total cholesterol to each density ridge plot.\n  Hints (click here)  Using stat_summary() will help you add the median.\n   Solutions (click here)   theme(axis.title.x = element_text(hjust = 0.5)) makes the x-axis title center justified. you can change shape within stat_summary() to be anything you like, either an R shape, a specific keyboard key, or even a pasted emoji. The default is a point. when you set a theme(), anything that comes below will override what code comes previous, so for this reason, if you are going to amend a pre-made theme, first call the pre-made theme, and then make any changes you like below.  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + stat_summary(fun = median) + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5)) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", x = \"Total Cholesterol, mmol/L\", y = \"Age, by Decade\", fill = \"Age (years)\") #\u0026gt; Picking joint bandwidth of 0.224 #\u0026gt; Warning: Removed 6 rows containing missing values (geom_segment).       Bonus 4  Commonly used cutoffs for cholesterol are: \u0026lt; 5.2 mmol/L is normal, 5.2-6.2 mmol/L is borderline high and \u0026gt; 6.2 mmol is high. Add a vertical cutoff line showing the level below which cholesterol would be considered normal.\n  Hints (click here)  Using geom_vline() will let you add a vertical line with an xintercept that is appropriate.    Solutions (click here)  NHANES_over20_noNA %\u0026gt;% ggplot(aes(x = TotChol, y = AgeDecade, fill = AgeDecade)) + geom_density_ridges(alpha = 0.7, scale = 0.9) + stat_summary(fun = median) + geom_vline(aes(xintercept = 5.2)) + theme_ipsum_rc() + theme(axis.title.x = element_text(hjust = 0.5), axis.title.y = element_text(hjust = 0.5)) + labs(title = \"Total Cholesterol by Age\", subtitle = \"Data from the National Health and Nutrition Examination Survey (NHANES)\", caption = \"Vertical line indicates upper limit of normal cholesterol\", x = \"Total Cholesterol, mmol/L\", y = \"Age, by Decade\", fill = \"Age (years)\") #\u0026gt; Picking joint bandwidth of 0.224 #\u0026gt; Warning: Removed 6 rows containing missing values (geom_segment).       ","date":1610668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611108419,"objectID":"a799f7bb592979c0bb09905b9a132709","permalink":"https://biodash.github.io/codeclub/05_ggplot-round-2/","publishdate":"2021-01-15T00:00:00Z","relpermalink":"/codeclub/05_ggplot-round-2/","section":"codeclub","summary":"During this fifth session of Code Club, we will be continuing to learn to use ggplot2, including techniques that better enable us to see our true data distribution.","tags":null,"title":"Session 5: ggplot2, round 2","type":"codeclub"},{"authors":["Michael Broe"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Learn the philosophy of coding a graphic. Learn the basic template of a ggplot2 graphic, so you can reuse it for multiple chart types. Learn how you can quickly add visual information to a graphic using aesthetics and layers.   Intro: The ggplot2 philosophy We have already seen that in R, instead of manually manipulating data frames as you might do when editing Excel sheets, we code the operations we want to perform using dplyr verbs like select(), mutate(), inner_join(), and so on.\nIn a similar way when performing visualization, instead of clicking on a chart type in Excel, we code the chart in R.\nAnd just as dplyr gives us efficient ways to manipulate data frames, ggplot2 (which is also part of the tidyverse) gives us efficient ways to manipulate charts/plots/graphics (we use these terms interchangeably).\nThe gg in ggplot2 stands for grammar of graphics, a systematic approach for designing statistical plots developed by Leland Wilkinson. The idea behind this was to think about \u0026lsquo;pulling apart\u0026rsquo; various plots into their shared component pieces, then provide code that could put them together again. We can then create new plots like we create new sentences (once we understand this grammar).\nThere are two parts to this. First, the \u0026lsquo;nouns and verbs\u0026rsquo; we need to work with plots are very different than those we need to work with data frames. ggplot2 is like a mini-language of its own, with its own verbs and syntax.\nSecond, this notion of pulling apart a graphic leads to the idea of layers. You can build up a plot of any complexity by overlaying different views of the same data.\nThere\u0026rsquo;s a learning curve here for sure, but there are a couple of things that help us.\nFirst, every graphic shares a common template. This is like thinking about the sentence \u0026ldquo;The cat sat on the mat\u0026rdquo; grammatically as the template NP V PP (Noun Phrase \u0026ldquo;The cat\u0026rdquo;, Verb \u0026ldquo;sat\u0026rdquo;, Prepositional Phrase \u0026ldquo;on the mat\u0026rdquo;). Once you understand this structure you can \u0026ldquo;say\u0026rdquo; a lot of different things.\n(And I mean a lot. The ggplot cheat sheet lists over 40 plot-types, but because this is a language, users can create their own extensions that you can also utilize, adding over 80 more.)\nSecond, the way we put layers together is identical to the way we use pipes. You can read %\u0026gt;% as \u0026ldquo;and then\u0026rdquo;: select() and then mutate() and then summarize(). In graphics, we can say \u0026ldquo;show this layer, and then overlay this layer, and then overlay this layer\u0026rdquo;, etc., using a very similar syntax.\n Examples So how does this work in practice? We\u0026rsquo;ll work through visualizing the iris dataset that you\u0026rsquo;ve seen before. This is an extremely famous dataset that was first analyzed by R. A. Fisher in 1936: The use of multiple measurements in taxonomic problems. He was attempting to use petal and sepal measurements to discriminate one species from another.\nggplot2 is part of the tidyverse package so we need to load that first:\n# this assumes you've already installed tidyverse library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  0.8.5 #\u0026gt; ✔ tidyr  1.0.3 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   And recall that the iris dataset (3 species, 50 observations per species) is automatically available to us:\nhead(iris) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa   What is the correlation between petal length and width in these species? Are longer petals also wider? We can visualize this with a scatterplot. But first let\u0026rsquo;s look a the ggplot template. (Note the package is ggplot2, the command is ggplot.)\nggplot(data = \u0026lt;DATA\u0026gt;) + \u0026lt;GEOM_FUNCTION\u0026gt;(mapping = aes(\u0026lt;MAPPINGS\u0026gt;))  These are the obligatory parts of any plot. The first argument to ggplot() is the data frame:\nggplot(data = iris)   This is not very interesting! but it\u0026rsquo;s notable that it is something. ggplot() has created a base coordinate system (a base layer) that we can add visual layers to. The add a layer operator is \u0026ldquo;+\u0026rdquo;, which is the ggplot equivalent of the pipe symbol, and it must occur at the end of the line.\nThe next argument specifies the kind plot we want: scatterplot, bar chart, fitted line, boxplot, pie chart, etc. ggplot2 refers to these as geoms: the geometrical object that a plot uses to represent data. You can see an overview of many of these geoms in the cheat sheet. The geom for a scatterplot is geom_point().\nBut we also require a mapping argument, which maps the variables in the dataset we want to focus on to their visual representation in the plot.\nAnd finally we need to specify an aesthetic for the geometric objects in the plot, which will control things like shape, color, transparency, etc. Perhaps surprisingly, for a scatterplot, the x and y coordinates are aesthetics, since these control, not the shape or color, but the relative position of the points in the coordinate system.\nHere is our complete plot:\nggplot(data = iris) + geom_point(mapping = aes(x = Petal.Length, y = Petal.Width))   There is clearly a positive correlation between length and width. And we can make this even more apparent by visually fitting a line to the data, by overlaying another geom in the same plot.\nggplot(data = iris) + geom_point(mapping = aes(x = Petal.Length, y = Petal.Width)) + geom_smooth(mapping = aes(x = Petal.Length, y = Petal.Width)) #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'   There is clearly some code redundancy here, and we really don\u0026rsquo;t want the x, y mapping of these two layers to be independent. We can extract the common mapping information and move it to the top level:\nggplot(data = iris, (mapping = aes(x = Petal.Length, y = Petal.Width))) + geom_point() + geom_smooth() #\u0026gt; `geom_smooth()` using method = 'loess' and formula 'y ~ x'   So we have the possibility of local layer specifications, and global specifications. Global specifications are inherited by all the local layers.\nThe power of aesthetics The aim of Fisher\u0026rsquo;s paper was to try to discriminate different species based on their morphological measurements. It looks from this plot that there are two distinct clusters. Do these clusters correspond to different species? There are two clusters, but three species. How can we explore this further?\nOur current plot uses two numeric variables: Petal.Length and Petal.width. We can add a third categorical variable, like Species, to a two dimensional scatterplot by mapping it to a different visual aesthetic. We\u0026rsquo;ve mapped length and width to x,y coordinates. Now we\u0026rsquo;ll simultaneously map species to color by expanding our list of aesthetics:\nggplot(data = iris) + (mapping = aes(x = Petal.Length, y = Petal.Width, color = Species)) + geom_point()   The R help for a specific geoms will list, among other things, all the aesthetics that geom supports.\nBreakout Rooms In the exercises we\u0026rsquo;ll be looking a little more at the iris data, and in addition, the NHANES data we used last week, and the left-joined bird dataset we built last week in Excercise 7.\nIf you haven\u0026rsquo;t installed the NHANES dataset do:\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/d4/h4yjqs1560zbsgvrrwbmbp5r0000gn/T//RtmpPvm8W9/downloaded_packages   Once installed, load it with:\nlibrary(NHANES)   A prebuilt joined data set has been loaded on github.\n# create a data directory for the new file if you haven't done so yet: dir.create('data/birds', recursive = TRUE) #\u0026gt; Warning in dir.create(\"data/birds\", recursive = TRUE): 'data/birds' already exists # set the url joined_data_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/04_ggplot2/joined_data.tsv' # set the path for the downloaded file joined_file \u0026lt;- 'data/birds/joined_data.tsv' #download to file download.file(url = joined_data_url, destfile = joined_file) # read file joined_data \u0026lt;- read_tsv(joined_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; adult_body_mass_g = col_double(), #\u0026gt; adult_svl_cm = col_double(), #\u0026gt; longevity_y = col_double(), #\u0026gt; litter_or_clutch_size_n = col_double() #\u0026gt; )   Exercise 1 Revisit the iris data set, and plot sepal width (y) against sepal length (x) colored by species. Which morphological character, petals or sepals, provides the greatest discrimination between species?\n  Hints (click here)  Simply reuse the code we used for petals. You can often leverage code from an old plot for a new one.    Solution (click here)  ggplot(data = iris) + (mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point()   Note this solution shows yet another way to position global mapping information: as its own layer. This can help readability and avoid too many nested parentheses.\n   Exercise 2 Use the NHANES data set to plot body mass index (y) against height (x). Color by gender. Which gender has the highest BMI?\n  Hints (click here)  glimpse() the dataset to identify the variable names.    Solution (click here)  ggplot(data = NHANES) + geom_point(mapping = (aes(x = Height, y = BMI, color = Gender))) #\u0026gt; Warning: Removed 366 rows containing missing values (geom_point).      Exercise 3 Use the same plot but now color by physical activity. How active are those people with the highest BMI?\n  Hints (click here)  Again, glimpse() the dataset to identify the variable names.    Solution (click here)  ggplot(data = NHANES) + geom_point(mapping = (aes(x = Height, y = BMI, color = PhysActive))) #\u0026gt; Warning: Removed 366 rows containing missing values (geom_point).      Exercise 4 Often plotting the data allows us to identify outliers, which may be data-entry errors, or genuinely extreme data. Using the joined_data set, plot adult body mass (y) against longevity (x). Identify extreme data points at the high end of body mass. How can we identify what these points represent?\n  Hints (click here)  Examine the plot to find an appropriate threshold value, and filter the data using that value. How many data points are there passing that threshold? What species are represented by these data points? How many weights are reported? Why is the plot misleading here?    Solution (click here)  ggplot(data = joined_data) + geom_point(mapping = (aes(x = longevity_y, y = adult_body_mass_g))) #\u0026gt; Warning: Removed 24089 rows containing missing values (geom_point).   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) #\u0026gt; # A tibble: 228 x 9 #\u0026gt; species locality stateProvince eventDate species_en #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Cygnus… Findlay Ohio 2008-02-17 00:00:00 Mute Swan  #\u0026gt; 2 Cygnus… Dundee Ohio 2004-02-16 00:00:00 Mute Swan  #\u0026gt; 3 Cygnus… 44805 A… Ohio 2006-02-18 00:00:00 Mute Swan  #\u0026gt; 4 Cygnus… 45011 H… Ohio 2005-02-19 00:00:00 Mute Swan  #\u0026gt; 5 Cygnus… 45042 M… Ohio 2009-02-13 00:00:00 Trumpeter… #\u0026gt; 6 Cygnus… 44813 B… Ohio 2007-02-19 00:00:00 Mute Swan  #\u0026gt; 7 Cygnus… Spencer Ohio 2008-02-16 00:00:00 Mute Swan  #\u0026gt; 8 Cygnus… 44903 M… Ohio 2009-02-16 00:00:00 Mute Swan  #\u0026gt; 9 Cygnus… 44601 A… Ohio 2002-02-16 00:00:00 Mute Swan  #\u0026gt; 10 Cygnus… Avon La… Ohio 2007-02-17 00:00:00 Mute Swan  #\u0026gt; # … with 218 more rows, and 4 more variables: adult_body_mass_g \u0026lt;dbl\u0026gt;, #\u0026gt; # adult_svl_cm \u0026lt;dbl\u0026gt;, longevity_y \u0026lt;dbl\u0026gt;, litter_or_clutch_size_n \u0026lt;dbl\u0026gt;   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) %\u0026gt;% select(species) %\u0026gt;% distinct() #\u0026gt; # A tibble: 2 x 1 #\u0026gt; species  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Cygnus olor  #\u0026gt; 2 Cygnus buccinator   joined_data %\u0026gt;% filter(adult_body_mass_g \u0026gt; 10000) %\u0026gt;% select(adult_body_mass_g) %\u0026gt;% distinct() #\u0026gt; # A tibble: 2 x 1 #\u0026gt; adult_body_mass_g #\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 10230 #\u0026gt; 2 10300     Bonus, a new geom! Revisit the iris data and generate a density histogram for sepal length, categorized by species.\n  Hints (click here)  Use geom_density(). Check the help to see what aesthetics it supports. Note that while you 'color' a point, you 'fill' an area.    Solution (click here)  ggplot(data = iris) + (mapping = (aes(x = Sepal.Length, fill = Species))) + geom_density(alpha = 0.5)   Note, what does the alpha aesthetic control?    \n","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608135268,"objectID":"cbba979dcba464a35bad36bb366cb927","permalink":"https://biodash.github.io/codeclub/04_ggplot2/","publishdate":"2020-12-10T00:00:00Z","relpermalink":"/codeclub/04_ggplot2/","section":"codeclub","summary":"In this session of Code Club, we'll look at how to visualize data in R using **ggplot2**.","tags":null,"title":"Session 4: Visualizing Data","type":"codeclub"},{"authors":["Mike Sovic"],"categories":null,"content":"\n New To Code Club?   First, check out the Code Club Computer Setup instructions, which also has some pointers that might be helpful if you\u0026rsquo;re new to R or RStudio.\n  Please open RStudio before Code Club to test things out \u0026ndash; if you run into issues, join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n   Session Goals  Differentiate between different types of joins\u0026hellip;  inner_join() full_join() left_join() right_join()   Use a join function to add new variables to the birds dataset Keep practicing with dplyr core verbs from last week, esp\u0026hellip;  select() filter()   Answer the question \u0026ldquo;What Ohio bird species have the longest and shortest average lifespans?\u0026rdquo;.   Intro: Merging/Joining Datasets Sometimes you don\u0026rsquo;t have all your data in the same place. For example, maybe you have multiple Excel sheets for a project - each storing a different type of data for the same set of samples. Or maybe you\u0026rsquo;re interested in analyzing various metrics for US states and are getting the data from different places online - economic data from one database, climate data from another, and so on. As part of the process of data wrangling, it\u0026rsquo;s often useful to merge the separate datasets together according to a variable they share, possibly \u0026ldquo;SampleID\u0026rdquo; or \u0026ldquo;State Name\u0026rdquo; for the two above examples, respectively. R offers several ways to do this, but we\u0026rsquo;ll focus here on the set of *_join() functions available in dplyr. They include\u0026hellip;\n inner_join() full_join() left_join() right_join() semi_join() anti_join()  Check out the \u0026lsquo;Combine Data Sets\u0026rsquo; section of this cheat sheet for a brief look at these functions.\nYou can also get more details here, or, as with any R function, by accessing the function\u0026rsquo;s documentation inside R with the \u0026lsquo;?\u0026rsquo;. For example, type ?inner_join at your R prompt and hit Enter. (Make sure the package the function comes from is loaded first! In this case, you need dplyr, which is loaded as part of tidyverse.)\n Examples Below we\u0026rsquo;ll go through a few examples of joins. You\u0026rsquo;re welcome to follow along and run this code on your own, but it\u0026rsquo;s not necessary - the exercises in the breakout rooms are independent of these examples and will give you a chance to try these things out on your own.\nIf you want to follow along, you can find the code here.\n Since the *_join() functions come from the dplyr package, which is part of tidyverse, I\u0026rsquo;ll load that first\u0026hellip;\n#this assumes you've already installed tidyverse library(tidyverse)   The National Health and Nutrition Examination Survey (NHANES) dataset contains survey data obtained annually from ~5,000 individuals on a variety of health and lifestyle-related metrics. A subset of the data are available as an R package - install and load it\u0026hellip;\ninstall.packages(\"NHANES\", repos = \"http://cran.us.r-project.org\") #\u0026gt;  #\u0026gt; The downloaded binary packages are in #\u0026gt; /var/folders/s7/y_mgh3c54h9fjcyw9wqdkb8x4zs_jy/T//RtmpdHHxzY/downloaded_packages library(NHANES)   Now preview the dataset\u0026hellip;\nglimpse(NHANES) #\u0026gt; Rows: 10,000 #\u0026gt; Columns: 76 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 516… #\u0026gt; $ SurveyYr \u0026lt;fct\u0026gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, male, male, male, female, male, male, female, … #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10,… #\u0026gt; $ AgeDecade \u0026lt;fct\u0026gt; 30-39, 30-39, 30-39, 0-9, 40-49, 0-9, 0-9, 4… #\u0026gt; $ AgeMonths \u0026lt;int\u0026gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795… #\u0026gt; $ Race1 \u0026lt;fct\u0026gt; White, White, White, Other, White, White, White, Whi… #\u0026gt; $ Race3 \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, High School, High School, NA, Some Coll… #\u0026gt; $ MaritalStatus \u0026lt;fct\u0026gt; Married, Married, Married, NA, LivePartner, NA, NA, … #\u0026gt; $ HHIncome \u0026lt;fct\u0026gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, … #\u0026gt; $ HHIncomeMid \u0026lt;int\u0026gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 875… #\u0026gt; $ Poverty \u0026lt;dbl\u0026gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00… #\u0026gt; $ HomeRooms \u0026lt;int\u0026gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3… #\u0026gt; $ HomeOwn \u0026lt;fct\u0026gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, … #\u0026gt; $ Work \u0026lt;fct\u0026gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, … #\u0026gt; $ Weight \u0026lt;dbl\u0026gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7… #\u0026gt; $ Length \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HeadCirc \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.… #\u0026gt; $ BMICatUnder20yrs \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ BMI_WHO \u0026lt;fct\u0026gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plu… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, … #\u0026gt; $ BPSysAve \u0026lt;int\u0026gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111,… #\u0026gt; $ BPDiaAve \u0026lt;int\u0026gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, … #\u0026gt; $ BPSys1 \u0026lt;int\u0026gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124,… #\u0026gt; $ BPDia1 \u0026lt;int\u0026gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, … #\u0026gt; $ BPSys2 \u0026lt;int\u0026gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108,… #\u0026gt; $ BPDia2 \u0026lt;int\u0026gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, … #\u0026gt; $ BPSys3 \u0026lt;int\u0026gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114,… #\u0026gt; $ BPDia3 \u0026lt;int\u0026gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, … #\u0026gt; $ Testosterone \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ DirectChol \u0026lt;dbl\u0026gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, … #\u0026gt; $ TotChol \u0026lt;dbl\u0026gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, … #\u0026gt; $ UrineVol1 \u0026lt;int\u0026gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113,… #\u0026gt; $ UrineFlow1 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1… #\u0026gt; $ UrineVol2 \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ UrineFlow2 \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ Diabetes \u0026lt;fct\u0026gt; No, No, No, No, No, No, No, No, No, No, No, No, No, … #\u0026gt; $ DiabetesAge \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ HealthGen \u0026lt;fct\u0026gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vg… #\u0026gt; $ DaysPhysHlthBad \u0026lt;int\u0026gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0… #\u0026gt; $ DaysMentHlthBad \u0026lt;int\u0026gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA… #\u0026gt; $ LittleInterest \u0026lt;fct\u0026gt; Most, Most, Most, NA, Several, NA, NA, None, None, N… #\u0026gt; $ Depressed \u0026lt;fct\u0026gt; Several, Several, Several, NA, Several, NA, NA, None… #\u0026gt; $ nPregnancies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, … #\u0026gt; $ nBabies \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, N… #\u0026gt; $ Age1stBaby \u0026lt;int\u0026gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ SleepHrsNight \u0026lt;int\u0026gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, … #\u0026gt; $ SleepTrouble \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, … #\u0026gt; $ PhysActive \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes,… #\u0026gt; $ PhysActiveDays \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2,… #\u0026gt; $ TVHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ CompHrsDay \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … #\u0026gt; $ TVHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, … #\u0026gt; $ CompHrsDayChild \u0026lt;int\u0026gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, … #\u0026gt; $ Alcohol12PlusYr \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ AlcoholDay \u0026lt;int\u0026gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA,… #\u0026gt; $ AlcoholYear \u0026lt;int\u0026gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, … #\u0026gt; $ SmokeNow \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA,… #\u0026gt; $ Smoke100 \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No,… #\u0026gt; $ Smoke100n \u0026lt;fct\u0026gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smok… #\u0026gt; $ SmokeAge \u0026lt;int\u0026gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, … #\u0026gt; $ Marijuana \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Y… #\u0026gt; $ AgeFirstMarij \u0026lt;int\u0026gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, … #\u0026gt; $ RegularMarij \u0026lt;fct\u0026gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes… #\u0026gt; $ AgeRegMarij \u0026lt;int\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, … #\u0026gt; $ HardDrugs \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes,… #\u0026gt; $ SexEver \u0026lt;fct\u0026gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, … #\u0026gt; $ SexAge \u0026lt;int\u0026gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, … #\u0026gt; $ SexNumPartnLife \u0026lt;int\u0026gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA,… #\u0026gt; $ SexNumPartYear \u0026lt;int\u0026gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1… #\u0026gt; $ SameSex \u0026lt;fct\u0026gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, … #\u0026gt; $ SexOrientation \u0026lt;fct\u0026gt; Heterosexual, Heterosexual, Heterosexual, NA, Hetero… #\u0026gt; $ PregnantNow \u0026lt;fct\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …   To try out merging/joining, we\u0026rsquo;ll create two separate data frames by pulling out some variables from this NHANES dataset. One will contain demographic variables, and the other with have some physical measurements. Then we\u0026rsquo;ll join them back together. Let\u0026rsquo;s create the two sub-datasets first\u0026hellip;\n#Filter out rows with data from 2009-2010 and Age \u0026gt; 5,  #select a subset (4) of the variables, then get rid of  #all duplicate rows. Assign the output to object 'dem_data'. dem_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Age \u0026gt; 5) %\u0026gt;% select(ID, Gender, Age, Education) %\u0026gt;% distinct() #similar as above, but with a different filter and  #selecting different variables. Save as 'phys_data' phys_data \u0026lt;- NHANES %\u0026gt;% filter(SurveyYr == \"2009_10\") %\u0026gt;% filter(Height \u0026lt; 180) %\u0026gt;% select(ID, Height, BMI, Pulse) %\u0026gt;% distinct()   Now explore them a bit\u0026hellip;\n#view the first 6 rows of each - note the shared ID column head(dem_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Gender Age Education  #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 51624 male 34 High School  #\u0026gt; 2 51630 female 49 Some College #\u0026gt; 3 51638 male 9 NA  #\u0026gt; 4 51646 male 8 NA  #\u0026gt; 5 51647 female 45 College Grad #\u0026gt; 6 51654 male 66 Some College head(phys_data) #\u0026gt; # A tibble: 6 x 4 #\u0026gt; ID Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 165. 32.2 70 #\u0026gt; 2 51625 105. 15.3 NA #\u0026gt; 3 51630 168. 30.6 86 #\u0026gt; 4 51638 133. 16.8 82 #\u0026gt; 5 51646 131. 20.6 72 #\u0026gt; 6 51647 167. 27.2 62 #preview in another way - note the different numbers of observations (rows) glimpse(dem_data) #\u0026gt; Rows: 3,217 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51630, 51638, 51646, 51647, 51654, 51656, 51657, 516… #\u0026gt; $ Gender \u0026lt;fct\u0026gt; male, female, male, male, female, male, male, male, female,… #\u0026gt; $ Age \u0026lt;int\u0026gt; 34, 49, 9, 8, 45, 66, 58, 54, 10, 58, 50, 9, 33, 60, 16, 56… #\u0026gt; $ Education \u0026lt;fct\u0026gt; High School, Some College, NA, NA, College Grad, Some Colle… glimpse(phys_data) #\u0026gt; Rows: 3,021 #\u0026gt; Columns: 4 #\u0026gt; $ ID \u0026lt;int\u0026gt; 51624, 51625, 51630, 51638, 51646, 51647, 51654, 51657, 51659,… #\u0026gt; $ Height \u0026lt;dbl\u0026gt; 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 169.5, 169.4, 141.8,… #\u0026gt; $ BMI \u0026lt;dbl\u0026gt; 32.22, 15.30, 30.57, 16.82, 20.64, 27.24, 23.67, 26.03, 19.20,… #\u0026gt; $ Pulse \u0026lt;int\u0026gt; 70, NA, 86, 82, 72, 62, 60, 76, 80, 94, 74, 92, 84, 76, 64, 70…   Let\u0026rsquo;s use the shared ID column to join the two datasets together. We\u0026rsquo;ll do this in 4 different ways to compare different types of joins: inner_join(), left_join(), right_join(), and full_join(). Pay attention to the number of rows in the joined dataset each time and how it relates to the number of rows in each of the two individual datasets.\nThe basic structure of the dplyr *_join() functions is\u0026hellip;\n*_join(dataframe 'x', dataframe 'y', by = shared column name)\n 1 - inner_join() #perform an inner join join_inner \u0026lt;- inner_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_inner) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_inner) #\u0026gt; [1] 2806 7   2 - left_join() #perform an left join join_left \u0026lt;- left_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_left) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_left) #\u0026gt; [1] 3217 7   3 - right_join() #perform an right join join_right \u0026lt;- right_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_right) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_right) #\u0026gt; [1] 3021 7   4 - full_join() #perform an full join join_full \u0026lt;- full_join(dem_data, phys_data, by = \"ID\") #preview the new object head(join_full) #\u0026gt; # A tibble: 6 x 7 #\u0026gt; ID Gender Age Education Height BMI Pulse #\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 51624 male 34 High School 165. 32.2 70 #\u0026gt; 2 51630 female 49 Some College 168. 30.6 86 #\u0026gt; 3 51638 male 9 NA 133. 16.8 82 #\u0026gt; 4 51646 male 8 NA 131. 20.6 72 #\u0026gt; 5 51647 female 45 College Grad 167. 27.2 62 #\u0026gt; 6 51654 male 66 Some College 170. 23.7 60 #get dimensions dim(join_full) #\u0026gt; [1] 3432 7    Breakout rooms We\u0026rsquo;re going to add to our backyard birds dataset. I found a dataset that has life history data for a large number of species (birds and others). We\u0026rsquo;ll use species names to merge some of these life history variables in to the occurrence data we already have.\nIf you\u0026rsquo;re new and haven\u0026rsquo;t yet gotten the backyard bird dataset, get it first by running the code below. Otherwise, you can skip this step\u0026hellip;\n# create a directory called data that contains a subdirectory called birds dir.create('data/birds/', recursive = TRUE) # set the location of the file birds_file_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # set the path for the downloaded file birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' #download download.file(url = birds_file_url, destfile = birds_file)   Now (everybody), read in the bird data for this session\u0026hellip;\nbirds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' birds \u0026lt;- read_tsv(birds_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Exercise 1  Reduce the backyard bird dataset and keep just the following columns: species, locality, stateProvince, eventDate, species_en\n  Hints (click here)  Use select() to pull out the columns you want.    Solution (click here)  birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en)      Exercise 2  Check to make sure things look right - how many columns does the birds dataset now have?\n  Hints (click here)  Use the dim() function. Or the ncol() function. Or glimpse(). Or head(). Or str(). Or even summary(). There\u0026rsquo;s lots of ways to do this.    Solution (click here)  dim(birds) #\u0026gt; [1] 311441 5       Exercise 3  Now download and read in the new life history dataset (tab separated) available at https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv. Then explore it a bit - how many rows and columns are there?\n  Hints (click here)  Use the download.file() function like we did previously for the bird dataset. You\u0026rsquo;ll need to define the arguments \u0026lsquo;url\u0026rsquo; and \u0026lsquo;destfile\u0026rsquo; inside the parentheses. You can put the file anywhere you want, but I\u0026rsquo;d suggest in the same directory as the bird file we got, so, for example, the destination file could be \u0026ldquo;data/birds/life_history_data.tsv\u0026rdquo;.    Solution (click here)  #download the file from online and save it as a '.tsv' file (since it's tab delimited) download.file(url = \"https://github.com/biodash/biodash.github.io/raw/master/assets/data/birds/esa_life_history_data_cc.tsv\", destfile = \"data/birds/life_history_data.tsv\") #read the data in to R as an object named 'life_hist' life_hist \u0026lt;- read_tsv(file = \"data/birds/life_history_data.tsv\") #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; common_name = col_character(), #\u0026gt; female_maturity_d = col_double(), #\u0026gt; litter_or_clutch_size_n = col_double(), #\u0026gt; litters_or_clutches_per_y = col_double(), #\u0026gt; adult_body_mass_g = col_double(), #\u0026gt; maximum_longevity_y = col_double(), #\u0026gt; egg_mass_g = col_double(), #\u0026gt; incubation_d = col_double(), #\u0026gt; fledging_age_d = col_double(), #\u0026gt; longevity_y = col_double(), #\u0026gt; adult_svl_cm = col_double() #\u0026gt; ) #preview the data glimpse(life_hist) #\u0026gt; Rows: 21,322 #\u0026gt; Columns: 16 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Av… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Accipitriformes\", \"Accipitriformes\", \"Acci… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Accipitridae\", \"Accipitridae\", \"Accipitrid… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Accipiter\", \"Accipiter\", \"Accipiter\", \"Acc… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\"… #\u0026gt; $ common_name \u0026lt;chr\u0026gt; \"Pied Goshawk\", \"Shikra\", \"Bicolored Hawk\",… #\u0026gt; $ female_maturity_d \u0026lt;dbl\u0026gt; NA, 363.468, NA, NA, 363.468, NA, NA, 547.8… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.2… #\u0026gt; $ litters_or_clutches_per_y \u0026lt;dbl\u0026gt; NA, 1, NA, NA, 1, NA, NA, 1, NA, 1, NA, 1, … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500… #\u0026gt; $ maximum_longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 19.90000, NA, 2… #\u0026gt; $ egg_mass_g \u0026lt;dbl\u0026gt; NA, 21.00, 32.00, NA, 21.85, NA, 32.00, 19.… #\u0026gt; $ incubation_d \u0026lt;dbl\u0026gt; NA, 30.00, NA, NA, 32.50, NA, NA, 33.00, NA… #\u0026gt; $ fledging_age_d \u0026lt;dbl\u0026gt; NA, 32.00, NA, NA, 42.50, NA, NA, 24.25, NA… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 1… #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.…       Exercise 4  This new dataset contains life history data for more than just birds. What Classes of organisms are represented in the \u0026lsquo;Class\u0026rsquo; variable?\n  Hints (click here)  Try using a combination of the select() and distinct() functions to pull out the column you\u0026rsquo;re interested in, and then to get the distinct values, respectively.    Solutions (click here)  life_hist %\u0026gt;% select(class) %\u0026gt;% distinct() #\u0026gt; # A tibble: 3 x 1 #\u0026gt; class  #\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Aves  #\u0026gt; 2 Mammalia #\u0026gt; 3 Reptilia       Exercise 5  Reduce the life history dataset down to keep just the rows for Class Aves and the columns species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n. What are the dimensions now?\n  Hints (click here)  Use filter() along with an appropriate logical expression to keep the rows we want. Use select() to get the desired columns.    Solutions (click here)  # pull out target rows and columns life_hist_aves \u0026lt;- life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) dim(life_hist_aves) #\u0026gt; [1] 9802 5       Exercise 6  Preview each dataset again, just to make sure you\u0026rsquo;re clear about what\u0026rsquo;s in each one. Are there any columns that are shared between the two?\n  Hints (click here)  Consider glimpse() or head() to preview the datasets (tibbles/data frames). If you want to use a function to find shared columns, try a combination of intersect() and names().\n   Solutions (click here)  glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… intersect(names(birds), names(life_hist_aves)) #\u0026gt; [1] \"species\"       Exercise 7  Now lets join them together based on their shared variable. Not all species in the backyard bird (Ohio) dataset are included in the life history dataset. Likewise, there are life history data for many species that aren\u0026rsquo;t in the Ohio dataset. We want to keep all the Ohio observations, and merge in life history data for species where it\u0026rsquo;s availble, but we also don\u0026rsquo;t want to add in life history data for species that aren\u0026rsquo;t in the Ohio dataset. Choose an appropriate join function with those things in mind.\n  Hints (click here)  Try a left_join(), defining the Ohio backyard bird dataset as the \u0026lsquo;x\u0026rsquo; dataset in the join and the life history data as the \u0026lsquo;y\u0026rsquo; dataset. Get details on that function with ?left_join.    Solutions (click here)  joined_data \u0026lt;- left_join(x = birds, y = life_hist_aves, by = \"species\")       Exercise 8  What are the longest- and shortest-living bird species in Ohio based on the data in the longevity_y column?\n  Hints (click here)  Try using select() to pull out just the columns species and longevity_y, then use distinct() to get the unique rows, then arrange() based on the longevity_y column. You might also find the dplyr function desc() helpful.\nAlternatively, you could try grouping by species, then use summarise() to get either the max, min, or mean value for longevity_y for each species (there\u0026rsquo;s just one value for each species, so all of those statistics give the same value in this case). Then sort (arrange) the resulting summarized data frame on the longevity value.\n   Solutions (click here)  #option 1 - shortest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(longevity_y) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Certhia americana 4.6  #\u0026gt; 4 Acanthis hornemanni 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 1 - longest-lived birds joined_data %\u0026gt;% select(species, longevity_y) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(longevity_y)) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity_y #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows #option 2 - shortest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(longevity) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Loxia leucoptera 4  #\u0026gt; 2 Spiza americana 4  #\u0026gt; 3 Acanthis hornemanni 4.6  #\u0026gt; 4 Certhia americana 4.6  #\u0026gt; 5 Tringa flavipes 4.75 #\u0026gt; 6 Podiceps grisegena 4.8  #\u0026gt; 7 Calcarius lapponicus 5  #\u0026gt; 8 Anthus rubescens 5.1  #\u0026gt; 9 Perdix perdix 5.17 #\u0026gt; 10 Regulus satrapa 5.32 #\u0026gt; # … with 161 more rows #option 2 - longest-lived birds joined_data %\u0026gt;% group_by(species) %\u0026gt;% summarise(longevity = max(longevity_y)) %\u0026gt;% arrange(desc(longevity)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 171 x 2 #\u0026gt; species longevity #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Larus argentatus 33.4 #\u0026gt; 2 Larus glaucoides 33  #\u0026gt; 3 Larus thayeri 33  #\u0026gt; 4 Haliaeetus leucocephalus 33.0 #\u0026gt; 5 Larus fuscus 32.8 #\u0026gt; 6 Aquila chrysaetos 32  #\u0026gt; 7 Anas platyrhynchos 29  #\u0026gt; 8 Larus delawarensis 28.6 #\u0026gt; 9 Asio otus 27.8 #\u0026gt; 10 Cygnus olor 27.7 #\u0026gt; # … with 161 more rows       Bonus time! Bonus 1  What species in Ohio has the largest ratio of adult body mass to length (measured as snout vent length, or \u0026lsquo;adult_svl_cm\u0026rsquo;)?\n  Hints (click here)  Use mutate() to create a new variable containing the body mass divided by svl, then arrange the dataset using that new variable to get the species with the highest value.\n   Solutions (click here)  joined_data %\u0026gt;% mutate(ratio = adult_body_mass_g/adult_svl_cm) %\u0026gt;% select(species_en, ratio) %\u0026gt;% distinct() %\u0026gt;% arrange(desc(ratio)) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en ratio #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Mute Swan 71.8 #\u0026gt; 2 Wild Turkey 68.0 #\u0026gt; 3 Trumpeter Swan 64.9 #\u0026gt; 4 Bald Eagle 59.2 #\u0026gt; 5 Golden Eagle 56.2 #\u0026gt; 6 Canada Goose 48.3 #\u0026gt; 7 Tundra Swan 47.0 #\u0026gt; 8 Cackling Goose 44.4 #\u0026gt; 9 Snow Goose 35.1 #\u0026gt; 10 Snowy Owl 32.8 #\u0026gt; # … with 160 more rows       Bonus 2  There are 2 additional joins we haven\u0026rsquo;t talked about - semi_join() and anti_join(). Take a look at the documentation to see what these do. Use one of them to find what species in the backyard birds dataset are not in the life history dataset.\n  Hints (click here)  Use anti_join() and distinct().\n   Solutions (click here)  anti_join(birds, life_hist_aves, by = \"species\") %\u0026gt;% select(species, species_en) %\u0026gt;% distinct() #\u0026gt; # A tibble: 6 x 2 #\u0026gt; species species_en  #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;  #\u0026gt; 1 Dendrocopos pubescens Downy Woodpecker  #\u0026gt; 2 Spizelloides arborea American Tree Sparrow #\u0026gt; 3 Otus asio Eastern Screech Owl  #\u0026gt; 4 Larus minutus Little Gull  #\u0026gt; 5 Anas rubripes x platyrhynchos NA  #\u0026gt; 6 NA NA       Bonus 3  The life history dataset we downloaded above is actually a modified version of the original file, which is located at \u0026lsquo;http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\u0026rsquo;\nTry starting with the original file and repeating what we did above - merging the variables species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n in to the original birds dataset. First, make sure to get it read in correctly. Then pay attention to the species column in the life history dataset - what needs to be done before a join/merge can be performed?\n  Hints (click here)  Pay attention to how missing data are coded in this dataset (it\u0026rsquo;s -999). Also, data are very sparse for some of the variables - in other words, they have lots of missing data. This seems to cause a problem with the read_csv() function, as it only considers the first 1000 rows for the purpose of defining the class of each column. This can be a problem if all of the first 1000 rows are missing. Finally, it appears that even though this is a comma separated file (commas define the column breaks), there are a few instances where commas are used within a field. This happens in the \u0026lsquo;common name\u0026rsquo; column in a few cases where multiple common names are listed for a specific observation. This is one example of something that can become quite frustrating when trying to get data loaded in, and is worth keeping an eye out for. Fortunately, in our case, it only seems to happen for non-bird species in this dataset, which we filter out anyway, so it can be dealt with. However, if it had impacted any of the bird observations, I think fixing this might require a solution outside of R - possibly a command line approach.\n   Solutions (click here)  #download download.file(url = \"http://www.esapubs.org/archive/ecol/E096/269/Data_Files/Amniote_Database_Aug_2015.csv\", destfile = \"data/birds/orig_life_history.csv\") #read the data in to R as an object named 'full_life_hist' full_life_hist \u0026lt;- read_csv(\"data/birds/orig_life_history.csv\", na = \"-999\", col_types = cols(birth_or_hatching_svl_cm = col_double(), weaning_d = col_double(),gestation_d = col_double(), weaning_weight_g = col_double(), male_svl_cm = col_double(), female_svl_cm = col_double(), no_sex_svl_cm = col_double(), female_body_mass_at_maturity_g = col_double(), female_svl_at_maturity_cm = col_double())) #get the original version of the birds dataset birds \u0026lt;- read_tsv('data/birds/backyard-birds_Ohio.tsv') #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; ) #subset each for the columns and rows we want life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) birds \u0026lt;- birds %\u0026gt;% select(species, locality, stateProvince, eventDate, species_en) glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"albogularis\", \"badius\", \"bicolor\", \"brachyur… #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #notice the species column in the life history data doesn't include the genus name. Since the names don't match in the species column from each dataset, a join won't work. Add the genus variable in from the original life history data... life_hist_aves \u0026lt;- full_life_hist %\u0026gt;% filter(class == \"Aves\") %\u0026gt;% select(genus, species, adult_body_mass_g, adult_svl_cm, longevity_y, litter_or_clutch_size_n) #now use mutate to replace the species column so it includes both the genus and species... life_hist_aves \u0026lt;- life_hist_aves %\u0026gt;% mutate(species = paste0(genus, \" \", species)) %\u0026gt;% select(-genus) #preview again glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyanocit… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\", \"4… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\",… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 2007-0… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue J… glimpse(life_hist_aves) #\u0026gt; Rows: 9,802 #\u0026gt; Columns: 5 #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Accipiter albogularis\", \"Accipiter badius\", … #\u0026gt; $ adult_body_mass_g \u0026lt;dbl\u0026gt; 251.500, 140.000, 345.000, 142.000, 203.500, … #\u0026gt; $ adult_svl_cm \u0026lt;dbl\u0026gt; NA, 30.00, 39.50, NA, 33.50, NA, 39.50, 29.00… #\u0026gt; $ longevity_y \u0026lt;dbl\u0026gt; NA, NA, NA, NA, NA, NA, NA, 12.58333, NA, 12.… #\u0026gt; $ litter_or_clutch_size_n \u0026lt;dbl\u0026gt; NA, 3.250, 2.700, NA, 4.000, NA, 2.700, 4.250… #now we can join joined_data \u0026lt;- left_join(birds, life_hist_aves, by = \"species\")       \n","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607438884,"objectID":"ac21371122a93dfd4641f808ad457bc3","permalink":"https://biodash.github.io/codeclub/s03_joining-datasets/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/codeclub/s03_joining-datasets/","section":"codeclub","summary":"In this session of Code Club, we'll explore some methods for combining datasets according to a shared variable, with primary focus on the `*_join()` set of functions from **dplyr**. We'll also keep practicing with some of the core dplyr verbs from last session.","tags":null,"title":"Session 3: Joining Datasets","type":"codeclub"},{"authors":["Jessica Cooperstone"],"categories":null,"content":"\n Prep homework Basic computer setup   If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions, which also has pointers for if you\u0026rsquo;re new to R or RStudio.\n  If you\u0026rsquo;re able to do so, please open RStudio a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\n  New to dplyr? If you\u0026rsquo;ve never used dplyr before (or even if you have), you may find this cheat sheet useful.\n Getting Started Want to download an R script with the content from today\u0026rsquo;s session? # directory for Code Club Session 2: dir.create(\"S02\") # directory for our script # (\"recursive\" to create two levels at once.) dir.create(\"S02/scripts/\") # save the url location for today's script todays_R_script \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/content/codeclub/02_dplyr-core-verbs/2_Dplyr_one-table_verbs.R' # indicate the name of the new script file Session2_dplyr_core \u0026lt;- \"S02/scripts/Session2_script.R\" # go get that file!  download.file(url = todays_R_script, destfile = Session2_dplyr_core)    1 - What is data wrangling? It has been estimated that the process of getting your data into the appropriate formats takes about 80% of the total time of analysis. We will talk about formatting as tidy data (e.g., such that each column is a single variable, each row is a single observation, and each cell is a single value, you can learn more about tidy data here) in a future session of Code Club.\nThe package dplyr, as part of the tidyverse has a number of very helpful functions that will help you get your data into a format suitable for your analysis.\n What will we go over today\nThese five core dplyr() verbs will help you get wrangling.\n  select() - picks variables (i.e., columns) based on their names  filter() - picks observations (i.e., rows) based on their values  mutate() - makes new variables, keeps existing columns  arrange() - sorts rows based on values in columns  summarize() - reduces values down to a summary form     2 - Get ready to wrangle Let\u0026rsquo;s get set up and grab some data so that we can get familiar with these verbs\n You can do this locally, or at OSC. You can find instructions if you are having trouble here.  First load your libraries.\nlibrary(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.4.0 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   Then let\u0026rsquo;s access the iris dataset that comes pre-loaded in base R. We will take that data frame and assign it to a new object called iris_data. Then we will look at our data.\niris_data \u0026lt;- iris # look at the first 6 rows, all columns head(iris_data) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa # check the structure of iris_data glimpse(iris_data) #\u0026gt; Rows: 150 #\u0026gt; Columns: 5 #\u0026gt; $ Sepal.Length \u0026lt;dbl\u0026gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4… #\u0026gt; $ Sepal.Width \u0026lt;dbl\u0026gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3… #\u0026gt; $ Petal.Length \u0026lt;dbl\u0026gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1… #\u0026gt; $ Petal.Width \u0026lt;dbl\u0026gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0… #\u0026gt; $ Species \u0026lt;fct\u0026gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, …   This dataset contains the measurements (in cm) of Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width for three different Species of iris, setosa, versicolor, and virginica.\n 3 - Using select() select() allows you to pick certain columns to be included in your data frame.\nWe will create a dew data frame called iris_petals_species that includes the columns Species, Petal.Length and Petal.Width.\niris_petals_species \u0026lt;- iris_data %\u0026gt;% select(Species, Petal.Length, Petal.Width)   What does our new data frame look like?\nhead(iris_petals_species) #\u0026gt; Species Petal.Length Petal.Width #\u0026gt; 1 setosa 1.4 0.2 #\u0026gt; 2 setosa 1.4 0.2 #\u0026gt; 3 setosa 1.3 0.2 #\u0026gt; 4 setosa 1.5 0.2 #\u0026gt; 5 setosa 1.4 0.2 #\u0026gt; 6 setosa 1.7 0.4   Note - look what happened to the order of the columns!\nThis is not the only way to select columns.\nYou could also subset by indexing with the square brackets, but you can see how much more readable using select() is. It\u0026rsquo;s nice not to have to refer back to remember what column is which index.\niris_data_indexing \u0026lt;- iris_data[,3:5] head(iris_data_indexing) #\u0026gt; Petal.Length Petal.Width Species #\u0026gt; 1 1.4 0.2 setosa #\u0026gt; 2 1.4 0.2 setosa #\u0026gt; 3 1.3 0.2 setosa #\u0026gt; 4 1.5 0.2 setosa #\u0026gt; 5 1.4 0.2 setosa #\u0026gt; 6 1.7 0.4 setosa   iris_data_c \u0026lt;- iris_data[,c(\"Petal.Length\", \"Petal.Width\", \"Species\")] head(iris_data_c) #\u0026gt; Petal.Length Petal.Width Species #\u0026gt; 1 1.4 0.2 setosa #\u0026gt; 2 1.4 0.2 setosa #\u0026gt; 3 1.3 0.2 setosa #\u0026gt; 4 1.5 0.2 setosa #\u0026gt; 5 1.4 0.2 setosa #\u0026gt; 6 1.7 0.4 setosa     4 - Using filter()  Artwork by Allison Horst.   filter() allows you to pick certain observations (i.e, rows) based on their values to be included in your data frame.\nWe will create a new data frame that only includes information about the irises where their Species is setosa.\niris_setosa \u0026lt;- iris_data %\u0026gt;% filter(Species == \"setosa\")   Let\u0026rsquo;s check the dimensions of our data frame. Remember, our whole data set is 150 observations, and we are expecting 50 observations per Species.\ndim(iris_setosa) #\u0026gt; [1] 50 5    5 - Using mutate()  Artwork by Allison Horst.  mutate() allows you to make new variables, while keeping all your existing columns.\nLet\u0026rsquo;s make a new column that is the ratio of Sepal.Length/Sepal.Width\niris_sepal_length_to_width \u0026lt;- iris_data %\u0026gt;% mutate(Sepal.Length_div_Sepal.Width = Sepal.Length/Sepal.Width)   head(iris_sepal_length_to_width) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 5.1 3.5 1.4 0.2 setosa #\u0026gt; 2 4.9 3.0 1.4 0.2 setosa #\u0026gt; 3 4.7 3.2 1.3 0.2 setosa #\u0026gt; 4 4.6 3.1 1.5 0.2 setosa #\u0026gt; 5 5.0 3.6 1.4 0.2 setosa #\u0026gt; 6 5.4 3.9 1.7 0.4 setosa #\u0026gt; Sepal.Length_div_Sepal.Width #\u0026gt; 1 1.457143 #\u0026gt; 2 1.633333 #\u0026gt; 3 1.468750 #\u0026gt; 4 1.483871 #\u0026gt; 5 1.388889 #\u0026gt; 6 1.384615   Note \u0026ndash; see the new column location\n 6 - Using arrange() Very often you will want to order your data frame by some values. To do this, you can use arrange().\nLet\u0026rsquo;s arrange the values in our iris_data by Sepal.Length.\niris_data_sort_Sepal.Length \u0026lt;- iris_data %\u0026gt;% arrange(Sepal.Length) head(iris_data_sort_Sepal.Length) #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; 1 4.3 3.0 1.1 0.1 setosa #\u0026gt; 2 4.4 2.9 1.4 0.2 setosa #\u0026gt; 3 4.4 3.0 1.3 0.2 setosa #\u0026gt; 4 4.4 3.2 1.3 0.2 setosa #\u0026gt; 5 4.5 2.3 1.3 0.3 setosa #\u0026gt; 6 4.6 3.1 1.5 0.2 setosa   What if we want to arrange by Sepal.Length, but within Species? We can do that using the helper group_by().\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% arrange(Sepal.Length) #\u0026gt; # A tibble: 150 x 5 #\u0026gt; # Groups: Species [3] #\u0026gt; Sepal.Length Sepal.Width Petal.Length Petal.Width Species #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;  #\u0026gt; 1 4.3 3 1.1 0.1 setosa  #\u0026gt; 2 4.4 2.9 1.4 0.2 setosa  #\u0026gt; 3 4.4 3 1.3 0.2 setosa  #\u0026gt; 4 4.4 3.2 1.3 0.2 setosa  #\u0026gt; 5 4.5 2.3 1.3 0.3 setosa  #\u0026gt; 6 4.6 3.1 1.5 0.2 setosa  #\u0026gt; 7 4.6 3.4 1.4 0.3 setosa  #\u0026gt; 8 4.6 3.6 1 0.2 setosa  #\u0026gt; 9 4.6 3.2 1.4 0.2 setosa  #\u0026gt; 10 4.7 3.2 1.3 0.2 setosa  #\u0026gt; # … with 140 more rows    7 - Using summarize() By using summarize(), you can create a new data frame that has the summary output you have requested.\nWe can calculate the mean Sepal.Length across our dataset.\niris_data %\u0026gt;% summarize(mean = mean(Sepal.Length)) #\u0026gt; mean #\u0026gt; 1 5.843333   What if we want to calculate means for each Species?\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(mean = mean(Sepal.Length)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species mean #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 setosa 5.01 #\u0026gt; 2 versicolor 5.94 #\u0026gt; 3 virginica 6.59   We can integrate some helper functions into our code to simply get out a variety of outputs. We can use across() to apply our summary aross a set of columns. I really like this function.\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(across(where(is.numeric), mean)) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 5 #\u0026gt; Species Sepal.Length Sepal.Width Petal.Length Petal.Width #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 setosa 5.01 3.43 1.46 0.246 #\u0026gt; 2 versicolor 5.94 2.77 4.26 1.33  #\u0026gt; 3 virginica 6.59 2.97 5.55 2.03   This can also be useful for counting observations per group. Here, how many iris observations do we have per Species?\niris_data %\u0026gt;% group_by(Species) %\u0026gt;% tally() #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50 iris_data %\u0026gt;% count(Species) #\u0026gt; Species n #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50 iris_data %\u0026gt;% group_by(Species) %\u0026gt;% summarize(n = n()) #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) #\u0026gt; # A tibble: 3 x 2 #\u0026gt; Species n #\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 setosa 50 #\u0026gt; 2 versicolor 50 #\u0026gt; 3 virginica 50    8 - Breakout rooms! Read in data Now you try! We are going to use the Great Backyard Birds dataset we downloaded two weeks ago and you will apply the functions we have learned above to investigate this dataset.\nIf you weren\u0026rsquo;t here for Session 1, get the birds data set.\n# create a directory called S02 dir.create('S02') # within S02, create a directory called data, within, a directory called birds dir.create('data/birds/', recursive = TRUE)   Download the file from the internet.\n# set the location of the file birds_file_url \u0026lt;- 'https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv' # set the path for the downloaded file birds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' # download  download.file(url = birds_file_url, destfile = birds_file)   If you were here for Session 1, join back in! Let\u0026rsquo;s read in our data.\nbirds_file \u0026lt;- 'data/birds/backyard-birds_Ohio.tsv' birds \u0026lt;- read_tsv(birds_file) #\u0026gt;  #\u0026gt; ── Column specification ──────────────────────────────────────────────────────── #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Exercises Below you can find our breakout room exercises for today.\nExercise 1  Investigate the structure of the birds dataset.\n  Solution (click here)  glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …       Exercise 2  Create a new data frame that removes the column range.\n  Hints (click here)  \nTry using select(). Remember, you can tell select() what you want to keep, and what you want to remove.    Solutions (click here)  birds_no_range \u0026lt;- birds %\u0026gt;% select(-range) head(birds_no_range) #\u0026gt; # A tibble: 6 x 11 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Pass… Corvi… Cyan… Cyanoc… 44805 A… Ohio 40.9 #\u0026gt; 2 Aves Pass… Corvi… Cyan… Cyanoc… 45244 C… Ohio 39.1 #\u0026gt; 3 Aves Pass… Corvi… Cyan… Cyanoc… 44132 E… Ohio 41.6 #\u0026gt; 4 Aves Pass… Corvi… Cyan… Cyanoc… 45242 C… Ohio 39.2 #\u0026gt; 5 Aves Pass… Corvi… Cyan… Cyanoc… 45246 C… Ohio 39.3 #\u0026gt; 6 Aves Pass… Corvi… Cyan… Cyanoc… 44484 W… Ohio 41.2 #\u0026gt; # … with 3 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;       Exercise 3  How many unique species of birds have been observed?.\n  Hints (click here)  Try using summarize() with a group_by() helper.    Solutions (click here)  # using a combo of group_by() and summarize() unique_birds \u0026lt;- birds %\u0026gt;% group_by(species_en) %\u0026gt;% summarize() #\u0026gt; `summarise()` ungrouping output (override with `.groups` argument) dim(unique_birds) # question - are there really 170 different birds observed? take a look at this summary #\u0026gt; [1] 170 1 # a one line, base R approach length(unique(birds$species_en)) #\u0026gt; [1] 170 # another base R approach using distinct() and nrow() birds %\u0026gt;% distinct(species_en) %\u0026gt;% # find distinct occurences nrow() # counts rows #\u0026gt; [1] 170 # using n_distinct() birds %\u0026gt;% summarize(n_distinct(species_en)) #\u0026gt; # A tibble: 1 x 1 #\u0026gt; `n_distinct(species_en)` #\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 170       Exercise 4  How many times have Bald Eagles been observed?.\n  Hints (click here)  Try using filter(). Remember the syntax you need to use to indicate you are looking for a Bald Eagle.    Solutions (click here)  birds_bald_eagle \u0026lt;- birds %\u0026gt;% filter(species_en == \"Bald Eagle\") dim(birds_bald_eagle) #\u0026gt; [1] 381 12       Exercise 5  How many times have any kind of eagle been observed?. Group hint: there are only Bald Eagle and Golden Eagle in this dataset.\n  Hints (click here)  There is a way to denote OR within filter().    More Hints (click here)  You denote OR by using the vertical bar.    Solutions (click here)  birds_alleagles \u0026lt;- birds %\u0026gt;% filter(species_en == \"Bald Eagle\" | species_en == \"Golden Eagle\") dim(birds_alleagles) #\u0026gt; [1] 386 12       Exercise 6  What is the northern most location of the bird observations in Ohio?\n  Hints (click here)  Try using arrange(). You can arrange in both ascending and descending order. You can also use your Ohio knowledge to check if you\u0026rsquo;ve done this correctly.    Solutions (click here)  birds_sort_lat \u0026lt;- birds %\u0026gt;% arrange(-decimalLatitude) head(birds_sort_lat) #\u0026gt; # A tibble: 6 x 12 #\u0026gt; class order family genus species locality stateProvince decimalLatitude #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; #\u0026gt; 1 Aves Pass… Cardi… Card… Cardin… Conneaut Ohio 41.9 #\u0026gt; 2 Aves Pass… Ember… Zono… Zonotr… Conneaut Ohio 41.9 #\u0026gt; 3 Aves Colu… Colum… Zena… Zenaid… Conneaut Ohio 41.9 #\u0026gt; 4 Aves Pici… Picid… Dend… Dendro… Conneaut Ohio 41.9 #\u0026gt; 5 Aves Anse… Anati… Anas Anas p… Conneaut Ohio 41.9 #\u0026gt; 6 Aves Pass… Turdi… Sial… Sialia… Conneaut Ohio 41.9 #\u0026gt; # … with 4 more variables: decimalLongitude \u0026lt;dbl\u0026gt;, eventDate \u0026lt;dttm\u0026gt;, #\u0026gt; # species_en \u0026lt;chr\u0026gt;, range \u0026lt;chr\u0026gt;       Bonus time! Bonus 1  What is the most commonly observed bird in Ohio?\n  Hints (click here)  Try using tally() and a little helper term.\n   Solutions (click here)  unique_birds_tally \u0026lt;- birds %\u0026gt;% group_by(species_en) %\u0026gt;% tally(sort = TRUE) head(unique_birds_tally) #\u0026gt; # A tibble: 6 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Northern Cardinal 23064 #\u0026gt; 2 Mourning Dove 19135 #\u0026gt; 3 Dark-eyed Junco 18203 #\u0026gt; 4 Downy Woodpecker 17196 #\u0026gt; 5 House Sparrow 15939 #\u0026gt; 6 Blue Jay 15611 # another option birds %\u0026gt;% count(species_en, sort = TRUE) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Northern Cardinal 23064 #\u0026gt; 2 Mourning Dove 19135 #\u0026gt; 3 Dark-eyed Junco 18203 #\u0026gt; 4 Downy Woodpecker 17196 #\u0026gt; 5 House Sparrow 15939 #\u0026gt; 6 Blue Jay 15611 #\u0026gt; 7 American Goldfinch 14732 #\u0026gt; 8 House Finch 14551 #\u0026gt; 9 Tufted Titmouse 14409 #\u0026gt; 10 Black-capped Chickadee 13471 #\u0026gt; # … with 160 more rows       Bonus 2  What is the least commonly observed bird (or birds) in Ohio?\n  Hints (click here)  Try using the data frame you\u0026rsquo;ve created in the previous exercise.    Solutions (click here)  unique_birds_tally %\u0026gt;% arrange(n) #\u0026gt; # A tibble: 170 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Arctic Redpoll 1 #\u0026gt; 2 Clay-colored Sparrow 1 #\u0026gt; 3 Dickcissel 1 #\u0026gt; 4 Eurasian Wigeon 1 #\u0026gt; 5 Great Egret 1 #\u0026gt; 6 Green Heron 1 #\u0026gt; 7 Grey Partridge 1 #\u0026gt; 8 Harris's Sparrow 1 #\u0026gt; 9 Lesser Yellowlegs 1 #\u0026gt; 10 Lincoln's Sparrow 1 #\u0026gt; # … with 160 more rows # or, if you knew the rarest was those observed only once  unique_birds_tally %\u0026gt;% filter(n == 1) #\u0026gt; # A tibble: 19 x 2 #\u0026gt; species_en n #\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 Arctic Redpoll 1 #\u0026gt; 2 Clay-colored Sparrow 1 #\u0026gt; 3 Dickcissel 1 #\u0026gt; 4 Eurasian Wigeon 1 #\u0026gt; 5 Great Egret 1 #\u0026gt; 6 Green Heron 1 #\u0026gt; 7 Grey Partridge 1 #\u0026gt; 8 Harris's Sparrow 1 #\u0026gt; 9 Lesser Yellowlegs 1 #\u0026gt; 10 Lincoln's Sparrow 1 #\u0026gt; 11 Loggerhead Shrike 1 #\u0026gt; 12 Nelson's Sparrow 1 #\u0026gt; 13 Northern Rough-winged Swallow 1 #\u0026gt; 14 Orchard Oriole 1 #\u0026gt; 15 Prairie Falcon 1 #\u0026gt; 16 Red-throated Loon 1 #\u0026gt; 17 Ross's Goose 1 #\u0026gt; 18 Warbling Vireo 1 #\u0026gt; 19 Western Osprey 1       Bonus 3  In what year were the most Bald Eagles observed?\n  Hints (click here)  You may want to convert your date column to a more simplified year-only date. Check out the package lubridate.    Solutions (click here)  library(lubridate) #\u0026gt;  #\u0026gt; Attaching package: 'lubridate' #\u0026gt; The following objects are masked from 'package:base': #\u0026gt;  #\u0026gt; date, intersect, setdiff, union birds_bald_eagle_year \u0026lt;- birds_bald_eagle %\u0026gt;% mutate(year = year(eventDate)) %\u0026gt;% # year() takes a date and outputs only year group_by(year) %\u0026gt;% tally() arrange(birds_bald_eagle_year, -n) #\u0026gt; # A tibble: 11 x 2 #\u0026gt; year n #\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; #\u0026gt; 1 2008 81 #\u0026gt; 2 2006 66 #\u0026gt; 3 2009 58 #\u0026gt; 4 2007 40 #\u0026gt; 5 2005 30 #\u0026gt; 6 2004 26 #\u0026gt; 7 2000 23 #\u0026gt; 8 2001 23 #\u0026gt; 9 2003 15 #\u0026gt; 10 2002 14 #\u0026gt; 11 1999 5      \n","date":1606694400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606593313,"objectID":"83eb9b3aa4eba14930c3c05a7e4ad0bc","permalink":"https://biodash.github.io/codeclub/02_dplyr-core-verbs/","publishdate":"2020-11-30T00:00:00Z","relpermalink":"/codeclub/02_dplyr-core-verbs/","section":"codeclub","summary":"During this second session of Code Club, we will be learning how to use some of the most popular dplyr one-table functions, including filter, select, mutate, arrange, and summarize.","tags":null,"title":"Session 2: dplyr core verbs","type":"codeclub"},{"authors":["Jelmer Poelstra"],"categories":[],"content":"\n Prep homework Basic computer setup If you didn\u0026rsquo;t already do this, please follow the Code Club Computer Setup instructions.\nTest if it works Please open RStudio locally or start an OSC RStudio Server session.\nNov 19 addition: If you\u0026rsquo;re working locally, test if you can load the tidyverse package with library(\u0026quot;tidyverse\u0026quot;) inside R. (If you haven\u0026rsquo;t installed the tidyverse yet, please go to the Code Club Computer Setup instructions.)\nIf you have not used RStudio before, take a moment to explore what\u0026rsquo;s in the panels and tabs. (It may help to check out Mike Sovic\u0026rsquo;s 1-minute intro to the RStudio interface or RStudio\u0026rsquo;s 3-minute intro.)\nIf you\u0026rsquo;re able to do so, please open RStudio again a bit before Code Club starts \u0026ndash; and in case you run into issues, please join the Zoom call early and we\u0026rsquo;ll troubleshoot.\nNew to R? If you\u0026rsquo;re completely new to R, it will be useful to have a look at some of the resources listed on our New to R? page prior to Code Club.\n Slides On Friday, we started with a couple of introductory slides.\n  1 - Create an RStudio Project Projects are an RStudio-specific concept that create a special file (.Rproj), primarily to designate a directory as the working directory for everything within it. We recommend creating exactly one separate Project for each research project with an R component \u0026ndash; and for things like Code Club.\n Why use Projects?\nIn brief, Projects help you to organize your work and to make it more portable.\n  They record which scripts (and R Markdown files) are open in RStudio, and will reopen all of those when you reopen the project. This becomes quite handy, say, when you work on three different projects, each of which uses a number of scripts.\n  When using Projects, you generally don\u0026rsquo;t have to manually set your working directory, and can use relative file paths to refer to files within the project. This way, even if you move the project directory, or copy it to a different computer, the same paths will still work. (This would not be the case if you used setwd() which will generally require you to use an absolute path, e.g. setwd(\u0026quot;C:/Users/Jelmer/Documents/\u0026quot;).)\n  Projects encourage you to organize research projects inside self-contained directories, rather than with files spread around your computer. This can save you a lot of headaches and increases reproducibility. And because R will restart whenever you switch Projects, there is no risk of unwanted cross-talk between your projects.\n    Let\u0026rsquo;s create an RStudio Project for Code Club:\n  Open RStudio locally or start an OSC RStudio Server session.\n(If you\u0026rsquo;re at OSC, you should see a file 0_CODECLUB.md that\u0026rsquo;s open in your top-left panel. You can ignore/close this file.)\n  If you\u0026rsquo;re working locally, create a directory wherever you like on your computer for all things Code Club. You can do this in R using dir.create(\u0026quot;path/to/your/dir\u0026quot;), or outside of R.\n(If you\u0026rsquo;re at OSC, skip this step because you\u0026rsquo;re automatically inside a Code Club-specific, personal directory.)\n  Click File (top menu bar) \u0026gt; New Project, and then select Existing Directory.\n  If you\u0026rsquo;re working locally, select the Code Club directory that you created in the previous step.\n  If you\u0026rsquo;re working at OSC, keep the default choice \u0026ldquo; ~\u0026rdquo; (i.e., home), which is the directory you started in when entering the RStudio Server session.\n    After RStudio automatically reloads, you should see the file ending in .Rproj in the RStudio Files tab in the lower right pane, and you will have the Project open. All done for now!\n  (For future Code Club sessions: RStudio will by default reopen the most recently used Project, and therefore, OSC users will have the Project automatically opened. If you\u0026rsquo;re working locally and are also using other Projects, you can open this Project with File \u0026gt; Open Project inside RStudio, or by clicking the .Rproj file in your file browser, which will open RStudio and the Project.)\n 2 - Orienting ourselves Where are we? We don\u0026rsquo;t need to set our working directory, because our newly created Project is open, and therefore, our working directory is the directory that contains the .Rproj file.\nTo see where you are, type or copy into the console (bottom left):\n# Print the working directory: getwd() # List the files in your current directory: dir() # This should print at least the `.RProj` file.   Create directories Create two new directories \u0026ndash; one for this session, and one for a dataset that we will download shortly (and will be reusing across sessions):\n# Dir for Code Club Session 1: dir.create(\"S01\") # Dir for our bird data: # (\"recursive\" to create two levels at once.) dir.create(\"data/birds/\", recursive = TRUE)   Create a script To keep a record of what we are doing, and to easily modify and rerun earlier commands, we\u0026rsquo;ll want to save our commands in a script and execute them from there, rather than typing our commands directly in the console.\n  Click File (top menu bar) \u0026gt; New File \u0026gt; R script.\n  Save the script (File \u0026gt; Save) as S01.R inside your S01 directory.\n  First line of the script We will now load the core set of 8 tidyverse packages all at once. To do so, type/copy the command below on the first line of the script, and then execute it by clicking Run (top right of script pane) or by pressing Ctrl Enter (Windows/Linux, this should also work in your browser) or ⌘ Enter (Mac).\n# If you're working locally, and did not install it yet: # install.packages(\"tidyverse\") # Load the tidyverse (meta)package: library(tidyverse) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr  0.3.4 #\u0026gt; ✔ tibble  3.0.4 ✔ dplyr  1.0.2 #\u0026gt; ✔ tidyr  1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr  1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag()   If this worked, you should get the same output as shown in the code block above: it attached 8 packages, and it warns that some of its functions are now \u0026ldquo;masking\u0026rdquo; base R functions.\n The tidyverse is a very popular and useful ecosystem of R packages for data analysis, which we will be using a lot in Code Club.\nWhen we refer to \u0026ldquo;base R\u0026rdquo; as opposed to the tidyverse, we mean functions that are loaded in R by default (without loading a package), and that can perform similar operations in a different way.\n   3 - Getting our dataset We downloaded a Great Backyard Bird Count (GBBC) dataset from the Global Biodiversity Information Facility (GBIF). Because the file was 3.1 GB large, we selected only the records from Ohio and removed some uninformative columns. We also added columns with English names and the breeding range for each species. We\u0026rsquo;ll download the resulting much smaller file (41.5 MB) from our Github repo.\n The Great Backyard Bird Count The GBBC is an annual citizen science event where everyone is encouraged to to identify and count birds in their backyard \u0026ndash; or anywhere else \u0026ndash; for at least 15 minutes, and report their sightings online. Since 2013, it is a global event, but it has been organized in the US and Canada since 1998.\n  Download the data Let\u0026rsquo;s download the dataset using the download.file() function:\n# The URL to our file: birds_file_url \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_Ohio.tsv\" # The path to the file we want to download to: birds_file \u0026lt;- \"data/birds/backyard-birds_Ohio.tsv\" # Download: download.file(url = birds_file_url, destfile = birds_file)   Read the data Now, let\u0026rsquo;s read the file into R. The .tsv extension (\u0026ldquo;tab-separated values\u0026rdquo;) tells us this is a plain text file in which columns are separated by tabs, so we will use a convenience function from the readr package (which is loaded as part of the core set tidyverse packages) for exactly this type of file:\n# Read the data: birds \u0026lt;- read_tsv(file = birds_file) #\u0026gt; Parsed with column specification: #\u0026gt; cols( #\u0026gt; class = col_character(), #\u0026gt; order = col_character(), #\u0026gt; family = col_character(), #\u0026gt; genus = col_character(), #\u0026gt; species = col_character(), #\u0026gt; locality = col_character(), #\u0026gt; stateProvince = col_character(), #\u0026gt; decimalLatitude = col_double(), #\u0026gt; decimalLongitude = col_double(), #\u0026gt; eventDate = col_datetime(format = \"\"), #\u0026gt; species_en = col_character(), #\u0026gt; range = col_character() #\u0026gt; )   Done! We have now read our data into a tibble, which is a type of data frame (formally a data.frame): R\u0026rsquo;s object class to deal with tabular data wherein each column can contain a different type of data (numeric, characters/strings, etc).\n 4 - Exploring backyard birds Exercise 1 What\u0026rsquo;s in the dataset?\n  Explore the dataset using some functions and methods you may know to get a quick overview of data(frames), and try to understand what you see. What does a single row represent, and what is in each column? (Be sure to check out the hints below at some point, especially if you\u0026rsquo;re stuck.)\n  Pay attention to the data types (e.g., \u0026ldquo;character\u0026rdquo; or chr) of the different columns, which several of these functions print. The output of our read_tsv() command also printed this information \u0026ndash; this function parsed our columns as the types we see now. Were all the columns parsed correctly?\n  How many rows and how many columns does the dataset have?\n  What are some questions you would like to explore with this dataset? We\u0026rsquo;ll collect some of these and try to answer them in later sessions. If your group has sufficient R skills already, you are also welcome to go ahead and try to answer one or more of these questions.\n    Hints (click here)  # Type an object's name to print it to screen: birds # Same as above, but explicitly calling print(): print(birds) # For column-wise information (short for \"structure\"): str(birds) # tidyverse version of str(): glimpse(birds) # In RStudio, open object in a separate tab: View(birds)     Note that in R, dbl (for \u0026ldquo;double\u0026rdquo;) and num (for \u0026ldquo;numeric\u0026rdquo;) are both used, and almost interchangeably so, for floating point numbers. (Integers are a separate type that are simply called \u0026ldquo;integers\u0026rdquo; and abbreviated as int, but we have no integer columns in this dataset.)\n  read_tsv() parsed our date as a \u0026ldquo;date-time\u0026rdquo; (dttm or POSIXct for short), which contains both a date and a time. In our case, it looks like the time is always \u0026ldquo;00:00:00\u0026rdquo; and thus doesn\u0026rsquo;t provide any information.\n     Solutions (click here)  # Just printing the glimpse() output, # which will show the number of rows and columns: glimpse(birds) #\u0026gt; Rows: 311,441 #\u0026gt; Columns: 12 #\u0026gt; $ class \u0026lt;chr\u0026gt; \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Aves\", \"Ave… #\u0026gt; $ order \u0026lt;chr\u0026gt; \"Passeriformes\", \"Passeriformes\", \"Passeriformes\", \"… #\u0026gt; $ family \u0026lt;chr\u0026gt; \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Corvidae\", \"Cor… #\u0026gt; $ genus \u0026lt;chr\u0026gt; \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitta\", \"Cyanocitt… #\u0026gt; $ species \u0026lt;chr\u0026gt; \"Cyanocitta cristata\", \"Cyanocitta cristata\", \"Cyano… #\u0026gt; $ locality \u0026lt;chr\u0026gt; \"44805 Ashland\", \"45244 Cincinnati\", \"44132 Euclid\",… #\u0026gt; $ stateProvince \u0026lt;chr\u0026gt; \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohio\", \"Ohi… #\u0026gt; $ decimalLatitude \u0026lt;dbl\u0026gt; 40.86166, 39.10666, 41.60768, 39.24236, 39.28207, 41… #\u0026gt; $ decimalLongitude \u0026lt;dbl\u0026gt; -82.31558, -84.32972, -81.50085, -84.35545, -84.4688… #\u0026gt; $ eventDate \u0026lt;dttm\u0026gt; 2007-02-16, 2007-02-17, 2007-02-17, 2007-02-19, 200… #\u0026gt; $ species_en \u0026lt;chr\u0026gt; \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blue Jay\", \"Blu… #\u0026gt; $ range \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …   # You can also check the number of rows and columns directly using: dim(birds) # Will return the number of rows and columns #\u0026gt; [1] 311441 12 nrow(birds) # Will return the number of rows #\u0026gt; [1] 311441 ncol(birds) # Will return the number of columns #\u0026gt; [1] 12     \n  Bonus material If your breakout group is done with Exercise 1, you can have a look at the bonus material below which includes another exercise. You can also have a look at this as homework. Or not at all!\n readr options for challenging files Earlier, we successfully read in our file without specifying any arguments other than the file name to the read_tsv() function, i.e. with all the default options. It is not always this easy!\nSome options for more complex cases:\n  The more general counterpart of this function is read_delim(), which allows you to specify the delimiter using the sep argument, e.g. delim=\u0026quot;\\t\u0026quot; for tabs.\n  There are also arguments to these functions for when you need to skip lines, when you don\u0026rsquo;t have column headers, when you need to specify the column types of some or all the columns, and so forth \u0026ndash; see this example:\nmy_df \u0026lt;- read_delim( file = \"file.txt\", delim = \"\\t\", # Specify tab as delimiter col_names = FALSE, # First line is not a header skip = 3, # Skip the first three lines comment = \"#\", # Skip any line beginning with a \"#\" col_types = cols( # Specify column types col1 = col_character(), # ..We only need to specify columns for  col2 = col_double() # ..which we need non-automatic typing ) )       Exercise 2 (Optional) Read this file!\nTry to read the following file into R, which is a modified and much smaller version of the bird dataset.\nMake the function parse the \u0026ldquo;order\u0026rdquo; column as a factor, and the \u0026ldquo;year\u0026rdquo;, \u0026ldquo;month\u0026rdquo;, and \u0026ldquo;day\u0026rdquo; columns as whatever you think is sensible.\n# Download and read the file: birds2_file_url \u0026lt;- \"https://raw.githubusercontent.com/biodash/biodash.github.io/master/assets/data/birds/backyard-birds_read-challenge.txt\" birds2_file \u0026lt;- \"data/birds/backyard-birds_read-challenge.txt\" download.file(url = birds2_file_url, destfile = birds2_file)   # Your turn! birds2 \u0026lt;- read_ # Complete the command     Hints (click here)    The file is saved as .txt, so the delimiter is not obvious \u0026ndash; first have a look at it (open it in RStudio, a text editor, or the terminal) to determine the delimiter. Then, use read_delim() with manual specification of the delimiter using the delim argument, or use a specialized convenience function.\n  Besides a leading line with no data, there is another problematic line further down. You will need both the skip and comment arguments to circumvent these.\n  Note that readr erroneously parses month as a character column if you don\u0026rsquo;t manually specify its type.\n  Note that you can also use a succinct column type specification like col_types = \u0026quot;fc\u0026quot;, which would parse, for a two-column file, the first column as a factor and the second as a character \u0026ndash; type e.g. ?read_tsv for details.\n     Bare solution (click here)  # With succint column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = \"fcdiii\" ) # With long column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = cols( order = col_factor(), year = col_integer(), month = col_integer(), day = col_integer() ) )      Solution with explanations (click here)  # With succinct column type specification: birds2 \u0026lt;- read_csv( # `read_csv()`: file is comma-delimited file = birds2_file, skip = 1, # First line is not part of the dataframe comment = \"$\", # Line 228 is a comment that starts with `$` col_types = \"fcdiii\" # \"f\" for factor, \"c\" for character, ) # ..\"d\" for double (=numeric), # ..\"i\" for integer. # With long column type specification: birds2 \u0026lt;- read_csv( file = birds2_file, skip = 1, comment = \"$\", col_types = cols( # We can omit columns for which we order = col_factor(), # ..accept the automatic parsing, year = col_integer(), # ..when using the long specification.  month = col_integer(), day = col_integer() ) )      Other options for reading tabular data There are also functions in base R that read tabular data, such as read.table() and read.delim().\nThese are generally slower than the readr functions, and have less sensible default options to their arguments. Particularly relevant is how columns with characters (strings) are parsed \u0026ndash; until R 4.0, which was released earlier this year, base R\u0026rsquo;s default behavior was to parse them as factors, and this is generally not desirable1. readr functions will never convert columns with strings to factors.\nIf speed is important, such as when reading in very large files (~ 100s of MBs or larger), you should consider using the fread() function from the data.table package.\nFinally, some examples of reading other types of files:\n Read excel files directly using the readxl package. Read Google Sheets directly from the web using the googlesheets4 package. Read non-tabular data using the base R readLines() function.    \n  You can check which version of R you are running by typing sessionInfo(). You can also check directly how strings are read by default with default.stringsAsFactors(). To avoid conversion to factors, specify stringsAsFactors = FALSE in your read.table() / read.delim() function call. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1604448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605969915,"objectID":"5b8cbf273e25c833a3e7f4cd2615654b","permalink":"https://biodash.github.io/codeclub/01_backyard-birds/","publishdate":"2020-11-04T00:00:00Z","relpermalink":"/codeclub/01_backyard-birds/","section":"codeclub","summary":"In the first session of Code Club, we'll make sure that everyone is properly set up, create an RStudio Project, and start working with some data from the Great Backyard Bird Count.","tags":["codeclub","backyard-birds"],"title":"Session 1: Backyard Birds","type":"codeclub"},{"authors":["Jelmer Poelstra","Mike Sovic","Stephen Opiyo","Michael Broe","Jessica Cooperstone"],"categories":[],"content":" Welcome to OSU Code Club! Materials for each episode will be provided in posts like this one, collected in the Code Club Sessions page.\n  For more information about OSU Code Club, and a form to sign up, see the About Code Club page.\n  For info on upcoming sessions, see here.\n  You can code locally or in your browser, see our page with computer setup instructions.\n  If you are completely new to R, see our page with resources and tips.\n  You can also suggest a topic to be covered at Code Club.\n   \n","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604625794,"objectID":"fa58e17bb7648ba3c1d43d18ecb64a3a","permalink":"https://biodash.github.io/codeclub/00_welcome-to-codeclub/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/codeclub/00_welcome-to-codeclub/","section":"codeclub","summary":"Welcome to OSU Code Club! In this brief post, we point you to information related to Code Club on the website.","tags":["codeclub"],"title":"Welcome to Code Club","type":"codeclub"},{"authors":null,"categories":null,"content":"All material is released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1578092400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601687264,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://biodash.github.io/license/","publishdate":"2020-01-04T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"All material is released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":null,"categories":null,"content":"About BioDASH  The BioDASH website aims to assemble bioinformatic and computational training resources for researchers at The Ohio State University. It\u0026rsquo;s a joint initiative by bioinformaticians at OSU\u0026rsquo;s Molecular and Cellular Imaging Center (MCIC) - Computational Biology Lab, the Center for Applied Plant Sciences (CAPS), and the Department of Evolution, Ecology and Organismal Biology (EEOB).\nAs of fall 2020, this is still very much a work in progress, but more content will be added soon!\n Main Contributors    Jelmer Poelstra, MCIC Wooster\n   Mike Sovic, CAPS\n   Michael Broe, EEOB\n    \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605479556,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://biodash.github.io/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"About BioDASH  The BioDASH website aims to assemble bioinformatic and computational training resources for researchers at The Ohio State University. It\u0026rsquo;s a joint initiative by bioinformaticians at OSU\u0026rsquo;s Molecular and Cellular Imaging Center (MCIC) - Computational Biology Lab, the Center for Applied Plant Sciences (CAPS), and the Department of Evolution, Ecology and Organismal Biology (EEOB).","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"About OSU Code Club  OSU Code Club is a regularly occurring, interactive, online gathering to improve coding skills. We aim for a supportive and fun culture of learning together, and hope to offer something to participants at any experience level.\nIn each meeting, a presenter first introduces a concept or tool to be used for a challenge. Then, we work on the challenge in pairs or trios in Zoom \u0026ldquo;breakout rooms,\u0026rdquo; and finally, we reconvene to see what approaches were taken and to share lessons learned.\nThe idea for this Code Club was taken from a recent paper in PLoS Computational Biology: Ten simple rules to increase computational skills among biologists with Code Clubs. We liked this idea because of the high level of interaction and because gradual, well-spaced practice is an excellent way to retain what you learn.\nJoin us \u0026mdash; and perhaps present a session too! As the organizers, we are happy to present sessions, but we hope that as a participant, you will also want to do so.\nFor example, you could consult the group about an actual challenge you are facing in your data analysis. You could also introduce everyone to an new package or approach you\u0026rsquo;ve been using, or one that you are excited about and want to dive into \u0026ndash; teaching a topic can be one of the best ways to learn it!\n Practical information  In the first series, we will focus on all things R: from data analysis and visualization to efficient coding, R Markdown, and so on. There will be no consistent analysis type or data type \u0026mdash; instead, we will focus on building general skills and applying those to a wide variety of data. Each week, materials and suggested reading will be posted up front at the Sessions page. Like at a Journal Club, doing some preparatory homework by reading these materials will help you get the most out of it. Each session is intended to be mostly stand-alone to allow for occasional participation. To allow for a welcoming environment for participants at all levels of experience, we ask everyone to be respectful, patient, and collaborative when interacting at Code Club. This is not a competitive event. We have a separate page with computer setup instructions, where you\u0026rsquo;ll see that we also accommodate participating through your browser without any installations. We also have a form to suggest topics for Code Club!   Organizers   Jelmer Poelstra - bioinformatician at MCIC Wooster  Mike Sovic - bioinformatician at CAPS  Stephen Opiyo - biostatistician at MCIC Columbus  Michael Broe - bioinformatician at EEOB  Jessica Cooperstone - Asst. Professor at HCS \u0026amp; FST   Sign up To sign up, please fill out the Google Form below. Hope to see you at Code Club!\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605227816,"objectID":"affd8a75456abca4d01de73213cffddb","permalink":"https://biodash.github.io/codeclub-about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-about/","section":"","summary":"About OSU Code Club  OSU Code Club is a regularly occurring, interactive, online gathering to improve coding skills. We aim for a supportive and fun culture of learning together, and hope to offer something to participants at any experience level.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  R \u0026ndash; Getting Started and Some Tips  New to R? If you are completely new to R, we recommend watching at least the first couple of videos from Mike Sovic\u0026rsquo;s Youtube playlist of short videos on R, and ideally all of them, prior to attending Code Club. Here is the first video:\n  In case you want to do more self-study (note that this is not required/needed), here are some additional resources:\n A useful and fun written tutorial is R for cats. For a more systematic and lengthy introduction to R, see A Tutorial Introduction to R (this gets fairly advanced after section 9). Excellent comprehensive introductions are the R Basics and Visualization classes by Rafael Irizarry that can be freely accessed; you do have to create an account.  Also, don\u0026rsquo;t hesitate to reach out to the Code Club organizers if you have any questions!\n Miscellaneous R tips Useful settings By default, R will try to save your \u0026ldquo;environment\u0026rdquo; (e.g., your loaded data, variables, etc) when you exit, and then reload everything the way it was upon restarting R. However, this is bad! You should always be able to reproduce your environment given a set of commands saved in an R script or R Markdown document, whereas saving and reloading your environment encourages you to be sloppy about this.\nTo disable this in RStudio, go to Tools \u0026gt; Global Options \u0026gt; General and set the options as follows:\n  Recommended R/RStudio settings   To start R in the same way from the command line:\nR --no-save --no-restore-data \n Installing R packages CRAN packages To install an R package that is available at CRAN, the default R package repository, from within R (e.g. in the R console in RStudio), use the install.packages() function.\nThe install.packages() function will handle dependencies within R \u0026ndash; i.e., it will install other R packages that your package depends on. Occasionally, when the install function needs to compile a package from source, errors arise that relate to missing system dependencies (i.e. software outside of R).\nOn Mac and Linux, these system dependencies are best installed outside of R, such as with homebrew on Mac or apt on Ubuntu. The errror message you got when trying to install an R package should tell you which system dependencies are needed.\nOn Windows, you can use the installr package to install such dependencies or other software from within R \u0026ndash; for example:\ninstall.packages(\u0026#34;installr\u0026#34;) # Install the installr package first installlr::install.RStudio() # Install RStudio installr::install.python() # Install Python \nSystem setup to installing packages \u0026ldquo;from source\u0026rdquo; Sometimes you need to install a package from source, that is, you need to compile the package rather than simply installing a pre-existing binary. (On Linux, where installing from source is often needed, this should work without additional steps.) On Windows and Mac, installing from source is generally only needed when you install a package from outside of CRAN (such as from Github, see below), but you will need to make sure you have the following non-R software:\nOn Windows, you will need Rtools ( Rtools installation instructions).\nOn a Mac, you will need Xcode (which can be installed from the Mac App store).\nYou can test whether or not you are able to install packages from source using the devtools package:\ninstall.packages(\u0026#34;devtools\u0026#34;) # Install the devtools package devtools::has_devel() # Check whether you can install packages from source For a bit more info, see this page.\nInstalling packages from Github To install a package from Github, use either the devtools or the remotes package \u0026ndash; for example:\ninstall.packages(\u0026#34;devtools\u0026#34;) # Install the devtools package devtools::install_github(\u0026#34;kbroman/broman\u0026#34;) # Install from a repository using \u0026#34;\u0026lt;username\u0026gt;/\u0026lt;repo-name\u0026gt;\u0026#34; This will install the package from source, so you will need to make sure you are able to do so by following the instructions in the section right above this one.\nInstalling packages from Bioconductor If you\u0026rsquo;re doing bioinformatic analyses in R, you will probably run into packages that are not on CRAN but on Bioconductor. To install a package from Bioconductor, use the BiocManager package \u0026ndash; for example:\ninstall.packages(\u0026#34;BiocManager\u0026#34;) # Install the BiocManager package BiocManager::install(\u0026#34;edgeR\u0026#34;) # Install the edgeR package from Bioconductor \nUpdating R Consider updating R if you have an older version of R installed. Specifically, in the first session of Code Club, we\u0026rsquo;ve seen problems when installing the tidyverse with R versions below R 3.6.\nYou can check which version of R you have by looking at the first lines of output when running the following command inside R:\nsessionInfo() To update:   Windows: You can update R from within R. The updateR() function will also take care of updating your packages:\ninstall.packages(\u0026#34;installr\u0026#34;) installr::updateR()   Mac: Download and install the latest .pkg file as if you were installing it for the first time.\n  Linux: In Ubuntu, if you installed R with apt or apt-get, you can use apt-get upgrade in a terminal. Otherwise, download and install the latest version after removing the old one. Rtask has some instructions for upgrading to R 4.0 in Ubuntu (along with upgrading to Ubuntu 20.04).\n  Re-installing your packages after updating (Mac and Linux) While the installr::updateR() function for Windows users takes care of reinstalling your packages along with updating R, Mac and Linux users will have to manually re-install their packages. Some people prefer to re-install these packages on the fly, which can end up being a way to get rid of packages you no longer use.\nBut if you want immediately reinstall all your packages, run this before you upgrade:\nmy_packages \u0026lt;- installed.packages() saveRDS(my_packages, \u0026#34;my_packages.rds\u0026#34;) Then, after you\u0026rsquo;ve installed the latest R version:\nmy_packages \u0026lt;- readRDS(\u0026#34;CurrentPackages.rds\u0026#34;) install.packages(my_packages[1, ]) This will only work for packages available on CRAN. Of course, you can check your list for Github-only and Bioconductor packages and then install those with their respective commands (see below). Yes, this can be a bit of a hassle!\n  \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606845019,"objectID":"ca094992cb695d0d14880a7b5e13427b","permalink":"https://biodash.github.io/codeclub-novice/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-novice/","section":"","summary":"Code Club:  R \u0026ndash; Getting Started and Some Tips  New to R? If you are completely new to R, we recommend watching at least the first couple of videos from Mike Sovic\u0026rsquo;s Youtube playlist of short videos on R, and ideally all of them, prior to attending Code Club.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  Information for Presenters  Introduction   Each Code Club session should be represented by one post on the website at https://biodash.github.io/codeclub/.\n  Regular presenters will be given direct access to the Github repository and will be able to push a new post to the website directly.\n  Occasional presenters can either send their material directly to Jelmer or create a \u0026ldquo;pull request\u0026rdquo; with their new post.\n  Content should be written in R Markdown (.Rmd) or \u0026ldquo;plain\u0026rdquo; Markdown (.md). If you write in .Rmd, you need to render to .md locally. Conversion of .md to an HTML file suitable for the website will be done automatically upon pushing the master branch of the repository.\n  Make sure to get the session materials onto the website at least several days before the session.\n   Getting your files onto the site 1: Get the repo You only need to do this if you want to create a pull request or push your content to the website directly. If you want to send your (R) Markdown file by email, skip this and continue to Step 2.\nThe following assumes you have git installed, set up, have a Github account, and have your git linked up to Github.\nOption A: Fork the repo to prep for a Pull Request   Fork the repo: go to https://github.com/biodash/biodash.github.io and click the Fork button way in the top-right corner of the page.\n  Get the URL for your repo: In your forked repo, click the green Code button and copy the URL for the repo to your clipboard (either the HTTPS or the SSH URL; the former will be less likely to lead to authentication problems).\n  Go to a dir that you would like to be the parent dir of the Biodash/Codeclub repo:\ncd my-dir   Clone your forked repo, using the URL that you copied to your clipboard:\ngit clone https://github.com/\u0026lt;YOUR-USERNAME\u0026gt;/biodash.github.io.git   Move into the newly cloned (downloaded) repository dir:\ncd biodash.github.io   Add the original repository as an \u0026ldquo;upstream\u0026rdquo; remote:\ngit remote add upstream https://github.com/biodash/biodash.github.io.git  You can check which remote repos (i.e., repos on Github) are linked to your local repo using:\ngit remote -v This should show your forked repo as \u0026ldquo;origin\u0026rdquo;, and the original repo as \u0026ldquo;upstream\u0026rdquo;. You won\u0026rsquo;t be able to push to the original repo, but you can push to your forked repo and then submit a pull request, as we\u0026rsquo;ll do below.\n    Option B: Clone the repo directly (direct access required)   Go to a dir that you would like to be the parent dir of the Biodash/Codeclub repo:\ncd my-dir   Clone the website repo:\ngit clone https://github.com/biodash/biodash.github.io.git # Using HTTPS # Or: `git clone git@github.com:biodash/biodash.github.io.git` using SSH   Create a new branch (by way of example called \u0026ldquo;my-branch\u0026rdquo;) and switch to it:\ngit checkout -b my-branch Creating a new branch is not strictly necessary but it may be safer/easier to experiment in.\n   2: Create a Code Club post   Here, we\u0026rsquo;ll use the hugodown package to create a Markdown skeleton for our post, and below we\u0026rsquo;ll also use hugodown to preview the site.\nNote that you can easily bypass hugodown by simply copying the YAML header from the first code club session (see here for the .Rmd file) into a new file and taking it from there.  If you don\u0026rsquo;t have the hugodown package installed, install it:\nremotes::install_github(\u0026#34;r-lib/hugodown\u0026#34;) # Or equivalently, use devtools::install_githhub()   A post bundle is a separate folder for a post which will hold the R Markdown file that contains the post, as well as associated images and so on. To create a post bundle along with a R Markdown file that already has many useful YAML header tags:\nhugodown::use_post(\u0026#39;codeclub/\u0026lt;session-number\u0026gt;_\u0026lt;short-title\u0026gt;\u0026#39;) # An example would be: hugodown::use_post(\u0026#39;codeclub/01_intro-to-R\u0026#39;) The \u0026lt;session-number\u0026gt; is the actual Code Club session number, and \u0026lt;short-title\u0026gt; is a short title that you would like to give the post, which will be used for links and the folder name.\n The name of the .Rmd file will be index.Rmd, and it should keep that name! Keep this name also if you create your .Rmd manually or by copying the file from another Code Club session. It will eventually turn into index.html, which is the name that will trigger the file to be displayed on the website.     Fill out some of the YAML, such as the title, subtitle, authors (in kebab-case, e.g. john-doe, to link to your author profile; note that Jelmer\u0026rsquo;s name here is \u0026ldquo;admin\u0026rdquo;), and optionally tags and summary (the summary will appear on Biodash\u0026rsquo;s front page in the \u0026ldquo;Recent Posts\u0026rdquo; widget; this can be good to fill out here because the default summary can be awkward, as it combines headers and paragraphs).\n If you specify a date using the `date` tag in the YAML, and this date is in the future (e.g. the date of the Code Club session), the page will not be built and will thus not appear on the website! Specifiying the date using `date` or `lastmod` in the YAML is not particularly useful anyway -- when you edit the post after the specified date, it will use the edit date.     Write the contents of your Code Club session that you would like to share with participants, in R Markdown format. For formatting tips, see below.\n   If you want participants to load an R Markdown file or script:\nAn easy solution is to place the file in the same directory as your post, and include it in your git commit, so it will be uploaded to Github. In that case, the URL to the file for direct downloads for participants will be: https://raw.githubusercontent.com/biodash/biodash.github.io/master/docs/codeclub/\u0026lt;session-number\u0026gt;_\u0026lt;short-title\u0026gt;/\u0026lt;filename\u0026gt;.\nIn your post, include a function call like file.download(\u0026lt;script-URL\u0026gt;) for participants to get the file \u0026ndash; this will work both for participants working locally and those working in an OSC RStudio Server instance.\nIf your session contains a dataset:\nLike for the markdown/script, place the file(s) in the same directory as your post. If you have a markdown/script for participants, include file.download(\u0026lt;dataset-URL\u0026gt;) in this file, otherwise include it directly in your post.\n    Convert your .Rmd (R Markdown) file to a .md (Markdown) file.\n Hugo renders .md but not .Rmd to HTML, so we have to always render to .md first when writing in .Rmd.   Since your output is specified as hugodown::md_document, this is done most easily by \u0026ldquo;knitting\u0026rdquo; your post in RStudio by clicking Knit in the top bar, or by pressing Ctrl + Shift + K.\n   3: Preview your post or build the website (optional) You can do this in two ways, from RStudio or from the command line.\nOption A: In RStudio   Install Hugo:\nhugodown::hugo_install(\u0026#34;0.66.0\u0026#34;)   Preview the website:\nhugodown::hugo_start() #\u0026gt; Starting server on port 1313 This will provide a preview RStudio. To look at it in a browser, go to localhost:1313, where 1313 corresponds to the port returned in the R console (see above).\n  Option B: From the command line   Install Hugo using these instructions.\n  Serve the website locally:\nhugo serve You will see a message that includes \u0026ldquo;Web Server is available at [\u0026hellip;]\u0026rdquo;. Click the link or copy and paste the address into a browser, and you will see the rendered website.\nThe server will keep running and will update whenever you save changes in a file that is within the website directory, until you stop it using Ctrl + C.\n   Side note: Building the website Note that you don\u0026rsquo;t need to build the website, because it will be built automatically from Markdown files whenever you push to (the master branch of) the Github repo.\nBut as background info, or in case automatic builds fail, here is how you would build the site:\n  Using Hugo from the shell:\nhugo -d docs/   Using hugodown in R:\nhugodown::hugo_build(dest = \u0026#34;docs\u0026#34;)   The entire rendered website is in the docs/ dir; HTML files rendered from Markdown files will be placed there, any images and other files will be copied there, and so on.\n   4: Commit   Add the files from your post:\ngit add codeclub/\u0026lt;your-post-name\u0026gt;/* ## Or, e.g. if you added files elswehere too, or have built the site: # git add *   Check if all your changes and new files have been staged:\ngit status   Commit:\ngit commit -m \u0026#34;Add CodeClub session \u0026lt;session-nr\u0026gt; by \u0026lt;your-name\u0026gt;\u0026#34;    5: Push or submit pull request Your Markdown (.md) file(s) will be built along with the rest of the website by Hugo. Using Github Actions, this will be done automatically upon pushing to the master branch on Github, which is all we need to do. Note that the built website will be committed by Github Actions not to the master branch but to the gh-actions branch.\nOption A: Create a pull request When you create a pull request, you are asking the maintainers of a repository to pull your changes into their repository.\n  Pull from the original repo to make sure your repo is up-to-date:\ngit pull upstream master # \u0026#34;upstream\u0026#34; refers to the original Github repo This will first fetch the upstream changes and then merge them into your local repo, thus keeping your local changes. If git does not manage to perform this merge automatically, which can happen if the same parts of the same files have been edited both locally and upstream, there will be a merge conflict which you will need to resolve manually.\n  Push to your forked repo:\ngit push origin master # \u0026#34;origin\u0026#34; refers to your forked Github repo   Create the pull request:\n Go to the Pull requests page of our repo at https://github.com/biodash/biodash.github.io/pulls. Click the green button on the right that says New pull request. Under the large Compare changes header, click Compare across forks. In the drop-down menu to the right of the arrow, select your fork. Enter a title (e.g. \u0026ldquo;New Post: Session 6\u0026quot;) and description (say a little more about the post) for the pull request. Click the green button Send pull request.    For a more detailed step-by-step of creating a pull request from a fork, see here.\nOption B: Push to the site repo (direct access required)   Merge your branch with the main (master) branch:\ngit checkout master # Move to the master branch prior to merging git merge my-branch # Merge into master (assuming your branch was named \u0026#34;my-branch\u0026#34;)   Push to the master branch:\ngit push origin master    6: Install packages at OSC (optional) Many R packages are already installed at OSC (nearly 200 for R 4.0.2), including the tidyverse. You can check which packages have been installed by typing, in an R session at OSC:\nlibrary() This will list packages by library, which should include two locations available to all OSC users (starting with /usr/local/R), your personal library, and the Code Club library (/fs/ess/PAS1838/CODECLUB/Rpkgs).\nIf you want to make another package available to Code Club participants, you can do so as follows in an RStudio Server session at OSC:\ninstall.packages(\u0026#34;\u0026lt;pkg-name\u0026gt;\u0026#34;, lib = \u0026#34;/fs/ess/PAS1838/CODECLUB/Rpkgs\u0026#34;) This library is available to all members of the Code Club OSC classroom project. To check specifically which packages are available in this library \u0026ndash; and whether your newly installed package has indeed been installed here, type:\nlibrary(lib.loc = \u0026#34;/fs/ess/PAS1838/CODECLUB/Rpkgs\u0026#34;) Alternatively, you can let participants working at OSC install the packages themselves, like participants that work locally will have to do.\n Formatting tips Miscellaneous   If you want a Table of Contents (TOC) for your file, add a line toc: true to the YAML (not indented, as it is not an option of the output format).\n  To add an image, put it in the same directory as the markdown file, and refer to it without prepending a path.\n  \u0026lt;br\u0026gt; will insert a line break, which can be useful to get more space between sections.\n  I add lines above each major section header using ---- (preceded by a \u0026lt;br\u0026gt;).\n  Add a line that reads source_extension: '.Rmd' (not indented) to your R Markdown, which will ensure that there is a link to the source document at the top of your post.\nEDIT: I have removed these source links for now. They were also visible in the \u0026ldquo;Recent Posts\u0026rdquo; widget on the home page, and some people clicked on that link rather than the website link. Then, they ended up on in the Github repo but didn\u0026rsquo;t even know they were in the wrong place since the contents of the post is present.\n  Hidden sections It can be useful to provide solutions to small challenges in the file, but to hide them by default. This can be done with a little HTML:\n\u0026lt;details\u0026gt; \u0026lt;summary\u0026gt; Solution (click here) \u0026lt;/summary\u0026gt; \u0026lt;br\u0026gt; ... Your solution - this can be a long section including a code block... ```{r} install.packages(\u0026quot;tidyverse\u0026quot;) ``` \u0026lt;/details\u0026gt; This is rendered as:\n  Solution (click here)  \u0026hellip; Your solution - this can be a long section including a code block\u0026hellip;\ninstall.packages(\u0026quot;tidyverse\u0026quot;)  Info/alert notes To produce boxes to draw attention to specific content, you can use two classes specific to the Hugo Academic Theme (now branded as \u0026ldquo;Wowchemy\u0026rdquo;).\n  alert-note for a blue box with an info symbol:\n\u0026lt;div class=\u0026quot;alert alert-note\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is an alert note. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Which is rendered as:\n This is an alert note.     alert-warning for a red box with a warning symbol:\n\u0026lt;div class=\u0026quot;alert alert-warning\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is an alert warning. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Which is rendered as:\n This is an alert warning.     I also added a custom class, puzzle:\n\u0026lt;div class=\u0026quot;alert puzzle\u0026quot;\u0026gt; \u0026lt;div\u0026gt; This is a puzzle div, for do-it-yourself challenges. \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;  This is a puzzle div, for do-it-yourself challenges.   Custom classes and other custom formatting can be written in CSS in the assets/scss/custom.scss file.\n  All of these classes can also be called using pandoc\u0026rsquo;s ::: notation when you\u0026rsquo;re writing in .Rmd (but not if you\u0026rsquo;re writing in .md), e.g.:\n:::puzzle This is a puzzle div, for do-it-yourself challenges. :::   Code highlighting  Code highlighting doesn't work with out of the box with .Rmd files. But it should be possible to get it to work, stay tuned!   Hugo supports the highlighting of specific lines of code using the syntax below in md documents:\n```r {hl_lines=[1,\u0026quot;3-4\u0026quot;]} library(\u0026quot;tidyverse\u0026quot;) weight_df %\u0026gt;% mutate(mean_weight = mean(weight)) %\u0026gt;% select(mean_weight, everything()) dim(weight_df) ``` library(\u0026#34;tidyverse\u0026#34;) weight_df %\u0026gt;% mutate(mean_weight = mean(weight)) %\u0026gt;% select(mean_weight, everything()) dim(weight_df) Shortcodes  Like code highlighting, shortcodes only work with .md files. The blogdown package has a shortcode() function to support them (see here), but hugodown does not support them.   Hugo shortcodes are little code snippets for specific content. Some of these are specific to Wowchemy, and others are available for any Hugo site.\nHighlight text You can highlight text as follows:\nHere is some {{\u0026lt; hl \u0026gt;}}highlighted text{{\u0026lt; /hl \u0026gt;}}. This will render as:\nHere is some highlighted text.\nIcons Wowchemy supports shortcodes for icons, for instance:\n {{\u0026lt; icon name=\u0026#34;r-project\u0026#34; pack=\u0026#34;fab\u0026#34; \u0026gt;}}   {{\u0026lt; icon name=\u0026#34;python\u0026#34; pack=\u0026#34;fab\u0026#34; \u0026gt;}}   {{\u0026lt; icon name=\u0026#34;terminal\u0026#34; pack=\u0026#34;fas\u0026#34; \u0026gt;}} General Hugo shortcodes   To embed a Youtube video, use the following, replacing \u0026ldquo;videoID\u0026rdquo; by the actual ID (https://www.youtube.com/watch?v=ID) in\n{{\u0026lt; youtube ID \u0026gt;}}   To embed a Tweet, use the following, replacing \u0026ldquo;tweetID\u0026rdquo; by the actual ID (https://twitter.com/user/status/ID):\n{{\u0026lt; tweet ID \u0026gt;}}   For more info and more shortcodes, see the Hugo documentation on shortcodes.\n   \n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606671808,"objectID":"d2a87fb5a2b4f8e331f2a7033b4ca3df","permalink":"https://biodash.github.io/codeclub-present/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-present/","section":"","summary":"Code Club:  Information for Presenters  Introduction   Each Code Club session should be represented by one post on the website at https://biodash.github.io/codeclub/.\n  Regular presenters will be given direct access to the Github repository and will be able to push a new post to the website directly.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  Schedule  Upcoming sessions    Session nr. Fri 3 pm Presenter Topic (+ link)     10 Feb 19 Jessica Faceting, animating, and combining plots   11 Feb 26 Stephen Making maps with ggmap   12 Mar 5 Jelmer Loops and if statements   13 Mar 12 Michael The apply family of functions   14 Mar 19 Mike purrr    See also the BioDASH calendar for an overview.\n Past sessions    Session nr. Date Presenter Topic (+ link) Other Attendants     1 Nov 18 \u0026amp; 20 Jelmer  RStudio Projects \u0026amp; getting started  slides 56   2 Dec 2 \u0026amp; 4 Jessica  dplyr core verbs  37   3 Dec 9 \u0026amp; 11 Mike S.  Joining datasets  26   4 Dec 16 \u0026amp; 18 Michael B.  ggplot2 \u0026ndash; round 1  27   5 Jan 15 Jessica  ggplot2 \u0026ndash; round 2  23   6 Jan 22 Stephen  Factors  21   7 Jan 29 Jelmer  R Markdown  23   8 Feb 5 Mike S.  Pivoting data  18   9 Feb 12 Michael B.  Subsetting data  19                ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613226642,"objectID":"21036bb90326781dbc3f5f76a5396fb3","permalink":"https://biodash.github.io/codeclub-schedule/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-schedule/","section":"","summary":"Code Club:  Schedule  Upcoming sessions    Session nr. Fri 3 pm Presenter Topic (+ link)     10 Feb 19 Jessica Faceting, animating, and combining plots   11 Feb 26 Stephen Making maps with ggmap   12 Mar 5 Jelmer Loops and if statements   13 Mar 12 Michael The apply family of functions   14 Mar 19 Mike purrr    See also the BioDASH calendar for an overview.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  Computer setup and other information for newcomers  Here, you will find general information on computer setup for Code Club. Additional setup instructions for individual sessions will appear in the posts for each session.\nSummary:   You can work with local installations or in your browser at the Ohio Supercomputer Center (OSC).\n  If you plan to work with local installations:\n   Install R and install RStudio, or update R if your version is below R 3.6.\n   Install the tidyverse and test if you can load it.\n  We recommend you do make sure you have an active OSC account with access to the Code Club project, as a backup option.\n    If you plan to work at OSC:\n  Sign up at OSC if you don\u0026rsquo;t have an account yet.\n  You should have received\n  Test if you can start an RStudio Server session.\n    Planning to come to Code Club for the first time? Have a look at the  introductory slides.\n  Have not used R before? See the  Getting Started with R page.\n  Have not used RStudio before?\nTake a moment to explore it, and it may help to check out Mike Sovic\u0026rsquo;s 1-minute intro or RStudio\u0026rsquo;s intro video.\n  Questions? Don\u0026rsquo;t hesitate to contact Jelmer or one of the other organizers. You can also join the Code Club Zoom call 15 minutes early, and one or more of the organizers will be on there already.\n   Option 1 \u0026ndash; OSC All Code Club participants will get access to the Ohio Supercomputer Center (OSC) Classroom Project for Code Club (PAS1838). This way, you can code in (e.g.) RStudio from your browser rather than with a local installation. This is a good option if you prefer not to install anything locally or run into problems when doing so.\nIf you already had an OSC account, you should have been added to the Code Club OSC project and can continue to the second step. Otherwise, please follow the instructions below to sign up and get access to the project.\nSign up at OSC To sign up:\n  Go to https://my.osc.edu/ and click the blue \u0026ldquo;Sign Up\u0026rdquo; bar.\n  In the bottom right portion of the form where you provide your info (see screenshot below), you should enter Code Club\u0026rsquo;s Project Code, which is PAS1838. If you want to use OSC, please do this on a day prior to your first Code Club participation. This way, there is time to troubleshoot if needed. Moreover, the Code Club option on the Interactive Apps page below can take a few hours to appear after you become a member of the project.\n    Enter Project Code PAS1838 in the red box (click to enlarge)   Run RStudio Server during Code Club  OSC OnDemand lets you access OSC resources through your browser and run a couple of applications with GUIs, like RStudio. It has a separate access point, https://class.osc.edu/, for classroom projects such as this one.\n To get started, go to https://class.osc.edu/ and log in with your OSC username and password. Then, click on Interactive Apps in the blue top bar, and select RStudio Server (Owens and Pitzer): Form. Now, you\u0026rsquo;re on a page from which you can launch an RStudio server that will run on an OSC cluster.  Under Class Materials, select Code Club. Under Number of hours, enter 2. Click Launch.   Now, you should see a box like this:    Your job should start running pretty soon, and when it\u0026rsquo;s ready the box should look like this:    Click Connect to RStudio Server at the bottom of the box, and an RStudio Server instance will open. You\u0026rsquo;re ready to go!  More about OSC The above instructions should be all you need to access RStudio using OSC, but there is lot more to OSC than that! For more information about using OSC, see the excellent Getting Started materials on their website (make sure not to miss the HOWTOs). Also, Mike Sovic has a YouTube playlist \u0026ldquo;Getting Started With High Performance Computing (HPC)\u0026quot; at his channel The Data Point.\n Option 2 \u0026ndash; Local install Summary You will need:\n R: At least version 3.6 \u0026ndash; See here for instructions to update R RStudio R packages that we will regularly use:  tidyverse    Install R  Windows: Download and run the .exe file for the latest version of R from https://cran.r-project.org/bin/windows/base/, by clicking the large Download R [version-number] for Windows link at the top of the gray box. Mac: Download and run the .pkg file for the latest version of R from https://cran.r-project.org/bin/macosx/, by clicking the link just below Latest release. On a Linux distribution, you can also install R using the website above, but you may prefer to use a package manager instead \u0026ndash; for instance, seee these instructions for installing the latest R version on Ubuntu 20.04 using the apt package manager.  Install RStudio RStudio is a so-called Integrated Development Environment (IDE) for R, with side-by-side panes for an R script, an R concole, plots, help documents, and much more. While it is perfectly possible to use R without RStudio, RStudio has become the de facto standard for working with R and is very useful.\nTo install RStudio, go to the RStudio download page and download and run the installer file for your operating system.\nInstall the tidyverse Install the tidyverse, which is a collection of useful R packages, by typing the following command inside an R console:\ninstall.packages(\u0026#34;tidyverse\u0026#34;) Test whether you can load the tidyverse When you issue the command library(\u0026quot;tidyverse\u0026quot;), you should get the output shown below:\nlibrary(\u0026#34;tidyverse\u0026#34;) #\u0026gt; ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── #\u0026gt; ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 #\u0026gt; ✔ tibble 3.0.4 ✔ dplyr 1.0.2 #\u0026gt; ✔ tidyr 1.1.2 ✔ stringr 1.4.0 #\u0026gt; ✔ readr 1.3.1 ✔ forcats 0.5.0 #\u0026gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── #\u0026gt; ✖ dplyr::filter() masks stats::filter() #\u0026gt; ✖ dplyr::lag() masks stats::lag() If you get an error instead, please try to troubleshoot it. Updating R itself may be necessary, see here for instructions. You can also send the organizers of Code Club an email. And if you can\u0026rsquo;t get it to work yet, you can always use OSC for the time being, see the setup instructions further up on this page.\n More info Please see the  Getting started with R page for:\n  Resources to get started with R  Useful R and RStudio settings  The basics of installing packages in R  Instructions for updating R    \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612222973,"objectID":"a19fd71e3dc86af820a45dabc994dda5","permalink":"https://biodash.github.io/codeclub-setup/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-setup/","section":"","summary":"Code Club:  Computer setup and other information for newcomers  Here, you will find general information on computer setup for Code Club. Additional setup instructions for individual sessions will appear in the posts for each session.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Code Club:  Suggest a Topic!  Please use the form below to suggest a topic or concept to be covered at Code Club. You are also welcome to leave a suggestion relating to the general format, organization, or presentation of Code Club.\nIf you suggest a topic, a broad range of suggestions are welcome \u0026ndash; it need not fit neatly as a single Code Club session. So your suggestion can be as broad as \u0026ldquo;object-oriented programming\u0026rdquo; or as specific as a single R function that you happen to struggle with or that you just really like.\nWe do note that we prefer to cover topics that could be \u0026ndash;at least in principle\u0026ndash; of interest to a broad range of people, and are not specific to a certain data type. For instance, we would generally be hesitant to cover in detail the high-level functions of an R package to analyze microbiomic data.\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"aa47a17260af4b87dc29397d821ae7fd","permalink":"https://biodash.github.io/codeclub-suggest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/codeclub-suggest/","section":"","summary":"Code Club:  Suggest a Topic!  Please use the form below to suggest a topic or concept to be covered at Code Club. You are also welcome to leave a suggestion relating to the general format, organization, or presentation of Code Club.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Other Events  On this page, we will list upcoming events related to computational biology, coding, and data analysis. If you know of an event that you think should be listed here, please use the form in the Suggest\u0026hellip; page.\n Calendar   Recurring Events  CAPS Tn-seq working group  A working group dedicated to the analysis of Transposon-seq (Tn-seq) data. Every Monday from 10-11 am. Contact Mike Sovic for details.     MCIC bioinformatics office hour  Need some quick advice or help? Drop by! Every Tuesday from 2-4 pm: Zoom link. Advance notice to Jelmer Poelstra is appreciated. Anyone at OSU is welcome, and if you\u0026rsquo;re outside of OSU, feel free to inquire.     Center of Microbiome Center working groups  CoMS is running Virome, Microbiome, and Advanced Ecological Statistics working groups, see their page for more details.     Spring \u0026lsquo;21 Course: Practical Computing Skills for Biologists Jelmer Poelstra will teach a computing course as a section of PlantPath 8300 in the \u0026lsquo;21 spring semester.\n Focused on the command line, bash and Python scripting, and reproducible science with version control and automated workflows. 14 weeks, 2 credits, online-only Zoom sessions on Tuesdays and Thursdays from 3:55-4:55 pm. Graduate level, but undergraduates may be eligible to take the course as an IS. Sign up for class number 35953. Contact Jelmer for more information and a syllabus.   Upcoming Workshops None right now!\n Upcoming Outside of OSU  For Ohio Supercomputer Center events, such as regular introductory sessions to computing at OSC, see the OSC Events page.   Misc. Relevant OSU Courses   M5161: Introduction to Computational Genomics  M8161: Microbiome Informatics  PLNTPTH 7003.01: Agricultural Genomics: Principles and Applications  HCS 7806: Current Topics and Methods Courses  Includes \u0026ldquo;Genome Analytics\u0026rdquo; and \u0026ldquo;Methods in Data Visualization\u0026rdquo;.    ENR8600: Introduction to R for Environmental Sciences  MOLGEN 5645: Quantitative, Population, and Evolutionary Genetics  MOLGEN 5623: Genetics and Genomics  STAT 6625: Statistical Analysis of Genetic Data  STAT 6730: Introduction to Computational Statistics  FDSCTE 7600: Metabolomics, Principles and Practice   Past Events TBA\n  \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605226921,"objectID":"426d522fcb82973f95d828bcc08f03ff","permalink":"https://biodash.github.io/events/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/events/","section":"","summary":"Other Events  On this page, we will list upcoming events related to computational biology, coding, and data analysis. If you know of an event that you think should be listed here, please use the form in the Suggest\u0026hellip; page.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Material  Here, we will post tutorials, analysis pipelines, instructional videos, slidedecks, materials from past workshops, and so on.\n  MCIC/MCBL\u0026rsquo;s Read-the-Docs site with tutorials.\n  CAPS/Mike Sovic\u0026rsquo;s Youtube Channel.\n   Slides from RNA-seq intro meetings, Jan-Feb 2021.\n    \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613140153,"objectID":"4f38a30d7b59cc95eec7edb025f56bf6","permalink":"https://biodash.github.io/material/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/material/","section":"","summary":"Material  Here, we will post tutorials, analysis pipelines, instructional videos, slidedecks, materials from past workshops, and so on.\n  MCIC/MCBL\u0026rsquo;s Read-the-Docs site with tutorials.\n  CAPS/Mike Sovic\u0026rsquo;s Youtube Channel.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"An introduction to RNA-seq data processing and analysis     Date Topic Slides     2021-01-08 Introductory notes on RNA-seq and NGS data    2021-01-29 Running FastQC for many files at OSC    2021-02-05 Interpreting FastQC output    2021-02-12 Running MultiQC    2021-02-17 Preprocessing FASTQ files    2021-02-17 Intro to RNAseq alignment and STAR    2021-02-26 RNAseq read alignment with STAR    2021-02-26 More on BAM files       \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614873820,"objectID":"2abddd9e844fe678c3abd50333e2c077","permalink":"https://biodash.github.io/material_2021-01_rnaseq-intro/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/material_2021-01_rnaseq-intro/","section":"","summary":"An introduction to RNA-seq data processing and analysis     Date Topic Slides     2021-01-08 Introductory notes on RNA-seq and NGS data    2021-01-29 Running FastQC for many files at OSC    2021-02-05 Interpreting FastQC output    2021-02-12 Running MultiQC    2021-02-17 Preprocessing FASTQ files    2021-02-17 Intro to RNAseq alignment and STAR    2021-02-26 RNAseq read alignment with STAR    2021-02-26 More on BAM files","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Suggest a topic or event  If there is a topic you would like to see covered on this website, an event you would like to see happen, or an event you think should be listed under Events, please fill out the form below. You can also indicate whether you would like to help with this content or event!\nLoading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605479556,"objectID":"ec134bd5815c50401fba8e9a987eda12","permalink":"https://biodash.github.io/suggest/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/suggest/","section":"","summary":"Suggest a topic or event  If there is a topic you would like to see covered on this website, an event you would like to see happen, or an event you think should be listed under Events, please fill out the form below.","tags":null,"title":"","type":"page"},{"authors":null,"categories":null,"content":"Bioinformatics support  Available support Need assistance with bioinformatics, high-throughput sequencing data analysis, working with big data, etc? See below for a list of support options for OSU researchers\u0026hellip;\nMCIC The Molecular and Cellular Imaging Center (MCIC) is an OSU core facility with locations at the Wooster and Columbus campuses. Among other things, the MCIC provides end-to-end support for genomics projects \u0026mdash; from experimental design, library preparation, and sequencing, to infrastructure for and assistance with data analysis.\nThe bioinformatics section of the MCIC, the MCBL (MCIC Computational Biology Laboratory), works based on a membership model. MCBL members have access to bioinformatics support, our project at the Ohio Supercomputer Center for storage and computing, our two in-house servers, our computer lab in Wooster, and free access to workshops. To become a member, please fill out this form.\nFor bioinformatics consultation, anyone is free to contact Jelmer Poelstra or drop by at the Bioinformatics Office Hour over Zoom every Tuesday between 2 and 4 pm ( Zoom link - advance notice is appreciated!).\nCAPS The Center for Applied Plant Sciences (CAPS) supports OSU researchers working in the plant sciences. CAPS research scientist Mike Sovic leads bioinformatic support efforts for the Center. Details on opportunities and resources available through CAPS are at http://caps.osu.edu/bioinformatics.\nEEOB Research Scientist Michael Broe provides bioinformatics support to faculty and students in the Department of Evolution, Ecology and Organismal Biology. His primary role is to assist graduate students learning various types of computational analysis, but he also works directly with PIs. Types of analysis include whole genome assembly, genome annotation, transcriptomics, RADseq, hybrid capture pipelines, variant calling etc. Michael also teaches a 7 week Introduction to Computation in Biology for incoming EEOB graduate students each year, covering R, Python, and Unix.\n Request support Loading…   \n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605742851,"objectID":"6f03e42d6bae7b75ea525bec87eb719f","permalink":"https://biodash.github.io/support/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/support/","section":"","summary":"Bioinformatics support  Available support Need assistance with bioinformatics, high-throughput sequencing data analysis, working with big data, etc? See below for a list of support options for OSU researchers\u0026hellip;\nMCIC The Molecular and Cellular Imaging Center (MCIC) is an OSU core facility with locations at the Wooster and Columbus campuses.","tags":null,"title":"","type":"page"}]